{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-01T11:24:21.357705300Z",
     "start_time": "2024-12-01T11:24:21.330705Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def print_distribution(df, col):\n",
    "    positive = np.count_nonzero(df[col] == 1)\n",
    "    negative = np.count_nonzero(df[col] == 0)\n",
    "    print(\"- Positive:\", positive)\n",
    "    print(\"- Negative:\", negative)\n",
    "    print(\"- Verhältnis:\", negative / positive)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T11:02:47.973007900Z",
     "start_time": "2024-12-01T11:02:47.946009300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Erstellung neuer Train und Testdatensatz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "    label                                              tweet\nid                                                          \n1       0   @user when a father is dysfunctional and is s...\n2       0  @user @user thanks for #lyft credit i can't us...\n3       0                                bihday your majesty\n4       0  #model   i love u take with u all the time in ...\n5       0             factsguide: society now    #motivation",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>@user when a father is dysfunctional and is s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>@user @user thanks for #lyft credit i can't us...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>bihday your majesty</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_old = pd.read_csv(\"twitter_hate-speech-alt/train.csv\", index_col=0)\n",
    "df_train_old.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T11:02:48.245016600Z",
     "start_time": "2024-12-01T11:02:48.165017300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Positive: 2242\n",
      "- Negative: 29720\n",
      "- Verhältnis: 13.256021409455842\n"
     ]
    }
   ],
   "source": [
    "print_distribution(df_train_old, \"label\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T11:02:48.466014500Z",
     "start_time": "2024-12-01T11:02:48.392021700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Positive: 1574\n",
      "- Negative: 20799\n",
      "- Verhältnis: 13.214104193138501\n"
     ]
    }
   ],
   "source": [
    "train = df_train_old.sample(frac=0.7, random_state=200)\n",
    "print_distribution(train, \"label\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T11:03:22.441702300Z",
     "start_time": "2024-12-01T11:03:22.380705Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "       label                                              tweet\nid                                                             \n8886       0   @user #cinemaaawards final rehearsals!! geari...\n909        0  istg this is the best cheese ta but dayum expe...\n27613      0  this was amazing. the weather was not. #musica...\n15999      0  yes! #talented #sexy   âcriminal mindsâ ca...\n23817      0  want to be   while being #successful? see how ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8886</th>\n      <td>0</td>\n      <td>@user #cinemaaawards final rehearsals!! geari...</td>\n    </tr>\n    <tr>\n      <th>909</th>\n      <td>0</td>\n      <td>istg this is the best cheese ta but dayum expe...</td>\n    </tr>\n    <tr>\n      <th>27613</th>\n      <td>0</td>\n      <td>this was amazing. the weather was not. #musica...</td>\n    </tr>\n    <tr>\n      <th>15999</th>\n      <td>0</td>\n      <td>yes! #talented #sexy   âcriminal mindsâ ca...</td>\n    </tr>\n    <tr>\n      <th>23817</th>\n      <td>0</td>\n      <td>want to be   while being #successful? see how ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T11:05:04.873875300Z",
     "start_time": "2024-12-01T11:05:04.855882500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Positive: 668\n",
      "- Negative: 8921\n",
      "- Verhältnis: 13.354790419161677\n"
     ]
    }
   ],
   "source": [
    "test = df_train_old.drop(train.index)\n",
    "print_distribution(test, \"label\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T11:03:22.611520400Z",
     "start_time": "2024-12-01T11:03:22.579523200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "    label                                              tweet\nid                                                          \n4       0  #model   i love u take with u all the time in ...\n5       0             factsguide: society now    #motivation\n6       0  [2/2] huge fan fare and big talking before the...\n16      0  ouch...junior is angryð#got7 #junior #yugyo...\n18      1                             retweet if you agree! ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>[2/2] huge fan fare and big talking before the...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0</td>\n      <td>ouch...junior is angryð#got7 #junior #yugyo...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1</td>\n      <td>retweet if you agree!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T11:05:17.599894700Z",
     "start_time": "2024-12-01T11:05:17.566895900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "train.to_csv(\"twitter_hate-speech/train.csv\")\n",
    "test.to_csv(\"twitter_hate-speech/test.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T11:04:35.606463300Z",
     "start_time": "2024-12-01T11:04:35.437427100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bereinigung und Vorverarbeitung"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "from ftfy import fix_encoding\n",
    "\n",
    "from src.functions.clean_data_generic_functions import to_lowercase, expand_shortcuts, handle_userhandles, \\\n",
    "    handle_hashtags, extract_emojis, replace_emojis, replace_text_smileys, remove_url_from_tweet, remove_punctuation, \\\n",
    "    remove_special_characters, remove_digits, remove_word_from_column, lemmatize, remove_stop_words, \\\n",
    "    remove_most_frequent_words, remove_least_frequent_words, remove_duplicates, remove_na_from_column"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T15:35:35.046365700Z",
     "start_time": "2024-12-02T15:35:35.011366300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bereinigung und Vorverarbeitung: TRAIN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Geeignet für: Maschinelles Lernen\n",
    "es werden alle implementierten Cleaning und Preprocessing Schritte ausgeführt AUßER: Auflösen Negationen (nicht implementiert), Emojis Entfernen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "df_origin_train = pd.read_csv('./twitter_hate-speech/train.csv', index_col=0)\n",
    "df_clean_base_train = df_origin_train.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:08:55.087561700Z",
     "start_time": "2024-12-02T16:08:55.012570900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "df_clean_base_train.drop_duplicates(inplace=True)\n",
    "df_clean_base_train[\"tweet\"] = df_clean_base_train['tweet'].apply(fix_encoding)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:08:57.446164800Z",
     "start_time": "2024-12-02T16:08:55.780222900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def clean_dataframe_train(base_df):\n",
    "    i = 1\n",
    "    count = 20\n",
    "    df_cleaned = base_df.copy()\n",
    "    df_cleaned['tweet_cleaned'] = df_cleaned['tweet']\n",
    "\n",
    "    print(\"Start Cleaning\")\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: to_lowercase\")\n",
    "    df_cleaned = to_lowercase(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: expand_shortcuts\")\n",
    "    df_cleaned = expand_shortcuts(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_negations - SKIP\")\n",
    "    # # df_cleaned = remove_negations(df_cleaned)\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: handle_userhandles\")\n",
    "    df_cleaned = handle_userhandles(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: handle_hashtags\")\n",
    "    df_cleaned = handle_hashtags(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: extract_emojis\")\n",
    "    df_cleaned = extract_emojis(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: replace_emojis\")\n",
    "    df_cleaned = replace_emojis(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: replace_smileys\")\n",
    "    df_cleaned = replace_text_smileys(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_emojis - SKIP\")\n",
    "    # df_cleaned = remove_emojis(df_cleaned)\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_url_from_tweet\")\n",
    "    df_cleaned = remove_url_from_tweet(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_punctuation\")\n",
    "    df_cleaned = remove_punctuation(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_special_characters\")\n",
    "    df_cleaned = remove_special_characters(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_digis\")\n",
    "    df_cleaned = remove_digits(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_word_from_column: amp\")\n",
    "    df_cleaned = remove_word_from_column(df=df_cleaned, column_name=\"tweet_cleaned\", word=\"amp\")\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: lemmatize\")\n",
    "    df_cleaned = lemmatize(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_stop_words\")\n",
    "    df_cleaned = remove_stop_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_most_frequent_words\")\n",
    "    df_cleaned = remove_most_frequent_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_least_frequent_words\")\n",
    "    df_cleaned = remove_least_frequent_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_duplicates\")\n",
    "    df_cleaned = remove_duplicates(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_nans\")\n",
    "    df_cleaned = remove_na_from_column(df=df_cleaned, column_name=\"tweet_cleaned\")\n",
    "\n",
    "    print(\"All Cleaning done\")\n",
    "\n",
    "    return df_cleaned"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:08:57.514155800Z",
     "start_time": "2024-12-02T16:08:57.460161600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cleaning\n",
      "Cleaning Step 1/20: to_lowercase\n",
      "Cleaning Step 2/20: expand_shortcuts\n",
      "Cleaning Step 3/20: remove_negations - SKIP\n",
      "Cleaning Step 4/20: handle_userhandles\n",
      "Cleaning Step 5/20: handle_hashtags\n",
      "Cleaning Step 6/20: extract_emojis\n",
      "Cleaning Step 7/20: replace_emojis\n",
      "Cleaning Step 8/20: replace_smileys\n",
      "Cleaning Step 9/20: remove_emojis - SKIP\n",
      "Cleaning Step 10/20: remove_url_from_tweet\n",
      "Cleaning Step 11/20: remove_punctuation\n",
      "Cleaning Step 12/20: remove_special_characters\n",
      "Cleaning Step 13/20: remove_digis\n",
      "Cleaning Step 14/20: remove_word_from_column: amp\n",
      "Cleaning Step 15/20: lemmatize\n",
      "Cleaning Step 16/20: remove_stop_words\n",
      "Cleaning Step 17/20: remove_most_frequent_words\n",
      "Cleaning Step 18/20: remove_least_frequent_words\n",
      "Cleaning Step 19/20: remove_duplicates\n",
      "Cleaning Step 20/20: remove_nans\n",
      "All Cleaning done\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_train = clean_dataframe_train(df_clean_base_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:11:32.036478100Z",
     "start_time": "2024-12-02T16:09:02.960964600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "       label  \\\nid             \n8886       0   \n909        0   \n27613      0   \n15999      0   \n23817      0   \n\n                                                                                                                  tweet  \\\nid                                                                                                                        \n8886    @user #cinemaaawards final rehearsals!! gearing up for the evening!! #butterflies #stage  ! hope u all like it    \n909                                                                istg this is the best cheese ta but dayum expensive    \n27613                            this was amazing. the weather was not. #musical #london #matilda #westend #weekend  …    \n15999                    yes! #talented #sexy   ‘criminal minds’ casts @user as series regular for season 12  via @user   \n23817                           want to be   while being #successful? see how #worklifebalance helps in  by @user @user   \n\n                                                            tweet_cleaned  \\\nid                                                                          \n8886               final rehearsal gear evening butterfly stage hope like   \n909                                              good cheese ta expensive   \n27613                              amazing weather musical london weekend   \n15999  yes talented sexy ' criminal mind ' cast series regular season via   \n23817                                            want successful see help   \n\n       user_handle                                           hashtags emojis  \nid                                                                            \n8886             1             [#cinemaaawards, #butterflies, #stage]         \n909              0                                                 []         \n27613            0  [#musical, #london, #matilda, #westend, #weekend]         \n15999            2                                 [#talented, #sexy]         \n23817            2                    [#successful, #worklifebalance]         ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n      <th>tweet_cleaned</th>\n      <th>user_handle</th>\n      <th>hashtags</th>\n      <th>emojis</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8886</th>\n      <td>0</td>\n      <td>@user #cinemaaawards final rehearsals!! gearing up for the evening!! #butterflies #stage  ! hope u all like it</td>\n      <td>final rehearsal gear evening butterfly stage hope like</td>\n      <td>1</td>\n      <td>[#cinemaaawards, #butterflies, #stage]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>909</th>\n      <td>0</td>\n      <td>istg this is the best cheese ta but dayum expensive</td>\n      <td>good cheese ta expensive</td>\n      <td>0</td>\n      <td>[]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>27613</th>\n      <td>0</td>\n      <td>this was amazing. the weather was not. #musical #london #matilda #westend #weekend  …</td>\n      <td>amazing weather musical london weekend</td>\n      <td>0</td>\n      <td>[#musical, #london, #matilda, #westend, #weekend]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>15999</th>\n      <td>0</td>\n      <td>yes! #talented #sexy   ‘criminal minds’ casts @user as series regular for season 12  via @user</td>\n      <td>yes talented sexy ' criminal mind ' cast series regular season via</td>\n      <td>2</td>\n      <td>[#talented, #sexy]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>23817</th>\n      <td>0</td>\n      <td>want to be   while being #successful? see how #worklifebalance helps in  by @user @user</td>\n      <td>want successful see help</td>\n      <td>2</td>\n      <td>[#successful, #worklifebalance]</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:12:35.117043900Z",
     "start_time": "2024-12-02T16:12:35.050050700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "df_cleaned_train.to_csv('./twitter_hate-speech/train_cleaned.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:12:36.182592400Z",
     "start_time": "2024-12-02T16:12:35.999599900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Geeignet für: Deep Learning (RNN)\n",
    "es werden nicht alle implementierten Cleaning und Preprocessing Schritte ausgeführt\n",
    "\n",
    "NICHT: Auflösen Negationen (nicht implementiert), Emojis Entfernen, Lemmatisierung, Stopwords entfernen, Most/Least frequent words entfernen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "df_origin_train_rnn = pd.read_csv('./twitter_hate-speech/train.csv', index_col=0)\n",
    "df_clean_base_train_rnn = df_origin_train_rnn.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:13:22.615142800Z",
     "start_time": "2024-12-02T16:13:22.532149100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "df_clean_base_train_rnn.drop_duplicates(inplace=True)\n",
    "df_clean_base_train_rnn[\"tweet\"] = df_clean_base_train_rnn['tweet'].apply(fix_encoding)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:13:24.697117600Z",
     "start_time": "2024-12-02T16:13:23.195174600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def clean_dataframe_train_rnn(base_df):\n",
    "    i = 1\n",
    "    count = 20\n",
    "    df_cleaned = base_df.copy()\n",
    "    df_cleaned['tweet_cleaned'] = df_cleaned['tweet']\n",
    "\n",
    "    print(\"Start Cleaning\")\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: to_lowercase\")\n",
    "    df_cleaned = to_lowercase(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: expand_shortcuts\")\n",
    "    df_cleaned = expand_shortcuts(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_negations - SKIP\")\n",
    "    # # df_cleaned = remove_negations(df_cleaned)\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: handle_userhandles\")\n",
    "    df_cleaned = handle_userhandles(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: handle_hashtags\")\n",
    "    df_cleaned = handle_hashtags(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: extract_emojis\")\n",
    "    df_cleaned = extract_emojis(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: replace_emojis\")\n",
    "    df_cleaned = replace_emojis(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: replace_smileys\")\n",
    "    df_cleaned = replace_text_smileys(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_emojis - SKIP\")\n",
    "    # df_cleaned = remove_emojis(df_cleaned)\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_url_from_tweet\")\n",
    "    df_cleaned = remove_url_from_tweet(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_punctuation\")\n",
    "    df_cleaned = remove_punctuation(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_special_characters\")\n",
    "    df_cleaned = remove_special_characters(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_digis\")\n",
    "    df_cleaned = remove_digits(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_word_from_column: amp\")\n",
    "    df_cleaned = remove_word_from_column(df=df_cleaned, column_name=\"tweet_cleaned\", word=\"amp\")\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: lemmatize  - SKIP\")\n",
    "    # df_cleaned = lemmatize(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_stop_words  - SKIP\")\n",
    "    # df_cleaned = remove_stop_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_most_frequent_words  - SKIP\")\n",
    "    # df_cleaned = remove_most_frequent_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_least_frequent_words  - SKIP\")\n",
    "    # df_cleaned = remove_least_frequent_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_duplicates\")\n",
    "    df_cleaned = remove_duplicates(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_nans\")\n",
    "    df_cleaned = remove_na_from_column(df=df_cleaned, column_name=\"tweet_cleaned\")\n",
    "\n",
    "    print(\"All Cleaning done\")\n",
    "\n",
    "    return df_cleaned"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:13:24.713119900Z",
     "start_time": "2024-12-02T16:13:24.707119400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cleaning\n",
      "Cleaning Step 1/20: to_lowercase\n",
      "Cleaning Step 2/20: expand_shortcuts\n",
      "Cleaning Step 3/20: remove_negations - SKIP\n",
      "Cleaning Step 4/20: handle_userhandles\n",
      "Cleaning Step 5/20: handle_hashtags\n",
      "Cleaning Step 6/20: extract_emojis\n",
      "Cleaning Step 7/20: replace_emojis\n",
      "Cleaning Step 8/20: replace_smileys\n",
      "Cleaning Step 9/20: remove_emojis - SKIP\n",
      "Cleaning Step 10/20: remove_url_from_tweet\n",
      "Cleaning Step 11/20: remove_punctuation\n",
      "Cleaning Step 12/20: remove_special_characters\n",
      "Cleaning Step 13/20: remove_digis\n",
      "Cleaning Step 14/20: remove_word_from_column: amp\n",
      "Cleaning Step 15/20: lemmatize  - SKIP\n",
      "Cleaning Step 16/20: remove_stop_words  - SKIP\n",
      "Cleaning Step 17/20: remove_most_frequent_words  - SKIP\n",
      "Cleaning Step 18/20: remove_least_frequent_words  - SKIP\n",
      "Cleaning Step 19/20: remove_duplicates\n",
      "Cleaning Step 20/20: remove_nans\n",
      "All Cleaning done\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_train_rnn = clean_dataframe_train_rnn(df_clean_base_train_rnn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:13:30.667897400Z",
     "start_time": "2024-12-02T16:13:24.715123500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "       label  \\\nid             \n8886       0   \n909        0   \n27613      0   \n15999      0   \n23817      0   \n\n                                                                                                                  tweet  \\\nid                                                                                                                        \n8886    @user #cinemaaawards final rehearsals!! gearing up for the evening!! #butterflies #stage  ! hope u all like it    \n909                                                                istg this is the best cheese ta but dayum expensive    \n27613                            this was amazing. the weather was not. #musical #london #matilda #westend #weekend  …    \n15999                    yes! #talented #sexy   ‘criminal minds’ casts @user as series regular for season 12  via @user   \n23817                           want to be   while being #successful? see how #worklifebalance helps in  by @user @user   \n\n                                                                                           tweet_cleaned  \\\nid                                                                                                         \n8886   cinemaaawards final rehearsals gearing up for the evening butterflies stage  hope You all like it   \n909                                                  istg this is the best cheese ta but dayum expensive   \n27613                      this was amazing the weather was not musical london matilda westend weekend …   \n15999                         yes talented sexy ‘criminal minds’ casts as series regular for season  via   \n23817                              want to be while being successful see how worklifebalance helps in by   \n\n       user_handle                                           hashtags emojis  \nid                                                                            \n8886             1             [#cinemaaawards, #butterflies, #stage]         \n909              0                                                 []         \n27613            0  [#musical, #london, #matilda, #westend, #weekend]         \n15999            2                                 [#talented, #sexy]         \n23817            2                    [#successful, #worklifebalance]         ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n      <th>tweet_cleaned</th>\n      <th>user_handle</th>\n      <th>hashtags</th>\n      <th>emojis</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8886</th>\n      <td>0</td>\n      <td>@user #cinemaaawards final rehearsals!! gearing up for the evening!! #butterflies #stage  ! hope u all like it</td>\n      <td>cinemaaawards final rehearsals gearing up for the evening butterflies stage  hope You all like it</td>\n      <td>1</td>\n      <td>[#cinemaaawards, #butterflies, #stage]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>909</th>\n      <td>0</td>\n      <td>istg this is the best cheese ta but dayum expensive</td>\n      <td>istg this is the best cheese ta but dayum expensive</td>\n      <td>0</td>\n      <td>[]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>27613</th>\n      <td>0</td>\n      <td>this was amazing. the weather was not. #musical #london #matilda #westend #weekend  …</td>\n      <td>this was amazing the weather was not musical london matilda westend weekend …</td>\n      <td>0</td>\n      <td>[#musical, #london, #matilda, #westend, #weekend]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>15999</th>\n      <td>0</td>\n      <td>yes! #talented #sexy   ‘criminal minds’ casts @user as series regular for season 12  via @user</td>\n      <td>yes talented sexy ‘criminal minds’ casts as series regular for season  via</td>\n      <td>2</td>\n      <td>[#talented, #sexy]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>23817</th>\n      <td>0</td>\n      <td>want to be   while being #successful? see how #worklifebalance helps in  by @user @user</td>\n      <td>want to be while being successful see how worklifebalance helps in by</td>\n      <td>2</td>\n      <td>[#successful, #worklifebalance]</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned_train_rnn.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:13:30.685897800Z",
     "start_time": "2024-12-02T16:13:30.665897600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "df_cleaned_train_rnn.to_csv('./twitter_hate-speech/train_cleaned_rnn.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:13:30.982888200Z",
     "start_time": "2024-12-02T16:13:30.681898400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bereinigung und Vorverarbeitung: TEST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Geeignet für: Maschinelles Lernen\n",
    "es werden alle implementierten Cleaning und Preprocessing Schritte ausgeführt AUßER welche, bei denen Zeilen gelöscht werden"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "df_origin_test = pd.read_csv('./twitter_hate-speech/test.csv', index_col=0)\n",
    "df_clean_base_test = df_origin_test.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:13:48.183242900Z",
     "start_time": "2024-12-02T16:13:48.119250500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "df_clean_base_test.drop_duplicates(inplace=True)\n",
    "df_clean_base_test[\"tweet\"] = df_clean_base_test['tweet'].apply(fix_encoding)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:13:49.701699100Z",
     "start_time": "2024-12-02T16:13:48.886729300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "def clean_dataframe_test(base_df):\n",
    "    i = 1\n",
    "    count = 20\n",
    "    df_cleaned = base_df.copy()\n",
    "    df_cleaned['tweet_cleaned'] = df_cleaned['tweet']\n",
    "\n",
    "    print(\"Start Cleaning\")\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: to_lowercase\")\n",
    "    df_cleaned = to_lowercase(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: expand_shortcuts\")\n",
    "    df_cleaned = expand_shortcuts(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_negations - SKIP\")\n",
    "    # # df_cleaned = remove_negations(df_cleaned)\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: handle_userhandles\")\n",
    "    df_cleaned = handle_userhandles(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: handle_hashtags\")\n",
    "    df_cleaned = handle_hashtags(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: extract_emojis\")\n",
    "    df_cleaned = extract_emojis(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: replace_emojis\")\n",
    "    df_cleaned = replace_emojis(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: replace_smileys\")\n",
    "    df_cleaned = replace_text_smileys(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_emojis - SKIP\")\n",
    "    # df_cleaned = remove_emojis(df_cleaned)\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_url_from_tweet\")\n",
    "    df_cleaned = remove_url_from_tweet(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_punctuation\")\n",
    "    df_cleaned = remove_punctuation(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_special_characters\")\n",
    "    df_cleaned = remove_special_characters(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_digis\")\n",
    "    df_cleaned = remove_digits(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_word_from_column: amp\")\n",
    "    df_cleaned = remove_word_from_column(df=df_cleaned, column_name=\"tweet_cleaned\", word=\"amp\")\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: lemmatize\")\n",
    "    df_cleaned = lemmatize(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_stop_words\")\n",
    "    df_cleaned = remove_stop_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_most_frequent_words\")\n",
    "    df_cleaned = remove_most_frequent_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_least_frequent_words\")\n",
    "    df_cleaned = remove_least_frequent_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_duplicates - SKIP\")\n",
    "    # df_cleaned = remove_duplicates(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_nans\")\n",
    "    df_cleaned = remove_na_from_column(df=df_cleaned, column_name=\"tweet_cleaned\")\n",
    "\n",
    "    print(\"All Cleaning done\")\n",
    "\n",
    "    return df_cleaned"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:13:49.711700300Z",
     "start_time": "2024-12-02T16:13:49.684700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cleaning\n",
      "Cleaning Step 1/20: to_lowercase\n",
      "Cleaning Step 2/20: expand_shortcuts\n",
      "Cleaning Step 3/20: remove_negations - SKIP\n",
      "Cleaning Step 4/20: handle_userhandles\n",
      "Cleaning Step 5/20: handle_hashtags\n",
      "Cleaning Step 6/20: extract_emojis\n",
      "Cleaning Step 7/20: replace_emojis\n",
      "Cleaning Step 8/20: replace_smileys\n",
      "Cleaning Step 9/20: remove_emojis - SKIP\n",
      "Cleaning Step 10/20: remove_url_from_tweet\n",
      "Cleaning Step 11/20: remove_punctuation\n",
      "Cleaning Step 12/20: remove_special_characters\n",
      "Cleaning Step 13/20: remove_digis\n",
      "Cleaning Step 14/20: remove_word_from_column: amp\n",
      "Cleaning Step 15/20: lemmatize\n",
      "Cleaning Step 16/20: remove_stop_words\n",
      "Cleaning Step 17/20: remove_most_frequent_words\n",
      "Cleaning Step 18/20: remove_least_frequent_words\n",
      "Cleaning Step 19/20: remove_duplicates - SKIP\n",
      "Cleaning Step 20/20: remove_nans\n",
      "All Cleaning done\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_test = clean_dataframe_test(df_clean_base_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:14:56.748978600Z",
     "start_time": "2024-12-02T16:13:50.353424300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "    label  \\\nid          \n4       0   \n5       0   \n6       0   \n16      0   \n18      1   \n\n                                                                                                                   tweet  \\\nid                                                                                                                         \n4                                                         #model   i love u take with u all the time in ur📱!!! 😙😎👄👅💦💦💦     \n5                                                                                 factsguide: society now    #motivation   \n6   [2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo     \n16                                                                 ouch...junior is angry😐#got7 #junior #yugyoem   #omg    \n18                                                                                                retweet if you agree!    \n\n                                                                                        tweet_cleaned  \\\nid                                                                                                      \n4   model take time mobile phone kiss sunglass mouth tongue sweat droplet sweat droplet sweat droplet   \n5                                                                       factsguide society motivation   \n6                                                                    huge fan big leave chaos pay get   \n16                                                                               angry get junior omg   \n18                                                                                      retweet agree   \n\n    user_handle                          hashtags  \\\nid                                                  \n4             0                          [#model]   \n5             0                     [#motivation]   \n6             0                 [#allshowandnogo]   \n16            0  [#got7, #junior, #yugyoem, #omg]   \n18            0                                []   \n\n                                                                                                                                                emojis  \nid                                                                                                                                                      \n4   :mobile_phone:,:kissing_face_with_smiling_eyes:,:smiling_face_with_sunglasses:,:mouth:,:tongue:,:sweat_droplets:,:sweat_droplets:,:sweat_droplets:  \n5                                                                                                                                                       \n6                                                                                                                                                       \n16                                                                                                                                      :neutral_face:  \n18                                                                                                                                                      ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n      <th>tweet_cleaned</th>\n      <th>user_handle</th>\n      <th>hashtags</th>\n      <th>emojis</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ur📱!!! 😙😎👄👅💦💦💦</td>\n      <td>model take time mobile phone kiss sunglass mouth tongue sweat droplet sweat droplet sweat droplet</td>\n      <td>0</td>\n      <td>[#model]</td>\n      <td>:mobile_phone:,:kissing_face_with_smiling_eyes:,:smiling_face_with_sunglasses:,:mouth:,:tongue:,:sweat_droplets:,:sweat_droplets:,:sweat_droplets:</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n      <td>factsguide society motivation</td>\n      <td>0</td>\n      <td>[#motivation]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo</td>\n      <td>huge fan big leave chaos pay get</td>\n      <td>0</td>\n      <td>[#allshowandnogo]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0</td>\n      <td>ouch...junior is angry😐#got7 #junior #yugyoem   #omg</td>\n      <td>angry get junior omg</td>\n      <td>0</td>\n      <td>[#got7, #junior, #yugyoem, #omg]</td>\n      <td>:neutral_face:</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1</td>\n      <td>retweet if you agree!</td>\n      <td>retweet agree</td>\n      <td>0</td>\n      <td>[]</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned_test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:14:56.794974500Z",
     "start_time": "2024-12-02T16:14:56.748978600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "df_cleaned_test.to_csv('./twitter_hate-speech/test_cleaned.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:14:56.965970300Z",
     "start_time": "2024-12-02T16:14:56.765979600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Geeignet für: Deep Learning (RNN)\n",
    "es werden nicht alle implementierten Cleaning und Preprocessing Schritte ausgeführt\n",
    "\n",
    "NICHT: Duplikate entfernen, Auflösen Negationen (nicht implementiert), Emojis Entfernen, Lemmatisierung, Stopwords entfernen, Most/Least frequent words entfernen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "df_origin_test_rnn = pd.read_csv('./twitter_hate-speech/test.csv', index_col=0)\n",
    "df_clean_base_test_rnn = df_origin_test_rnn.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:15:01.016798900Z",
     "start_time": "2024-12-02T16:15:00.971804300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "df_clean_base_test_rnn.drop_duplicates(inplace=True)\n",
    "df_clean_base_test_rnn[\"tweet\"] = df_clean_base_test_rnn['tweet'].apply(fix_encoding)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:15:01.846770200Z",
     "start_time": "2024-12-02T16:15:01.163795100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "def clean_dataframe_test_rnn(base_df):\n",
    "    i = 1\n",
    "    count = 20\n",
    "    df_cleaned = base_df.copy()\n",
    "    df_cleaned['tweet_cleaned'] = df_cleaned['tweet']\n",
    "\n",
    "    print(\"Start Cleaning\")\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: to_lowercase\")\n",
    "    df_cleaned = to_lowercase(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: expand_shortcuts\")\n",
    "    df_cleaned = expand_shortcuts(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_negations - SKIP\")\n",
    "    # # df_cleaned = remove_negations(df_cleaned)\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: handle_userhandles\")\n",
    "    df_cleaned = handle_userhandles(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: handle_hashtags\")\n",
    "    df_cleaned = handle_hashtags(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: extract_emojis\")\n",
    "    df_cleaned = extract_emojis(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: replace_emojis\")\n",
    "    df_cleaned = replace_emojis(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: replace_smileys\")\n",
    "    df_cleaned = replace_text_smileys(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_emojis - SKIP\")\n",
    "    # df_cleaned = remove_emojis(df_cleaned)\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_url_from_tweet\")\n",
    "    df_cleaned = remove_url_from_tweet(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_punctuation\")\n",
    "    df_cleaned = remove_punctuation(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_special_characters\")\n",
    "    df_cleaned = remove_special_characters(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_digis\")\n",
    "    df_cleaned = remove_digits(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_word_from_column: amp\")\n",
    "    df_cleaned = remove_word_from_column(df=df_cleaned, column_name=\"tweet_cleaned\", word=\"amp\")\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: lemmatize - SKIP\")\n",
    "    # df_cleaned = lemmatize(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_stop_words - SKIP\")\n",
    "    # df_cleaned = remove_stop_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_most_frequent_words - SKIP\")\n",
    "    # df_cleaned = remove_most_frequent_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_least_frequent_words - SKIP\")\n",
    "    # df_cleaned = remove_least_frequent_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_duplicates  - SKIP\")\n",
    "    # df_cleaned = remove_duplicates(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_nans\")\n",
    "    df_cleaned = remove_na_from_column(df=df_cleaned, column_name=\"tweet_cleaned\")\n",
    "\n",
    "    print(\"All Cleaning done\")\n",
    "\n",
    "    return df_cleaned"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:15:01.874767500Z",
     "start_time": "2024-12-02T16:15:01.836771200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cleaning\n",
      "Cleaning Step 1/20: to_lowercase\n",
      "Cleaning Step 2/20: expand_shortcuts\n",
      "Cleaning Step 3/20: remove_negations - SKIP\n",
      "Cleaning Step 4/20: handle_userhandles\n",
      "Cleaning Step 5/20: handle_hashtags\n",
      "Cleaning Step 6/20: extract_emojis\n",
      "Cleaning Step 7/20: replace_emojis\n",
      "Cleaning Step 8/20: replace_smileys\n",
      "Cleaning Step 9/20: remove_emojis - SKIP\n",
      "Cleaning Step 10/20: remove_url_from_tweet\n",
      "Cleaning Step 11/20: remove_punctuation\n",
      "Cleaning Step 12/20: remove_special_characters\n",
      "Cleaning Step 13/20: remove_digis\n",
      "Cleaning Step 14/20: remove_word_from_column: amp\n",
      "Cleaning Step 15/20: lemmatize - SKIP\n",
      "Cleaning Step 16/20: remove_stop_words - SKIP\n",
      "Cleaning Step 17/20: remove_most_frequent_words - SKIP\n",
      "Cleaning Step 18/20: remove_least_frequent_words - SKIP\n",
      "Cleaning Step 19/20: remove_duplicates  - SKIP\n",
      "Cleaning Step 20/20: remove_nans\n",
      "All Cleaning done\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_test_rnn = clean_dataframe_test_rnn(df_clean_base_test_rnn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:15:04.536672100Z",
     "start_time": "2024-12-02T16:15:01.858773500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "    label  \\\nid          \n4       0   \n5       0   \n6       0   \n16      0   \n18      1   \n\n                                                                                                                   tweet  \\\nid                                                                                                                         \n4                                                         #model   i love u take with u all the time in ur📱!!! 😙😎👄👅💦💦💦     \n5                                                                                 factsguide: society now    #motivation   \n6   [2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo     \n16                                                                 ouch...junior is angry😐#got7 #junior #yugyoem   #omg    \n18                                                                                                retweet if you agree!    \n\n                                                                                                                                                                            tweet_cleaned  \\\nid                                                                                                                                                                                          \n4   model i love You take with You all the time in ur mobile phone  kissing face with smiling eyes smiling face with sunglasses mouth tongue sweat droplets sweat droplets sweat droplets   \n5                                                                                                                                                       factsguide society now motivation   \n6                                                                               huge fan fare and big talking before they leave chaos and pay disputes when they get there allshowandnogo   \n16                                                                                                                                ouchjunior is angry neutral face got junior yugyoem omg   \n18                                                                                                                                                                   retweet if you agree   \n\n    user_handle                          hashtags  \\\nid                                                  \n4             0                          [#model]   \n5             0                     [#motivation]   \n6             0                 [#allshowandnogo]   \n16            0  [#got7, #junior, #yugyoem, #omg]   \n18            0                                []   \n\n                                                                                                                                                emojis  \nid                                                                                                                                                      \n4   :mobile_phone:,:kissing_face_with_smiling_eyes:,:smiling_face_with_sunglasses:,:mouth:,:tongue:,:sweat_droplets:,:sweat_droplets:,:sweat_droplets:  \n5                                                                                                                                                       \n6                                                                                                                                                       \n16                                                                                                                                      :neutral_face:  \n18                                                                                                                                                      ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n      <th>tweet_cleaned</th>\n      <th>user_handle</th>\n      <th>hashtags</th>\n      <th>emojis</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ur📱!!! 😙😎👄👅💦💦💦</td>\n      <td>model i love You take with You all the time in ur mobile phone  kissing face with smiling eyes smiling face with sunglasses mouth tongue sweat droplets sweat droplets sweat droplets</td>\n      <td>0</td>\n      <td>[#model]</td>\n      <td>:mobile_phone:,:kissing_face_with_smiling_eyes:,:smiling_face_with_sunglasses:,:mouth:,:tongue:,:sweat_droplets:,:sweat_droplets:,:sweat_droplets:</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n      <td>factsguide society now motivation</td>\n      <td>0</td>\n      <td>[#motivation]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo</td>\n      <td>huge fan fare and big talking before they leave chaos and pay disputes when they get there allshowandnogo</td>\n      <td>0</td>\n      <td>[#allshowandnogo]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0</td>\n      <td>ouch...junior is angry😐#got7 #junior #yugyoem   #omg</td>\n      <td>ouchjunior is angry neutral face got junior yugyoem omg</td>\n      <td>0</td>\n      <td>[#got7, #junior, #yugyoem, #omg]</td>\n      <td>:neutral_face:</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1</td>\n      <td>retweet if you agree!</td>\n      <td>retweet if you agree</td>\n      <td>0</td>\n      <td>[]</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned_test_rnn.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:15:04.564670200Z",
     "start_time": "2024-12-02T16:15:04.539671Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "df_cleaned_test_rnn.to_csv('./twitter_hate-speech/test_cleaned_rnn.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:15:04.739665900Z",
     "start_time": "2024-12-02T16:15:04.554674400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bereinigung und Vorverarbeitung: PREDICT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Geeignet für: Maschinelles Lernen\n",
    "es werden alle implementierten Cleaning und Preprocessing Schritte ausgeführt AUßER welche, bei denen Zeilen gelöscht werden"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "df_origin_predict = pd.read_csv('./twitter_hate-speech/predict.csv', index_col=0)\n",
    "df_clean_base_predict = df_origin_predict.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:15:04.740666Z",
     "start_time": "2024-12-02T16:15:04.664666700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "df_clean_base_predict.drop_duplicates(inplace=True)\n",
    "df_clean_base_predict[\"tweet\"] = df_clean_base_predict['tweet'].apply(fix_encoding)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:15:06.248606500Z",
     "start_time": "2024-12-02T16:15:04.712672700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "def clean_dataframe_predict(base_df):\n",
    "    i = 1\n",
    "    count = 20\n",
    "    df_cleaned = base_df.copy()\n",
    "    df_cleaned['tweet_cleaned'] = df_cleaned['tweet']\n",
    "\n",
    "    print(\"Start Cleaning\")\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: to_lowercase\")\n",
    "    df_cleaned = to_lowercase(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: expand_shortcuts\")\n",
    "    df_cleaned = expand_shortcuts(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_negations - SKIP\")\n",
    "    # # df_cleaned = remove_negations(df_cleaned)\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: handle_userhandles\")\n",
    "    df_cleaned = handle_userhandles(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: handle_hashtags\")\n",
    "    df_cleaned = handle_hashtags(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: extract_emojis\")\n",
    "    df_cleaned = extract_emojis(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: replace_emojis\")\n",
    "    df_cleaned = replace_emojis(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: replace_smileys\")\n",
    "    df_cleaned = replace_text_smileys(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_emojis - SKIP\")\n",
    "    # df_cleaned = remove_emojis(df_cleaned)\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_url_from_tweet\")\n",
    "    df_cleaned = remove_url_from_tweet(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_punctuation\")\n",
    "    df_cleaned = remove_punctuation(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_special_characters\")\n",
    "    df_cleaned = remove_special_characters(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_digis\")\n",
    "    df_cleaned = remove_digits(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_word_from_column: amp\")\n",
    "    df_cleaned = remove_word_from_column(df=df_cleaned, column_name=\"tweet_cleaned\", word=\"amp\")\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: lemmatize\")\n",
    "    df_cleaned = lemmatize(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_stop_words\")\n",
    "    df_cleaned = remove_stop_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_most_frequent_words\")\n",
    "    df_cleaned = remove_most_frequent_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_least_frequent_words\")\n",
    "    df_cleaned = remove_least_frequent_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_duplicates - SKIP\")\n",
    "    # df_cleaned = remove_duplicates(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_nans\")\n",
    "    df_cleaned = remove_na_from_column(df=df_cleaned, column_name=\"tweet_cleaned\")\n",
    "\n",
    "    print(\"All Cleaning done\")\n",
    "\n",
    "    return df_cleaned"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:15:06.316604200Z",
     "start_time": "2024-12-02T16:15:06.232607Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cleaning\n",
      "Cleaning Step 1/20: to_lowercase\n",
      "Cleaning Step 2/20: expand_shortcuts\n",
      "Cleaning Step 3/20: remove_negations - SKIP\n",
      "Cleaning Step 4/20: handle_userhandles\n",
      "Cleaning Step 5/20: handle_hashtags\n",
      "Cleaning Step 6/20: extract_emojis\n",
      "Cleaning Step 7/20: replace_emojis\n",
      "Cleaning Step 8/20: replace_smileys\n",
      "Cleaning Step 9/20: remove_emojis - SKIP\n",
      "Cleaning Step 10/20: remove_url_from_tweet\n",
      "Cleaning Step 11/20: remove_punctuation\n",
      "Cleaning Step 12/20: remove_special_characters\n",
      "Cleaning Step 13/20: remove_digis\n",
      "Cleaning Step 14/20: remove_word_from_column: amp\n",
      "Cleaning Step 15/20: lemmatize\n",
      "Cleaning Step 16/20: remove_stop_words\n",
      "Cleaning Step 17/20: remove_most_frequent_words\n",
      "Cleaning Step 18/20: remove_least_frequent_words\n",
      "Cleaning Step 19/20: remove_duplicates - SKIP\n",
      "Cleaning Step 20/20: remove_nans\n",
      "All Cleaning done\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_predict = clean_dataframe_predict(df_clean_base_predict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:17:04.357260600Z",
     "start_time": "2024-12-02T16:15:06.243608Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                                                                                       tweet  \\\nid                                                                                                                                             \n31963                                               #studiolife #aislife #requires #passion #dedication #willpower   to find #newmaterials…    \n31964                                           @user #white #supremacists want everyone to see the new ‘  #birds’ #movie — and here’s why     \n31965                                                                safe ways to heal your #acne!!    #altwaystoheal #healthy   #healing!!    \n31966  is the hp and the cursed child book up for reservations already? if yes, where? if no, when? 😍😍😍   #harrypotter #pottermore #favorite   \n31967                                              3rd #bihday to my amazing, hilarious #nephew eli ahmir! uncle dave loves you and misses…    \n\n                                            tweet_cleaned  user_handle  \\\nid                                                                       \n31963                                require passion find            0   \n31964      white want everyone see new ' bird ' movie — ’            1   \n31965         safe way heal altwaystoheal healthy healing            0   \n31966         child book already yes harrypotter favorite            0   \n31967  rd bihday amazing hilarious nephew uncle dave miss            0   \n\n                                                                                   hashtags  \\\nid                                                                                            \n31963  [#studiolife, #aislife, #requires, #passion, #dedication, #willpower, #newmaterials]   \n31964                                               [#white, #supremacists, #birds, #movie]   \n31965                                           [#acne, #altwaystoheal, #healthy, #healing]   \n31966                                                [#harrypotter, #pottermore, #favorite]   \n31967                                                                    [#bihday, #nephew]   \n\n                                                                                             emojis  \nid                                                                                                   \n31963                                                                                                \n31964                                                                                                \n31965                                                                                                \n31966  :smiling_face_with_heart-eyes:,:smiling_face_with_heart-eyes:,:smiling_face_with_heart-eyes:  \n31967                                                                                                ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>tweet_cleaned</th>\n      <th>user_handle</th>\n      <th>hashtags</th>\n      <th>emojis</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>31963</th>\n      <td>#studiolife #aislife #requires #passion #dedication #willpower   to find #newmaterials…</td>\n      <td>require passion find</td>\n      <td>0</td>\n      <td>[#studiolife, #aislife, #requires, #passion, #dedication, #willpower, #newmaterials]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>31964</th>\n      <td>@user #white #supremacists want everyone to see the new ‘  #birds’ #movie — and here’s why</td>\n      <td>white want everyone see new ' bird ' movie — ’</td>\n      <td>1</td>\n      <td>[#white, #supremacists, #birds, #movie]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>31965</th>\n      <td>safe ways to heal your #acne!!    #altwaystoheal #healthy   #healing!!</td>\n      <td>safe way heal altwaystoheal healthy healing</td>\n      <td>0</td>\n      <td>[#acne, #altwaystoheal, #healthy, #healing]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>31966</th>\n      <td>is the hp and the cursed child book up for reservations already? if yes, where? if no, when? 😍😍😍   #harrypotter #pottermore #favorite</td>\n      <td>child book already yes harrypotter favorite</td>\n      <td>0</td>\n      <td>[#harrypotter, #pottermore, #favorite]</td>\n      <td>:smiling_face_with_heart-eyes:,:smiling_face_with_heart-eyes:,:smiling_face_with_heart-eyes:</td>\n    </tr>\n    <tr>\n      <th>31967</th>\n      <td>3rd #bihday to my amazing, hilarious #nephew eli ahmir! uncle dave loves you and misses…</td>\n      <td>rd bihday amazing hilarious nephew uncle dave miss</td>\n      <td>0</td>\n      <td>[#bihday, #nephew]</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned_predict.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:17:04.373260900Z",
     "start_time": "2024-12-02T16:17:04.356263200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "df_cleaned_predict.to_csv('./twitter_hate-speech/predict_cleaned.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:17:04.543254600Z",
     "start_time": "2024-12-02T16:17:04.370261200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Geeignet für: Deep Learning (RNN)\n",
    "es werden nicht alle implementierten Cleaning und Preprocessing Schritte ausgeführt\n",
    "\n",
    "NICHT: Duplikate entfernen, Auflösen Negationen (nicht implementiert), Emojis Entfernen, Lemmatisierung, Stopwords entfernen, Most/Least frequent words entfernen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "df_origin_predict_rnn = pd.read_csv('./twitter_hate-speech/predict.csv', index_col=0)\n",
    "df_clean_base_predict_rnn = df_origin_predict_rnn.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:17:04.605255900Z",
     "start_time": "2024-12-02T16:17:04.515258300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "df_clean_base_predict_rnn.drop_duplicates(inplace=True)\n",
    "df_clean_base_predict_rnn[\"tweet\"] = df_clean_base_predict_rnn['tweet'].apply(fix_encoding)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:17:05.852222900Z",
     "start_time": "2024-12-02T16:17:04.562258Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "def clean_dataframe_predict_rnn(base_df):\n",
    "    i = 1\n",
    "    count = 20\n",
    "    df_cleaned = base_df.copy()\n",
    "    df_cleaned['tweet_cleaned'] = df_cleaned['tweet']\n",
    "\n",
    "    print(\"Start Cleaning\")\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: to_lowercase\")\n",
    "    df_cleaned = to_lowercase(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: expand_shortcuts\")\n",
    "    df_cleaned = expand_shortcuts(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_negations - SKIP\")\n",
    "    # # df_cleaned = remove_negations(df_cleaned)\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: handle_userhandles\")\n",
    "    df_cleaned = handle_userhandles(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: handle_hashtags\")\n",
    "    df_cleaned = handle_hashtags(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: extract_emojis\")\n",
    "    df_cleaned = extract_emojis(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: replace_emojis\")\n",
    "    df_cleaned = replace_emojis(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: replace_smileys\")\n",
    "    df_cleaned = replace_text_smileys(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_emojis - SKIP\")\n",
    "    # df_cleaned = remove_emojis(df_cleaned)\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_url_from_tweet\")\n",
    "    df_cleaned = remove_url_from_tweet(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_punctuation\")\n",
    "    df_cleaned = remove_punctuation(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_special_characters\")\n",
    "    df_cleaned = remove_special_characters(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_digis\")\n",
    "    df_cleaned = remove_digits(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_word_from_column: amp\")\n",
    "    df_cleaned = remove_word_from_column(df=df_cleaned, column_name=\"tweet_cleaned\", word=\"amp\")\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: lemmatize - SKIP\")\n",
    "    # df_cleaned = lemmatize(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_stop_words - SKIP\")\n",
    "    # df_cleaned = remove_stop_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_most_frequent_words - SKIP\")\n",
    "    # df_cleaned = remove_most_frequent_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_least_frequent_words - SKIP\")\n",
    "    # df_cleaned = remove_least_frequent_words(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_duplicates  - SKIP\")\n",
    "    # df_cleaned = remove_duplicates(df_cleaned, 'tweet_cleaned')\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"Cleaning Step {i}/{count}: remove_nans\")\n",
    "    df_cleaned = remove_na_from_column(df=df_cleaned, column_name=\"tweet_cleaned\")\n",
    "\n",
    "    print(\"All Cleaning done\")\n",
    "\n",
    "    return df_cleaned"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:17:05.907203200Z",
     "start_time": "2024-12-02T16:17:05.865205900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cleaning\n",
      "Cleaning Step 1/20: to_lowercase\n",
      "Cleaning Step 2/20: expand_shortcuts\n",
      "Cleaning Step 3/20: remove_negations - SKIP\n",
      "Cleaning Step 4/20: handle_userhandles\n",
      "Cleaning Step 5/20: handle_hashtags\n",
      "Cleaning Step 6/20: extract_emojis\n",
      "Cleaning Step 7/20: replace_emojis\n",
      "Cleaning Step 8/20: replace_smileys\n",
      "Cleaning Step 9/20: remove_emojis - SKIP\n",
      "Cleaning Step 10/20: remove_url_from_tweet\n",
      "Cleaning Step 11/20: remove_punctuation\n",
      "Cleaning Step 12/20: remove_special_characters\n",
      "Cleaning Step 13/20: remove_digis\n",
      "Cleaning Step 14/20: remove_word_from_column: amp\n",
      "Cleaning Step 15/20: lemmatize - SKIP\n",
      "Cleaning Step 16/20: remove_stop_words - SKIP\n",
      "Cleaning Step 17/20: remove_most_frequent_words - SKIP\n",
      "Cleaning Step 18/20: remove_least_frequent_words - SKIP\n",
      "Cleaning Step 19/20: remove_duplicates  - SKIP\n",
      "Cleaning Step 20/20: remove_nans\n",
      "All Cleaning done\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_predict_rnn = clean_dataframe_predict_rnn(df_clean_base_predict_rnn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:17:10.321045Z",
     "start_time": "2024-12-02T16:17:05.883205200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                                                                                       tweet  \\\nid                                                                                                                                             \n31963                                               #studiolife #aislife #requires #passion #dedication #willpower   to find #newmaterials…    \n31964                                           @user #white #supremacists want everyone to see the new ‘  #birds’ #movie — and here’s why     \n31965                                                                safe ways to heal your #acne!!    #altwaystoheal #healthy   #healing!!    \n31966  is the hp and the cursed child book up for reservations already? if yes, where? if no, when? 😍😍😍   #harrypotter #pottermore #favorite   \n31967                                              3rd #bihday to my amazing, hilarious #nephew eli ahmir! uncle dave loves you and misses…    \n\n                                                                                                                                                                                                        tweet_cleaned  \\\nid                                                                                                                                                                                                                      \n31963                                                                                                                                  studiolife aislife requires passion dedication willpower to find newmaterials…   \n31964                                                                                                                                 white supremacists want everyone to see the new ‘ birds’ movie — and here’s why   \n31965                                                                                                                                                       safe ways to heal your acne altwaystoheal healthy healing   \n31966  is the hp and the cursed child book up for reservations already if yes where if no when smiling face with heart-eyes smiling face with heart-eyes smiling face with heart-eyes harrypotter pottermore favorite   \n31967                                                                                                                             rd bihday to my amazing hilarious nephew eli ahmir uncle dave loves you and misses…   \n\n       user_handle  \\\nid                   \n31963            0   \n31964            1   \n31965            0   \n31966            0   \n31967            0   \n\n                                                                                   hashtags  \\\nid                                                                                            \n31963  [#studiolife, #aislife, #requires, #passion, #dedication, #willpower, #newmaterials]   \n31964                                               [#white, #supremacists, #birds, #movie]   \n31965                                           [#acne, #altwaystoheal, #healthy, #healing]   \n31966                                                [#harrypotter, #pottermore, #favorite]   \n31967                                                                    [#bihday, #nephew]   \n\n                                                                                             emojis  \nid                                                                                                   \n31963                                                                                                \n31964                                                                                                \n31965                                                                                                \n31966  :smiling_face_with_heart-eyes:,:smiling_face_with_heart-eyes:,:smiling_face_with_heart-eyes:  \n31967                                                                                                ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>tweet_cleaned</th>\n      <th>user_handle</th>\n      <th>hashtags</th>\n      <th>emojis</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>31963</th>\n      <td>#studiolife #aislife #requires #passion #dedication #willpower   to find #newmaterials…</td>\n      <td>studiolife aislife requires passion dedication willpower to find newmaterials…</td>\n      <td>0</td>\n      <td>[#studiolife, #aislife, #requires, #passion, #dedication, #willpower, #newmaterials]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>31964</th>\n      <td>@user #white #supremacists want everyone to see the new ‘  #birds’ #movie — and here’s why</td>\n      <td>white supremacists want everyone to see the new ‘ birds’ movie — and here’s why</td>\n      <td>1</td>\n      <td>[#white, #supremacists, #birds, #movie]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>31965</th>\n      <td>safe ways to heal your #acne!!    #altwaystoheal #healthy   #healing!!</td>\n      <td>safe ways to heal your acne altwaystoheal healthy healing</td>\n      <td>0</td>\n      <td>[#acne, #altwaystoheal, #healthy, #healing]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>31966</th>\n      <td>is the hp and the cursed child book up for reservations already? if yes, where? if no, when? 😍😍😍   #harrypotter #pottermore #favorite</td>\n      <td>is the hp and the cursed child book up for reservations already if yes where if no when smiling face with heart-eyes smiling face with heart-eyes smiling face with heart-eyes harrypotter pottermore favorite</td>\n      <td>0</td>\n      <td>[#harrypotter, #pottermore, #favorite]</td>\n      <td>:smiling_face_with_heart-eyes:,:smiling_face_with_heart-eyes:,:smiling_face_with_heart-eyes:</td>\n    </tr>\n    <tr>\n      <th>31967</th>\n      <td>3rd #bihday to my amazing, hilarious #nephew eli ahmir! uncle dave loves you and misses…</td>\n      <td>rd bihday to my amazing hilarious nephew eli ahmir uncle dave loves you and misses…</td>\n      <td>0</td>\n      <td>[#bihday, #nephew]</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned_predict_rnn.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:17:10.373039Z",
     "start_time": "2024-12-02T16:17:10.323044700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "df_cleaned_predict_rnn.to_csv('./twitter_hate-speech/predict_cleaned_rnn.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:17:10.531034400Z",
     "start_time": "2024-12-02T16:17:10.336042300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T16:17:10.532034900Z",
     "start_time": "2024-12-02T16:17:10.512035100Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
