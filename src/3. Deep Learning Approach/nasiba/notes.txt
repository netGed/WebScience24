https://gpt5.blog/glove-global-vectors-for-word-representation/
https://nlp.stanford.edu/projects/glove/

word2vec hat gr√∂√üere Vector Size nicht gut funktioniert
Vektor Size √Ñnderung bei 

Bei Glove wurden keine Durschnittsvektoren sondern die richtigen Vektoren genommen

Warum wird One-Hot-Encoding verwendet?
‚úÖ Notwendig f√ºr neuronale Netze: Viele Machine-Learning-Modelle (z. B. neuronale Netze mit Softmax-Output) erwarten die Klassen als One-Hot-Vektoren.
‚úÖ Verhindert falsche numerische Interpretationen: Ein klassisches y_train = [0, 1, 2] k√∂nnte sonst als kontinuierliche Werte interpretiert werden.
‚úÖ Erleichtert die Berechnung des Fehlers (Loss Function): One-Hot-Vektoren werden oft mit der Kreuzentropie (categorical crossentropy) verwendet.
to_categorical() wandelt Labels in One-Hot-Format um.
Wird in klassifizierenden neuronalen Netzen ben√∂tigt.
Verhindert, dass das Modell f√§lschlicherweise Klassen als Zahlen mit Reihenfolge interpretiert.
Ist besonders wichtig bei mehrklassigen Klassifikationsproblemen mit Softmax-Aktivierung.

Problem von RNNs: Vanishing Gradient
Beim Training mit Backpropagation treten oft zwei Probleme auf:

Vanishing Gradient (verschwindender Gradient)
Wenn die Gradienten in tiefen Netzwerken immer kleiner werden, k√∂nnen fr√ºhere Zust√§nde kaum mehr beeinflusst werden. Das macht es schwer, langfristige Abh√§ngigkeiten zu lernen.
Exploding Gradient (explodierender Gradient)
Umgekehrt k√∂nnen die Gradienten in manchen F√§llen zu gro√ü werden, was das Training instabil macht.
üëé Standard-RNNs sind daher nicht gut f√ºr lange Sequenzen geeignet.
L√∂sung: LSTM (Long Short-Term Memory)

Evaulation:
Recall speziel f√ºr Klasse 1 festlegen: muss man in sigmoid nicht, da er dies automatisch macht

Precision (Genauigkeit der positiven Vorhersagen)
Class 0: 0.98 ‚Üí 98 % der vorhergesagten Class-0-F√§lle sind tats√§chlich korrekt.
Class 1: 0.88 ‚Üí 88 % der vorhergesagten Class-1-F√§lle sind tats√§chlich korrekt.

Recall (Empfindlichkeit / Wie viele Positive wurden erkannt?)
Class 0: 0.99 ‚Üí 99 % der tats√§chlichen Class-0-F√§lle wurden richtig vorhergesagt.
Class 1: 0.66 ‚Üí Nur 66 % der tats√§chlichen Class-1-F√§lle wurden richtig erkannt.
‚û° Der niedrige Recall f√ºr Class 1 bedeutet, dass 34 % der tats√§chlichen positiven F√§lle nicht erkannt wurden (False Negatives). Das Modell hat also Schwierigkeiten, alle Hate-Speech-F√§lle zu erkennen.

F1-Score (Harm. Mittelwert von Precision & Recall)
Class 0: 0.99 ‚Üí Sehr gut, da Precision & Recall hoch sind.
Class 1: 0.76 ‚Üí Mittelm√§√üig, da Recall niedrig ist.
‚û° Der niedrige F1-Score f√ºr Class 1 zeigt, dass das Modell nicht optimal darin ist, alle Hate-Speech-F√§lle korrekt zu erkennen.


F√ºr sigmoid ben√∂tigt man kein one hot coding, w√§hrend f√ºr softmax hot coding braucht

Durch class_weight wurde an model mit sigmoid eine neu gewichtete Klassen √ºbergeben

MEhr Neuronen mit sigmoid wie 256 hat ohne class_weight gut funktioniert, mit class_weight war es gleich 0

Verwenden einfach Recall() ohne class_id, da TensorFlow bei einem bin√§ren Klassifikationsproblem automatisch den Recall f√ºr Klasse 1 berechnet. Dadurch kannst du den Recall in jeder Epoche direkt in der history-Variable verfolgen, ohne One-Hot-Encoding der Labels.


EarlyStopping:
√úberwacht den Validierungsverlust (val_loss) w√§hrend des Trainings.
Falls sich der val_loss √ºber 5 aufeinanderfolgende Epochen nicht verbessert, wird das Training automatisch gestoppt.
Mit restore_best_weights=True stellt es nach dem Abbruch die besten Gewichte (die mit dem niedrigsten val_loss) wieder her.
Warum ist das wichtig?
Verhindert Overfitting: Falls das Modell zu lange trainiert, passt es sich zu stark an die Trainingsdaten an.
Spart Rechenzeit: Wenn das Modell bereits optimal ist, stoppt es das Training fr√ºher.

ReduceLROnPlateau:
Was macht es?
√úberwacht ebenfalls den Validierungsverlust (val_loss).
Falls sich der val_loss √ºber 3 Epochen nicht verbessert, wird die Lernrate (learning rate) halbiert (factor=0.5).
Die minimale Lernrate ist auf 0.00001 begrenzt.
Warum ist das wichtig?
Verhindert Instabilit√§t: Falls das Modell zu gro√üe Spr√ºnge macht, kann eine kleinere Lernrate helfen, das Modell feiner zu justieren.
Verbessert Konvergenz: Falls das Modell in einem lokalen Minimum feststeckt, kann eine kleinere Lernrate das Feintuning verbessern.
