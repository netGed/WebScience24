https://gpt5.blog/glove-global-vectors-for-word-representation/
https://nlp.stanford.edu/projects/glove/

word2vec hat gr√∂√üere Vector Size nicht gut funktioniert
Vektor Size √Ñnderung bei 

Bei Glove wurden keine Durschnittsvektoren sondern die richtigen Vektoren genommen

Warum wird One-Hot-Encoding verwendet?
‚úÖ Notwendig f√ºr neuronale Netze: Viele Machine-Learning-Modelle (z. B. neuronale Netze mit Softmax-Output) erwarten die Klassen als One-Hot-Vektoren.
‚úÖ Verhindert falsche numerische Interpretationen: Ein klassisches y_train = [0, 1, 2] k√∂nnte sonst als kontinuierliche Werte interpretiert werden.
‚úÖ Erleichtert die Berechnung des Fehlers (Loss Function): One-Hot-Vektoren werden oft mit der Kreuzentropie (categorical crossentropy) verwendet.
to_categorical() wandelt Labels in One-Hot-Format um.
Wird in klassifizierenden neuronalen Netzen ben√∂tigt.
Verhindert, dass das Modell f√§lschlicherweise Klassen als Zahlen mit Reihenfolge interpretiert.
Ist besonders wichtig bei mehrklassigen Klassifikationsproblemen mit Softmax-Aktivierung.

Problem von RNNs: Vanishing Gradient
Beim Training mit Backpropagation treten oft zwei Probleme auf:

Vanishing Gradient (verschwindender Gradient)
Wenn die Gradienten in tiefen Netzwerken immer kleiner werden, k√∂nnen fr√ºhere Zust√§nde kaum mehr beeinflusst werden. Das macht es schwer, langfristige Abh√§ngigkeiten zu lernen.
Exploding Gradient (explodierender Gradient)
Umgekehrt k√∂nnen die Gradienten in manchen F√§llen zu gro√ü werden, was das Training instabil macht.
üëé Standard-RNNs sind daher nicht gut f√ºr lange Sequenzen geeignet.
L√∂sung: LSTM (Long Short-Term Memory)