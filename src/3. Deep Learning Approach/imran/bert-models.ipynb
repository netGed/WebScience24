{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KsLnOMYzYf4G"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from sklearn.metrics import f1_score\n","from tqdm.notebook import tqdm\n","import random\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZ-nt0upZ0F4","executionInfo":{"status":"ok","timestamp":1735052367061,"user_tz":-60,"elapsed":27143,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}},"outputId":"efb2a53f-145c-4132-ec59-d7a404755f45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBTp8AUSYf4J"},"outputs":[],"source":["train_cleaned_data = pd.read_csv(\"drive/MyDrive/Colab Notebooks/data/train_cleaned_rnn.csv\")\n","test_cleaned_data = pd.read_csv(\"drive/MyDrive/Colab Notebooks/data/test_cleaned_rnn.csv\")\n","\n","X_train = train_cleaned_data['tweet_cleaned'].values\n","y_train = train_cleaned_data['label'].values\n","\n","X_test = test_cleaned_data['tweet_cleaned'].values\n","y_test = test_cleaned_data['label'].values"]},{"cell_type":"code","source":["# Tokenizer initialisieren\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","# Daten tokenisieren und in Tensoren umwandeln\n","def encode_data(texts, labels, tokenizer, max_length=256):\n","    encoded = tokenizer.batch_encode_plus(\n","        texts,\n","        add_special_tokens=True,\n","        return_attention_mask=True,\n","        pad_to_max_length=True,\n","        max_length=max_length,\n","        return_tensors='pt'\n","    )\n","    input_ids = encoded['input_ids']\n","    attention_masks = encoded['attention_mask']\n","    labels = torch.tensor(labels)\n","    return TensorDataset(input_ids, attention_masks, labels)\n","\n","train_data = encode_data(X_train, y_train, tokenizer)\n","test_data = encode_data(X_test, y_test, tokenizer)\n","\n","# DataLoader erstellen\n","batch_size = 16\n","\n","train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n","test_dataloader = DataLoader(test_data, sampler=RandomSampler(test_data), batch_size=batch_size)\n","\n","# Modell initialisieren\n","model = BertForSequenceClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels=2,  # Bin√§re Klassifikation\n","    output_attentions=False,\n","    output_hidden_states=False\n",")\n","\n","# Optimizer und Scheduler einrichten\n","optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n","epochs = 5\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=len(train_dataloader) * epochs\n",")\n","\n","# Seed setzen\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Training-Funktion\n","def train_model(dataloader, model, optimizer, scheduler, device):\n","    model.train()\n","    total_loss = 0\n","    for batch in dataloader:\n","        model.zero_grad()\n","        batch = tuple(b.to(device) for b in batch)\n","        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","        outputs = model(**inputs)\n","        loss = outputs[0]\n","        total_loss += loss.item()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","    return total_loss / len(dataloader)\n","\n","# Evaluation-Funktion\n","def evaluate(dataloader, model, device):\n","    model.eval()\n","    total_loss = 0\n","    predictions, true_labels = [], []\n","    for batch in dataloader:\n","        batch = tuple(b.to(device) for b in batch)\n","        with torch.no_grad():\n","            outputs = model(input_ids=batch[0], attention_mask=batch[1], labels=batch[2])\n","        loss = outputs[0]\n","        logits = outputs[1]\n","        total_loss += loss.item()\n","        predictions.append(logits.detach().cpu().numpy())\n","        true_labels.append(batch[2].detach().cpu().numpy())\n","    predictions = np.concatenate(predictions, axis=0)\n","    true_labels = np.concatenate(true_labels, axis=0)\n","    return total_loss / len(dataloader), predictions, true_labels\n","\n","# Training und Evaluation\n","for epoch in range(1, epochs + 1):\n","    print(f\"Epoch {epoch}\")\n","    train_loss = train_model(train_dataloader, model, optimizer, scheduler, device)\n","    print(f\"Training loss: {train_loss}\")\n","    val_loss, predictions, true_labels = evaluate(test_dataloader, model, device)\n","    preds_flat = np.argmax(predictions, axis=1).flatten()\n","    labels_flat = true_labels.flatten()\n","    f1 = f1_score(labels_flat, preds_flat, average='weighted')\n","    print(f\"Validation loss: {val_loss}\")\n","    print(f\"F1 Score: {f1}\")\n"],"metadata":{"id":"S8Z24ItnAV6y"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}