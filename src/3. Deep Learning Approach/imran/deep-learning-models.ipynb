{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "import nltk\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU,SimpleRNN\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import vectorize_functions\n",
    "import resampler\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_data = pd.read_csv(\"..\\\\..\\\\..\\\\data\\\\twitter_hate-speech\\\\train_basic_cleaned.csv\")\n",
    "test_cleaned_data = pd.read_csv(\"..\\\\..\\\\..\\\\data\\\\twitter_hate-speech\\\\test_basic_cleaned.csv\")\n",
    "\n",
    "X_train_data = train_cleaned_data['tweet']\n",
    "y_train_data = train_cleaned_data['label']\n",
    "\n",
    "X_test_data = test_cleaned_data['tweet']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_data)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "xtrain_seq = tokenizer.texts_to_sequences(X_train_data)\n",
    "xtest_seq = tokenizer.texts_to_sequences(X_test_data)\n",
    "\n",
    "max_len = max(len(seq) for seq in xtrain_seq) \n",
    "xtrain_pad = pad_sequences(xtrain_seq, padding='post', maxlen=max_len)\n",
    "xtest_pad = pad_sequences(xtest_seq, padding='post', maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed text @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\n",
      "\n",
      "Tokenized text [1, 1, 165, 9, 5485, 2484, 5, 64, 490, 638, 65, 70, 1485, 7896, 10268, 8, 7897, 16187, 10269]\n",
      "\n",
      "Padded text [    1     1   165     9  5485  2484     5    64   490   638    65    70\n",
      "  1485  7896 10268     8  7897 16187 10269     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0]\n",
      "\n",
      "Padded text Length:  42\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessed text', X_train_data[1])\n",
    "print('\\nTokenized text', xtrain_seq[1])\n",
    "print('\\nPadded text', xtrain_pad[1])\n",
    "print('\\nPadded text Length: ', len(xtrain_pad[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(xtrain_pad, y_train_data, test_size=0.2, random_state=42)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path, embedding_dim, word_index):\n",
    "    embedding_index = {}\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_index, embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "zip_file_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "r = requests.get(zip_file_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "zip_file_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\n",
    "r = requests.get(zip_file_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "def train_rnn(x_train, y_train, x_val, y_val, word_index, max_len, embedding_matrix=None, embedding_dim=300, epochs=5, batch_size=64):\n",
    "    model = Sequential()\n",
    "    if embedding_matrix is not None:\n",
    "        model.add(Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "    else:\n",
    "        model.add(Embedding(len(word_index) + 1, embedding_dim, input_length=max_len))\n",
    "    model.add(SimpleRNN(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))\n",
    "    return model, history\n",
    "\n",
    "def train_lstm(x_train, y_train, x_val, y_val, word_index, max_len, embedding_matrix=None, embedding_dim=300, epochs=5, batch_size=64):\n",
    "    model = Sequential()\n",
    "    if embedding_matrix is not None:\n",
    "        model.add(Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "    else:\n",
    "        model.add(Embedding(len(word_index) + 1, embedding_dim, input_length=max_len))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))\n",
    "    return model, history\n",
    "\n",
    "def train_gru(x_train, y_train, x_val, y_val, word_index, max_len, embedding_matrix=None, embedding_dim=300, epochs=5, batch_size=64):\n",
    "    model = Sequential()\n",
    "    if embedding_matrix is not None:\n",
    "        model.add(Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "    else:\n",
    "        model.add(Embedding(len(word_index) + 1, embedding_dim, input_length=max_len))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(GRU(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file_path = 'glove.6B.300d.txt'\n",
    "fasttext_file_path = 'wiki-news-300d-1M.vec'\n",
    "embedding_dim = 300\n",
    "glove_embedding_index, glove_embedding_matrix = load_embeddings(glove_file_path, embedding_dim, word_index)\n",
    "fasttext_embedding_index, fasttext_embedding_matrix = load_embeddings(fasttext_file_path, embedding_dim, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training LSTM with GloVe Embeddings\")\n",
    "lstm_glove, lstm_glove_history = train_lstm(\n",
    "    X_train_resampled, y_train_resampled, X_val, y_val, \n",
    "    word_index, max_len, embedding_matrix=glove_embedding_matrix)\n",
    "\n",
    "print(\"Training LSTM with FastText Embeddings\")\n",
    "lstm_fasttext, lstm_fasttext_history = train_lstm(\n",
    "    X_train_resampled, y_train_resampled, X_val, y_val, \n",
    "    word_index, max_len, embedding_matrix=fasttext_embedding_matrix)\n",
    "\n",
    "print(\"Training LSTM with Word2Vec Embeddings\")\n",
    "lstm_word2vec, lstm_word2vec_history = train_lstm(\n",
    "    X_train_resampled, y_train_resampled, X_val, y_val, \n",
    "    word_index, max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training GRU with GloVe Embeddings\")\n",
    "gru_glove, gru_glove_history = train_gru(\n",
    "    X_train_resampled, y_train_resampled, X_val, y_val, \n",
    "    word_index, max_len, embedding_matrix=glove_embedding_matrix)\n",
    "\n",
    "print(\"Training GRU with FastText Embeddings\")\n",
    "gru_fasttext, gru_fasttext_history = train_gru(\n",
    "    X_train_resampled, y_train_resampled, X_val, y_val, \n",
    "    word_index, max_len, embedding_matrix=fasttext_embedding_matrix)\n",
    "\n",
    "print(\"Training GRU with Word2Vec Embeddings\")\n",
    "gru_word2vec, gru_word2vec_history = train_gru(\n",
    "    X_train_resampled, y_train_resampled, X_val, y_val, \n",
    "    word_index, max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, history, X_val, y_val, X_test=None, test_data=None, output_file=None):\n",
    "    \"\"\"\n",
    "    Evaluates the model performance on validation data and predicts labels on test data (if provided).\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained model to evaluate.\n",
    "    - history: Training history object for visualization of metrics.\n",
    "    - X_val: Validation data (features).\n",
    "    - y_val: Validation data (labels).\n",
    "    - X_test: Test data (features, optional, for prediction).\n",
    "    - test_data: Original test data (optional, for saving predictions with original tweets).\n",
    "    - output_file: File path to save test predictions (optional).\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Step 1: Plot Training and Validation Accuracy and Loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Step 2: Evaluate on Validation Data\n",
    "    print(\"\\nEvaluating on Validation Data...\")\n",
    "    y_val_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    roc_auc = roc_auc_score(y_val, model.predict(X_val))\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Hate Speech', 'Hate Speech'], yticklabels=['No Hate Speech', 'Hate Speech'])\n",
    "    plt.title('Confusion Matrix - Validation Data')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "    \n",
    "    # Step 3: Predict on Test Data (if provided)\n",
    "    if X_test is not None:\n",
    "        print(\"\\nPredicting on Test Data...\")\n",
    "        test_predictions = model.predict(X_test)\n",
    "        predicted_labels = (test_predictions > 0.5).astype(int)\n",
    "        \n",
    "        # Save predictions to CSV (if test_data and output_file are provided)\n",
    "        if test_data is not None and output_file is not None:\n",
    "            results = pd.DataFrame({\n",
    "                'tweet': test_data['tweet'],\n",
    "                'predicted_label': predicted_labels.flatten()\n",
    "            })\n",
    "            results.to_csv(output_file, index=False)\n",
    "            print(f\"Test predictions saved to {output_file}\")\n",
    "        else:\n",
    "            print(\"Test predictions are ready (not saved).\")\n",
    "        \n",
    "        return predicted_labels\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluate_model(\n",
    "    gru_glove, gru_glove_history, X_val, y_val, \n",
    "    X_test=xtest_pad, test_data=test_cleaned_data, output_file=\"gru_glove_predictions.csv\"\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    gru_fasttext, gru_fasttext_history, X_val, y_val, \n",
    "    X_test=xtest_pad, test_data=test_cleaned_data, output_file=\"gru_fasttext_predictions.csv\"\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    gru_word2vec, gru_word2vec_history, X_val, y_val, \n",
    "    X_test=xtest_pad, test_data=test_cleaned_data, output_file=\"gru_word2vec_predictions.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\n",
    "    lstm_glove, lstm_glove_history, X_val, y_val, \n",
    "    X_test=xtest_pad, test_data=test_cleaned_data, output_file=\"lstm_glove_predictions.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "evaluate_model(\n",
    "    lstm_fasttext, lstm_fasttext_history, X_val, y_val, \n",
    "    X_test=xtest_pad, test_data=test_cleaned_data, output_file=\"lstm_fasttext_predictions.csv\"\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    lstm_word2vec, lstm_word2vec_history, X_val, y_val, \n",
    "    X_test=xtest_pad, test_data=test_cleaned_data, output_file=\"lstm_word2vec_predictions.csv\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
