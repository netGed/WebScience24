{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"KsLnOMYzYf4G","executionInfo":{"status":"ok","timestamp":1735052339922,"user_tz":-60,"elapsed":5166,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import imread\n","import nltk\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import f1_score, accuracy_score\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","import codecs\n","from tqdm import tqdm\n","import shutil\n","\n","\n","\n","\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import RandomOverSampler\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import LSTM, GRU,SimpleRNN\n","from keras.layers import Dense, Activation, Dropout\n","from keras.layers import Embedding\n","from keras.layers import BatchNormalization\n","from keras.utils import to_categorical\n","from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n","from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n","from keras.callbacks import EarlyStopping\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold\n","from imblearn.over_sampling import SMOTE\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","import tensorflow as tf\n","from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_curve, auc\n","from sklearn.utils.class_weight import compute_class_weight\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from nltk.stem.porter import PorterStemmer\n","from keras.callbacks import EarlyStopping\n","\n","import re\n","from tensorflow.keras.preprocessing.text import one_hot\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZ-nt0upZ0F4","executionInfo":{"status":"ok","timestamp":1735052367061,"user_tz":-60,"elapsed":27143,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}},"outputId":"efb2a53f-145c-4132-ec59-d7a404755f45"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"uBTp8AUSYf4J","executionInfo":{"status":"ok","timestamp":1735052370048,"user_tz":-60,"elapsed":2992,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}}},"outputs":[],"source":["train_cleaned_data = pd.read_csv(\"drive/MyDrive/Colab Notebooks/data/train_cleaned_rnn.csv\")\n","test_cleaned_data = pd.read_csv(\"drive/MyDrive/Colab Notebooks/data/test_cleaned_rnn.csv\")\n","\n","X_train_data = train_cleaned_data['tweet_cleaned']\n","y_train_data = train_cleaned_data['label']\n","\n","X_test_data = test_cleaned_data['tweet_cleaned']\n","y_test_data = test_cleaned_data['label']\n"]},{"cell_type":"markdown","source":["### Textvorverarbeitung"],"metadata":{"id":"-Tr4YuGlupp7"}},{"cell_type":"code","source":["def one_hot_enc(X_data, y_data):\n","  corpus = [str(text) for text in X_data]\n","  vocab_size = 5000\n","\n","  #one hot encoding\n","  one_hot_dir = [one_hot(words,vocab_size) for words in corpus]\n","\n","  embedded_layer = pad_sequences(one_hot_dir,padding = 'pre')\n","  return np.array(embedded_layer), np.array(y_data)"],"metadata":{"id":"FugTwaCh-JUB","executionInfo":{"status":"ok","timestamp":1735052412779,"user_tz":-60,"elapsed":494,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def tokenize(X_data, y_data, tokenizer=None, max_len=40):\n","    X_data = [str(x) for x in X_data]\n","\n","    # # Wenn kein Tokenizer bereitgestellt wird, erstelle einen neuen\n","    if tokenizer is None:\n","        tokenizer = Tokenizer(num_words=None, oov_token=\"<OOV>\")\n","        tokenizer.fit_on_texts(X_data)\n","\n","    # # Tokenisierung\n","    X_data_sequences = tokenizer.texts_to_sequences(X_data)\n","\n","    X_data_padded_sequences = pad_sequences(X_data_sequences, padding='post', maxlen=max_len)\n","    return X_data_padded_sequences, y_data, tokenizer\n"],"metadata":{"id":"SEK60o864rr2","executionInfo":{"status":"ok","timestamp":1735052415028,"user_tz":-60,"elapsed":539,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### Sampling"],"metadata":{"id":"8PAsw3AIwZwe"}},{"cell_type":"code","source":["def random_over_sample(X_data, y_data):\n","  ros = RandomOverSampler(random_state=42)\n","  X = X_data.values.reshape(-1, 1)\n","  y = y_data.values\n","\n","  X_resampled, y_resampled = ros.fit_resample(X, y)\n","  X_resampled = X_resampled[:, 0]\n","  return X_resampled, y_resampled\n"],"metadata":{"id":"FMn2kBpkusB4","executionInfo":{"status":"ok","timestamp":1735052417924,"user_tz":-60,"elapsed":445,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o2jE9QDeYf4N"},"source":["### Embeddings"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"EB-0iPjNYf4O","executionInfo":{"status":"ok","timestamp":1735052420741,"user_tz":-60,"elapsed":501,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}}},"outputs":[],"source":["def load_embeddings(file_path, embedding_dim, word_index):\n","    embedding_index = {}\n","    with open(file_path, encoding='utf-8') as f:\n","        for line in f:\n","            values = line.rstrip().split(' ')\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embedding_index[word] = coefs\n","\n","    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n","    for word, i in word_index.items():\n","        embedding_vector = embedding_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","\n","    return embedding_matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtLRyoM5Yf4O"},"outputs":[],"source":["import requests, zipfile, io\n","zip_file_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n","r = requests.get(zip_file_url)\n","z = zipfile.ZipFile(io.BytesIO(r.content))\n","z.extractall()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHn_3bm_Yf4P"},"outputs":[],"source":["import requests, zipfile, io\n","zip_file_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\n","r = requests.get(zip_file_url)\n","z = zipfile.ZipFile(io.BytesIO(r.content))\n","z.extractall()"]},{"cell_type":"markdown","metadata":{"id":"SG6Ry5glYf4P"},"source":["### Model"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"h8pOnEw_Yf4P","executionInfo":{"status":"ok","timestamp":1735054964319,"user_tz":-60,"elapsed":452,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}}},"outputs":[],"source":["def train_gru(x_train, y_train, x_val, y_val, vocab_size, max_len, embedding_matrix=None, embedding_dim=200, epochs=20, batch_size=32):\n","    model = Sequential()\n","    if embedding_matrix is not None:\n","        model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n","    else:\n","        model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n","    model.add(SpatialDropout1D(0.3))\n","    model.add(GRU(300))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n","\n","\n","    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))\n","    return model, history\n"]},{"cell_type":"code","source":["def train_gru_optimized(x_train, y_train, x_val, y_val, vocab_size, max_len, embedding_matrix=None, embedding_dim=300, epochs=5, batch_size=32, class_weights=None):\n","    model = Sequential()\n","    if embedding_matrix is not None:\n","        model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n","    else:\n","        model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n","    model.add(SpatialDropout1D(0.4))\n","    model.add(GRU(256, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy', 'Recall'])\n","    # model.compile(loss=focal_loss(alpha=0.5, gamma=2), optimizer=Adam(), metrics=['accuracy', 'Recall'])\n","\n","\n","    # Early Stopping Callback hinzuf√ºgen\n","    early_stopping = EarlyStopping(\n","        monitor='val_loss',\n","        patience=3,\n","        restore_best_weights=True\n","    )\n","\n","    history = model.fit(\n","        x_train,\n","        y_train,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        validation_data=(x_val, y_val),\n","        callbacks=[early_stopping]\n","    )\n","    return model, history\n","\n","def focal_loss(gamma=2., alpha=0.25):\n","    def focal_loss_fixed(y_true, y_pred):\n","        epsilon = tf.keras.backend.epsilon()\n","        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n","        y_true = tf.cast(y_true, tf.float32)\n","        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n","        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n","        loss = -alpha_t * tf.math.pow((1 - p_t), gamma) * tf.math.log(p_t)\n","        return tf.reduce_mean(loss)\n","    return focal_loss_fixed"],"metadata":{"id":"pxix7Iff3ymG","executionInfo":{"status":"ok","timestamp":1735060253956,"user_tz":-60,"elapsed":425,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5b5c8Aq7Yf4R"},"source":["### Evaluation"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"k5weY2AlYf4R","executionInfo":{"status":"ok","timestamp":1735052430743,"user_tz":-60,"elapsed":471,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}}},"outputs":[],"source":["def evaluate_on_val(model, history, X_val, y_val, threshold=0.35):\n","    \"\"\"\n","    Evaluates the model on validation data with detailed metrics and visualizes training performance.\n","\n","    Parameters:\n","    - model: Trained model to evaluate.\n","    - history: Training history object.\n","    - X_val: Validation data (features).\n","    - y_val: Validation data (labels).\n","    - threshold: Decision threshold to classify an instance as positive.\n","\n","    Returns:\n","    None\n","    \"\"\"\n","    # Plot Training and Validation Metrics\n","    plt.figure(figsize=(12, 6))\n","\n","    # Accuracy\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['accuracy'], label='Training Accuracy')\n","    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","    plt.title('Model Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    # Loss\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.title('Model Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Evaluate Validation Data\n","    print(\"\\nValidation Performance:\")\n","    y_val_pred = (model.predict(X_val) > threshold).astype(int)\n","\n","    accuracy = accuracy_score(y_val, y_val_pred)\n","    precision = precision_score(y_val, y_val_pred)\n","    recall = recall_score(y_val, y_val_pred)\n","    f1 = f1_score(y_val, y_val_pred)\n","    # roc_auc = roc_auc_score(y_val, model.predict(X_val))\n","\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","    # print(f\"ROC AUC: {roc_auc:.4f}\")\n","\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_val, y_val_pred))\n","\n","    # Confusion Matrix\n","    conf_matrix = confusion_matrix(y_val, y_val_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Hate Speech', 'Hate Speech'], yticklabels=['No Hate Speech', 'Hate Speech'])\n","    plt.title('Confusion Matrix - Validation Data')\n","    plt.xlabel('Predicted Labels')\n","    plt.ylabel('True Labels')\n","    plt.show()\n"]},{"cell_type":"code","source":["def evaluate_on_test(model, X_test, y_test, threshold=0.35):\n","    \"\"\"\n","    Evaluates the model on an independent test dataset with detailed metrics.\n","\n","    Parameters:\n","    - model: Trained model to evaluate.\n","    - X_test: Test data (features).\n","    - y_test: Test data (labels).\n","    - threshold: Decision threshold to classify an instance as positive.\n","\n","    Returns:\n","    None\n","    \"\"\"\n","    print(\"\\nTest Performance:\")\n","    y_test_pred = (model.predict(X_test) > threshold).astype(int)\n","\n","    accuracy = accuracy_score(y_test, y_test_pred)\n","    precision = precision_score(y_test, y_test_pred)\n","    recall = recall_score(y_test, y_test_pred)\n","    f1 = f1_score(y_test, y_test_pred)\n","\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_test_pred))\n","\n","    # Confusion Matrix\n","    conf_matrix = confusion_matrix(y_test, y_test_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', xticklabels=['No Hate Speech', 'Hate Speech'], yticklabels=['No Hate Speech', 'Hate Speech'])\n","    plt.title('Confusion Matrix - Test Data')\n","    plt.xlabel('Predicted Labels')\n","    plt.ylabel('True Labels')\n","    plt.show()\n"],"metadata":{"id":"8mCYPReqczv5","executionInfo":{"status":"ok","timestamp":1735052431152,"user_tz":-60,"elapsed":3,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PzBEGi3jYf4Q"},"source":["### Training/Evaluate"]},{"cell_type":"code","source":["def one_hot_and_split_data(X_train_data, y_train_data, X_test_data, y_test_data):\n","  X_gru, y_gru = one_hot_enc(X_train_data, y_train_data)\n","  X_train_gru,X_val_gru,y_train_gru,y_val_gru=train_test_split(X_gru,y_gru,test_size=0.2,random_state=0)\n","  X_test_gru, y_test_gru = one_hot_enc(X_test_data, y_test_data)\n","  return X_train_gru, X_val_gru, y_train_gru, y_val_gru, X_test_gru, y_test_gru"],"metadata":{"id":"YOeRWaXVgT-o","executionInfo":{"status":"ok","timestamp":1735052434781,"user_tz":-60,"elapsed":422,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def tokenize_and_split_data(X_train_data, y_train_data, X_test_data, y_test_data):\n","  X_gru, y_gru, tokenizer = tokenize(X_resampled, y_resampled)\n","  X_train_gru,X_val_gru,y_train_gru,y_val_gru=train_test_split(X_gru,y_gru,test_size=0.2,random_state=42)\n","  X_test_gru, y_test_gru, _= tokenize(X_test_data, y_test_data, tokenizer)\n","  return X_train_gru, X_val_gru, y_train_gru, y_val_gru, X_test_gru, y_test_gru, tokenizer"],"metadata":{"id":"IXYb8AyJ81jg","executionInfo":{"status":"ok","timestamp":1735052435285,"user_tz":-60,"elapsed":2,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["##### One-Hot"],"metadata":{"id":"BADpzbRfWHy5"}},{"cell_type":"code","source":["X_train_onehot,X_val_onehot,y_train_onehot,y_val_onehot, X_test_onehot, y_test_onehot = one_hot_and_split_data(X_train_data, y_train_data, X_test_data, y_test_data)\n","model, history = train_gru(X_train_onehot, y_train_onehot, X_val_onehot, y_val_onehot, 5000, len(X_train_onehot[0]), None, embedding_dim=200, epochs=10, batch_size=32)\n","evaluate_on_val(model, history, X_val_onehot, y_val_onehot)\n","evaluate_on_test(model, X_test_onehot, y_test_onehot)"],"metadata":{"id":"JngrBYU08FCs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Tokenize"],"metadata":{"id":"qYvvelxzWKJg"}},{"cell_type":"code","execution_count":36,"metadata":{"id":"WjaBkTX6Yf4M","executionInfo":{"status":"ok","timestamp":1735060289719,"user_tz":-60,"elapsed":2528,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}}},"outputs":[],"source":["X_resampled, y_resampled = random_over_sample(X_train_data, y_train_data)\n","X_train_token,X_val_token,y_train_token,y_val_token, X_test_token, y_test_token, tokenizer = tokenize_and_split_data(X_resampled, y_resampled, X_test_data, y_test_data)\n","glove_embedding_matrix = load_embeddings('drive/MyDrive/Colab Notebooks/data/embeddings/glove.twitter.27B.200d.txt', 200, tokenizer.word_index)"]},{"cell_type":"code","source":["model, history = train_gru(X_train_token, y_train_token, X_val_token, y_val_token, len(tokenizer.word_index)+1, 40, glove_embedding_matrix, embedding_dim=200, epochs=20, batch_size=32)\n","evaluate_on_val(model, history, X_val_token, y_val_token)\n","evaluate_on_test(model, X_test_token, y_test_token)"],"metadata":{"id":"LMTunIfELy0n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_optimized, history_optimized = train_gru_optimized(X_train_token, y_train_token, X_val_token, y_val_token, len(tokenizer.word_index) + 1, 40, glove_embedding_matrix, embedding_dim=200, epochs=40, batch_size=32)\n","evaluate_on_val(model_optimized, history_optimized, X_val_token, y_val_token)\n","evaluate_on_test(model_optimized, X_test_token, y_test_token)"],"metadata":{"id":"lAw4agFa2LhM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5f98b5ad-d7c6-4551-fdc4-1fabe20d11dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/40\n","\u001b[1m867/948\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m7s\u001b[0m 98ms/step - Recall: 0.6325 - accuracy: 0.5078 - loss: 48.5487"]}]},{"cell_type":"code","source":["from sklearn.preprocessing import MultiLabelBinarizer\n","\n","# Preprocess additional features\n","def preprocess_additional_features(data, feature_name):\n","    # Extrahiere die Features (Listen von Strings oder Arrays)\n","    feature_list = [x if isinstance(x, list) else [] for x in data[feature_name]]\n","    mlb = MultiLabelBinarizer()\n","    features_encoded = mlb.fit_transform(feature_list)\n","    return features_encoded, mlb\n","\n","# Preprocess Training-, Validation- und Test-Daten\n","hashtags_encoded, hashtags_mlb = preprocess_additional_features(X_train_data, 'hashtags')\n","emojis_encoded, emojis_mlb = preprocess_additional_features(X_train_data, 'emojis')\n","\n","# Aufteilen der Features entsprechend den Token-Splits\n","hashtags_train, hashtags_val, emojis_train, emojis_val = train_test_split(\n","    hashtags_encoded, emojis_encoded, test_size=0.2, random_state=42\n",")\n","\n","hashtags_test = hashtags_mlb.transform(X_test_data['hashtags'])\n","emojis_test = emojis_mlb.transform(X_test_data['emojis'])\n","\n","def train_gru_with_additional_features(x_train_text, x_train_hashtags, x_train_emojis, y_train,\n","                                       x_val_text, x_val_hashtags, x_val_emojis, y_val,\n","                                       vocab_size, max_len, embedding_matrix=None, embedding_dim=200,\n","                                       epochs=20, batch_size=32):\n","    # Text Input\n","    text_input = Input(shape=(max_len,), name=\"Text_Input\")\n","    if embedding_matrix is not None:\n","        embedding = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)(text_input)\n","    else:\n","        embedding = Embedding(vocab_size, embedding_dim, input_length=max_len)(text_input)\n","    text_dropout = SpatialDropout1D(0.4)(embedding)\n","    text_rnn = Bidirectional(GRU(256, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))(text_dropout)\n","\n","    # Hashtags Input\n","    hashtags_input = Input(shape=(x_train_hashtags.shape[1],), name=\"Hashtags_Input\")\n","    hashtags_dense = Dense(128, activation=\"relu\")(hashtags_input)\n","\n","    # Emojis Input\n","    emojis_input = Input(shape=(x_train_emojis.shape[1],), name=\"Emojis_Input\")\n","    emojis_dense = Dense(64, activation=\"relu\")(emojis_input)\n","\n","    # Combine all inputs\n","    combined = Concatenate()([text_rnn, hashtags_dense, emojis_dense])\n","    dense = Dense(128, activation=\"relu\")(combined)\n","    output = Dense(1, activation=\"sigmoid\")(dense)\n","\n","    # Define the model\n","    model = Model(inputs=[text_input, hashtags_input, emojis_input], outputs=output)\n","    model.compile(\n","        loss='binary_crossentropy',\n","        optimizer=Adam(learning_rate=0.001),\n","        metrics=['accuracy', 'Recall']\n","    )\n","\n","    # Early Stopping Callback\n","    early_stopping = EarlyStopping(\n","        monitor='val_loss',\n","        patience=3,\n","        restore_best_weights=True\n","    )\n","\n","    # Train the model\n","    history = model.fit(\n","        [x_train_text, x_train_hashtags, x_train_emojis],\n","        y_train,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        validation_data=([x_val_text, x_val_hashtags, x_val_emojis], y_val),\n","        callbacks=[early_stopping]\n","    )\n","\n","    return model, history\n"],"metadata":{"id":"owzSB30u8jzs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Modell trainieren mit zus√§tzlichen Features\n","model_optimized, history_optimized = train_gru_with_additional_features(\n","    x_train_text=X_train_token,\n","    x_train_hashtags=hashtags_train,\n","    x_train_emojis=emojis_train,\n","    y_train=y_train_token,\n","    x_val_text=X_val_token,\n","    x_val_hashtags=hashtags_val,\n","    x_val_emojis=emojis_val,\n","    y_val=y_val_token,\n","    vocab_size=len(tokenizer.word_index) + 1,\n","    max_len=40,\n","    embedding_matrix=glove_embedding_matrix,\n","    embedding_dim=200,\n","    epochs=40,\n","    batch_size=32\n",")\n"],"metadata":{"id":"qrG4F1kXOxir"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}