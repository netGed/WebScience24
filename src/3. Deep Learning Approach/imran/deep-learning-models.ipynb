{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KsLnOMYzYf4G"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import imread\n","import nltk\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import f1_score, accuracy_score\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","import codecs\n","from tqdm import tqdm\n","import shutil\n","\n","\n","\n","\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import RandomOverSampler\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import LSTM, GRU,SimpleRNN\n","from keras.layers import Dense, Activation, Dropout\n","from keras.layers import Embedding\n","from keras.layers import BatchNormalization\n","from keras.utils import to_categorical\n","from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n","from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n","from keras.callbacks import EarlyStopping\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold\n","from imblearn.over_sampling import SMOTE\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_curve, auc\n","from sklearn.utils.class_weight import compute_class_weight\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from nltk.stem.porter import PorterStemmer\n","from keras.callbacks import EarlyStopping\n","\n","import re\n","from tensorflow.keras.preprocessing.text import one_hot\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZ-nt0upZ0F4","executionInfo":{"status":"ok","timestamp":1734683120957,"user_tz":-60,"elapsed":26092,"user":{"displayName":"Imran Nteli","userId":"02354348163468558774"}},"outputId":"135c254c-450b-4440-f1b6-7175a26a964c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBTp8AUSYf4J"},"outputs":[],"source":["train_cleaned_data = pd.read_csv(\"drive/MyDrive/Colab Notebooks/data/train_cleaned_rnn.csv\")\n","test_cleaned_data = pd.read_csv(\"drive/MyDrive/Colab Notebooks/data/test_cleaned_rnn.csv\")\n","\n","X_train_data = train_cleaned_data['tweet_cleaned']\n","y_train_data = train_cleaned_data['label']\n","\n","X_test_data = test_cleaned_data['tweet_cleaned']\n","y_test_data = test_cleaned_data['label']\n"]},{"cell_type":"markdown","source":["### Textvorverarbeitung"],"metadata":{"id":"-Tr4YuGlupp7"}},{"cell_type":"code","source":["def one_hot_enc(X_data, y_data):\n","  corpus = [str(text) for text in X_data]\n","  vocab_size = 5000\n","\n","  #one hot encoding\n","  one_hot_dir = [one_hot(words,vocab_size) for words in corpus]\n","\n","  embedded_layer = pad_sequences(one_hot_dir,padding = 'pre')\n","  return np.array(embedded_layer), np.array(y_data)"],"metadata":{"id":"FugTwaCh-JUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize(X_data, y_data, tokenizer=None, max_len=40):\n","    X_data = [str(x) for x in X_data]\n","\n","    # # Wenn kein Tokenizer bereitgestellt wird, erstelle einen neuen\n","    if tokenizer is None:\n","        tokenizer = Tokenizer(num_words=None, oov_token=\"<OOV>\")\n","        tokenizer.fit_on_texts(X_data)\n","\n","    # # Tokenisierung\n","    X_data_sequences = tokenizer.texts_to_sequences(X_data)\n","\n","    X_data_padded_sequences = pad_sequences(X_data_sequences, padding='post', maxlen=max_len)\n","    return X_data_padded_sequences, y_data, tokenizer\n"],"metadata":{"id":"SEK60o864rr2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sampling"],"metadata":{"id":"8PAsw3AIwZwe"}},{"cell_type":"code","source":["def random_over_sample(X_data, y_data):\n","  ros = RandomOverSampler(random_state=42)\n","  X = X_data.values.reshape(-1, 1)\n","  y = y_data.values\n","\n","  X_resampled, y_resampled = ros.fit_resample(X, y)\n","  X_resampled = X_resampled[:, 0]\n","  return X_resampled, y_resampled\n"],"metadata":{"id":"FMn2kBpkusB4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o2jE9QDeYf4N"},"source":["### Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EB-0iPjNYf4O"},"outputs":[],"source":["def load_embeddings(file_path, embedding_dim, word_index):\n","    embedding_index = {}\n","    with open(file_path, encoding='utf-8') as f:\n","        for line in f:\n","            values = line.rstrip().split(' ')\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embedding_index[word] = coefs\n","\n","    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n","    for word, i in word_index.items():\n","        embedding_vector = embedding_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","\n","    return embedding_matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtLRyoM5Yf4O"},"outputs":[],"source":["import requests, zipfile, io\n","zip_file_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n","r = requests.get(zip_file_url)\n","z = zipfile.ZipFile(io.BytesIO(r.content))\n","z.extractall()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHn_3bm_Yf4P"},"outputs":[],"source":["import requests, zipfile, io\n","zip_file_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\n","r = requests.get(zip_file_url)\n","z = zipfile.ZipFile(io.BytesIO(r.content))\n","z.extractall()"]},{"cell_type":"markdown","metadata":{"id":"SG6Ry5glYf4P"},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8pOnEw_Yf4P"},"outputs":[],"source":["def train_gru(x_train, y_train, x_val, y_val, vocab_size, max_len, embedding_matrix=None, embedding_dim=300, epochs=20, batch_size=32):\n","    model = Sequential()\n","    if embedding_matrix is not None:\n","        model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n","    else:\n","        model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n","    model.add(SpatialDropout1D(0.3))\n","    model.add(GRU(300))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n","\n","\n","    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))\n","    return model, history\n"]},{"cell_type":"code","source":["def train_gru_optimized(x_train, y_train, x_val, y_val, vocab_size, max_len, embedding_matrix=None, embedding_dim=300, epochs=20, batch_size=32, class_weights=None):\n","    model = Sequential()\n","    if embedding_matrix is not None:\n","        model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n","    else:\n","        model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n","    model.add(SpatialDropout1D(0.4))  # Höheres Dropout für bessere Generalisierung\n","    model.add(GRU(256, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))  # Kleinere GRU-Einheiten und Recurrent Dropout\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy', 'Recall'])  # Recall als Metrik\n","\n","    early_stopping = EarlyStopping(\n","        monitor='val_loss',\n","        patience=3,\n","        restore_best_weights=True\n","    )\n","\n","    history = model.fit(\n","        x_train,\n","        y_train,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        validation_data=(x_val, y_val),\n","        class_weight=class_weights,  # Klassen-Gewichtung einfügen\n","        callbacks=[early_stopping]\n","    )\n","    return model, history"],"metadata":{"id":"pxix7Iff3ymG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5b5c8Aq7Yf4R"},"source":["### Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k5weY2AlYf4R"},"outputs":[],"source":["def evaluate_on_val(model, history, X_val, y_val, threshold=0.35):\n","    \"\"\"\n","    Evaluates the model on validation data with detailed metrics and visualizes training performance.\n","\n","    Parameters:\n","    - model: Trained model to evaluate.\n","    - history: Training history object.\n","    - X_val: Validation data (features).\n","    - y_val: Validation data (labels).\n","    - threshold: Decision threshold to classify an instance as positive.\n","\n","    Returns:\n","    None\n","    \"\"\"\n","    # Plot Training and Validation Metrics\n","    plt.figure(figsize=(12, 6))\n","\n","    # Accuracy\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['accuracy'], label='Training Accuracy')\n","    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","    plt.title('Model Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    # Loss\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.title('Model Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Evaluate Validation Data\n","    print(\"\\nValidation Performance:\")\n","    y_val_pred = (model.predict(X_val) > threshold).astype(int)\n","\n","    accuracy = accuracy_score(y_val, y_val_pred)\n","    precision = precision_score(y_val, y_val_pred)\n","    recall = recall_score(y_val, y_val_pred)\n","    f1 = f1_score(y_val, y_val_pred)\n","    # roc_auc = roc_auc_score(y_val, model.predict(X_val))\n","\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","    # print(f\"ROC AUC: {roc_auc:.4f}\")\n","\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_val, y_val_pred))\n","\n","    # Confusion Matrix\n","    conf_matrix = confusion_matrix(y_val, y_val_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Hate Speech', 'Hate Speech'], yticklabels=['No Hate Speech', 'Hate Speech'])\n","    plt.title('Confusion Matrix - Validation Data')\n","    plt.xlabel('Predicted Labels')\n","    plt.ylabel('True Labels')\n","    plt.show()\n"]},{"cell_type":"code","source":["def evaluate_on_test(model, X_test, y_test, threshold=0.35):\n","    \"\"\"\n","    Evaluates the model on an independent test dataset with detailed metrics.\n","\n","    Parameters:\n","    - model: Trained model to evaluate.\n","    - X_test: Test data (features).\n","    - y_test: Test data (labels).\n","    - threshold: Decision threshold to classify an instance as positive.\n","\n","    Returns:\n","    None\n","    \"\"\"\n","    print(\"\\nTest Performance:\")\n","    y_test_pred = (model.predict(X_test) > threshold).astype(int)\n","\n","    accuracy = accuracy_score(y_test, y_test_pred)\n","    precision = precision_score(y_test, y_test_pred)\n","    recall = recall_score(y_test, y_test_pred)\n","    f1 = f1_score(y_test, y_test_pred)\n","\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_test_pred))\n","\n","    # Confusion Matrix\n","    conf_matrix = confusion_matrix(y_test, y_test_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', xticklabels=['No Hate Speech', 'Hate Speech'], yticklabels=['No Hate Speech', 'Hate Speech'])\n","    plt.title('Confusion Matrix - Test Data')\n","    plt.xlabel('Predicted Labels')\n","    plt.ylabel('True Labels')\n","    plt.show()\n"],"metadata":{"id":"8mCYPReqczv5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PzBEGi3jYf4Q"},"source":["### Training/Evaluate"]},{"cell_type":"code","source":["def one_hot_and_split_data(X_train_data, y_train_data, X_test_data, y_test_data):\n","  X_gru, y_gru = one_hot_enc(X_train_data, y_train_data)\n","  X_train_gru,X_val_gru,y_train_gru,y_val_gru=train_test_split(X_gru,y_gru,test_size=0.2,random_state=0)\n","  X_test_gru, y_test_gru = one_hot_enc(X_test_data, y_test_data)\n","  return X_train_gru, X_val_gru, y_train_gru, y_val_gru, X_test_gru, y_test_gru"],"metadata":{"id":"YOeRWaXVgT-o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_and_split_data(X_train_data, y_train_data, X_test_data, y_test_data):\n","  X_gru, y_gru, tokenizer = tokenize(X_resampled, y_resampled)\n","  X_train_gru,X_val_gru,y_train_gru,y_val_gru=train_test_split(X_gru,y_gru,test_size=0.2,random_state=42)\n","  X_test_gru, y_test_gru, _= tokenize(X_test_data, y_test_data, tokenizer)\n","  return X_train_gru, X_val_gru, y_train_gru, y_val_gru, X_test_gru, y_test_gru"],"metadata":{"id":"IXYb8AyJ81jg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_onehot,X_val_onehot,y_train_onehot,y_val_onehot, X_test_onehot, y_test_onehot = one_hot_and_split_data(X_train_data, y_train_data, X_test_data, y_test_data)\n","model, history = train_gru(X_train_onehot, y_train_onehot, X_val_onehot, y_val_onehot, 5000, len(X_train_onehot[0]), None, embedding_dim=200, epochs=10, batch_size=32)\n","evaluate_on_val(model, history, X_val_onehot, y_val_onehot)\n","evaluate_on_test(model, X_test_onehot, y_test_onehot)"],"metadata":{"id":"JngrBYU08FCs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WjaBkTX6Yf4M"},"outputs":[],"source":["X_resampled, y_resampled = random_over_sample(X_train_data, y_train_data)\n","X_train_token,X_val_token,y_train_token,y_val_token, X_test_token, y_test_token = tokenize_and_split_data(X_resampled, y_resampled, X_test_data, y_test_data)\n","glove_embedding_matrix = load_embeddings('drive/MyDrive/Colab Notebooks/data/embeddings/glove.twitter.27B.200d.txt', 200, tokenizer.word_index)\n","\n","model, history = train_gru(X_train_token, y_train_token, X_val_token, y_val_token, len(tokenizer.word_index)+1, 40, glove_embedding_matrix, embedding_dim=200, epochs=20, batch_size=32)\n","evaluate_on_val(model, history, X_val_token, y_val_token)\n","evaluate_on_test(model, X_test_token, y_test_token)\n"]},{"cell_type":"code","source":["class_weights = compute_class_weight(class_weight='balanced', classes=[0, 1], y=y_train_gru)\n","class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n","\n","model_optimized, history_optimized = train_gru_optimized(X_train_token, y_train_token, X_val_token, y_val_token, len(tokenizer.word_index) + 1, 40, glove_embedding_matrix, embedding_dim=200, epochs=20, batch_size=32, class_weights=class_weights_dict)\n","evaluate_on_val(model, history, X_val_token, y_val_token)\n","evaluate_on_test(model, X_test_token, y_test_token)"],"metadata":{"id":"lAw4agFa2LhM"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}