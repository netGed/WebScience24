{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe51bf5d",
   "metadata": {},
   "source": [
    "# Vektorisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c2016a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d7d715",
   "metadata": {},
   "source": [
    "Dataset \"train_cleaned\" importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "117a74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../data/twitter_hate-speech/train_cleaned.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e15cf4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_cleaned</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emojis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>father selfish drag kid run</td>\n",
       "      <td>1</td>\n",
       "      <td>['#run']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thank lyft credit use cause offer van</td>\n",
       "      <td>2</td>\n",
       "      <td>['#lyft', '#disapointed', '#getthanked']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday majesty</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model take time mobile phone kiss sunglass mou...</td>\n",
       "      <td>0</td>\n",
       "      <td>['#model']</td>\n",
       "      <td>:mobile_phone:,:kissing_face_with_smiling_eyes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "      <td>0</td>\n",
       "      <td>['#motivation']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              tweet  \\\n",
       "id                                                             \n",
       "1       0   @user when a father is dysfunctional and is s...   \n",
       "2       0  @user @user thanks for #lyft credit i can't us...   \n",
       "3       0                                bihday your majesty   \n",
       "4       0  #model   i love u take with u all the time in ...   \n",
       "5       0             factsguide: society now    #motivation   \n",
       "\n",
       "                                        tweet_cleaned  user_handle  \\\n",
       "id                                                                   \n",
       "1                         father selfish drag kid run            1   \n",
       "2               thank lyft credit use cause offer van            2   \n",
       "3                                      bihday majesty            0   \n",
       "4   model take time mobile phone kiss sunglass mou...            0   \n",
       "5                       factsguide society motivation            0   \n",
       "\n",
       "                                    hashtags  \\\n",
       "id                                             \n",
       "1                                   ['#run']   \n",
       "2   ['#lyft', '#disapointed', '#getthanked']   \n",
       "3                                         []   \n",
       "4                                 ['#model']   \n",
       "5                            ['#motivation']   \n",
       "\n",
       "                                               emojis  \n",
       "id                                                     \n",
       "1                                                 NaN  \n",
       "2                                                 NaN  \n",
       "3                                                 NaN  \n",
       "4   :mobile_phone:,:kissing_face_with_smiling_eyes...  \n",
       "5                                                 NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d46687",
   "metadata": {},
   "source": [
    "Da es leere tweet_cleaned gibt müssen diese hier entfernt werden. Ansonsten werfen alle 3 Methoden Fehler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bff7d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['tweet_cleaned'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780468d9",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3127a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doku: https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer() # CountVectorizer(binary=True) würde nur 0 und 1 ausgeben\n",
    "X_bow = vectorizer.fit_transform(df.tweet_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac836c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aamp' 'aap' 'ab' ... 'zootopia' 'zoro' 'zzz']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323d9532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7562d5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9523671742196824\n"
     ]
    }
   ],
   "source": [
    "# Erster Test NB\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = df.label\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_bow,y)\n",
    "\n",
    "ypred = clf.predict(X_bow)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d898b8",
   "metadata": {},
   "source": [
    "#### Ergebnis Bag of Word in X_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ccd4a",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f0d079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doku https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df.tweet_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4a66df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aamp' 'aap' 'ab' ... 'zootopia' 'zoro' 'zzz']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4c9e8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef72bfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.954609569966364\n"
     ]
    }
   ],
   "source": [
    "# Erster Test NB\n",
    "\n",
    "y = df.label\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_tfidf,y)\n",
    "\n",
    "ypred_tfidf = clf.predict(X_tfidf)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y, ypred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c3bbd",
   "metadata": {},
   "source": [
    "#### Ergebnis TF-IDF in X_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b995e9b4",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8d0c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "data = df['tweet_cleaned'].map(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c945862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1                        [father, selfish, drag, kid, run]\n",
       "2            [thank, lyft, credit, use, cause, offer, van]\n",
       "3                                        [bihday, majesty]\n",
       "4        [model, take, time, mobile, phone, kiss, sungl...\n",
       "5                        [factsguide, society, motivation]\n",
       "                               ...                        \n",
       "31956    [less, week, grin, sweat, fold, hand, medium, ...\n",
       "31957         [fishing, tomorrow, wait, first, time, year]\n",
       "31958                                    [eat, youuu, red]\n",
       "31959    [see, turner, try, wrap, genuine, hero, like, ...\n",
       "31960    [listen, sad, song, monday, morning, otw, work...\n",
       "Name: tweet_cleaned, Length: 27649, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "783fbc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(data,window=4, min_count=1, sg=0) \n",
    "# sg=1: Skip-Gram\n",
    "# sg=0: Continuous Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77cb2b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3895292, 4126720)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.train(data, total_examples=len(df['tweet_cleaned']), epochs=20)\n",
    "# epochs = Anzahl Iterationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e13e573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inspiration', 0.8603976368904114),\n",
       " ('positivity', 0.8576184511184692),\n",
       " ('inspirational', 0.778499186038971),\n",
       " ('quoteoftheday', 0.7686036825180054),\n",
       " ('motivate', 0.7532286047935486),\n",
       " ('belief', 0.7525449395179749),\n",
       " ('entrepreneur', 0.7442077994346619),\n",
       " ('selflove', 0.7372958660125732),\n",
       " ('personaldevelopment', 0.7356842160224915),\n",
       " ('clarity', 0.7265296578407288)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=\"motivation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67e412b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meet', 0.39135491847991943),\n",
       " ('august', 0.3650570809841156),\n",
       " ('th', 0.35828283429145813),\n",
       " ('poland', 0.34213826060295105),\n",
       " ('july', 0.34154877066612244),\n",
       " ('yr', 0.3362632095813751),\n",
       " ('st', 0.3296425938606262),\n",
       " ('rd', 0.32583528757095337),\n",
       " ('half', 0.3231406509876251),\n",
       " ('date', 0.3174822926521301)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(negative=\"motivation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c942d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100\n",
    "\n",
    "def w2v_vector(tokenized_tweet, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokenized_tweet:\n",
    "        try:\n",
    "            vec += w2v.wv[word].reshape((1, size))\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c42d088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v = np.zeros((len(data),size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65fd6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    X_w2v[i,:] = w2v_vector(data.iloc[i],size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "feee32ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.32555764,  0.39498052, -0.32812717, ..., -0.99470037,\n",
       "        -0.20116415, -0.89298161],\n",
       "       [-0.3282834 ,  0.54676195,  0.00112542, ..., -0.26532286,\n",
       "        -0.15750324, -0.49165652],\n",
       "       [ 0.31196176,  0.22025196, -0.50505333, ..., -0.36300076,\n",
       "         0.16350489,  0.62725972],\n",
       "       ...,\n",
       "       [-0.63985382, -0.65229094,  0.92911165, ..., -0.65501505,\n",
       "         0.32828084, -0.16535795],\n",
       "       [-0.30423937,  0.46370349,  0.08884029, ..., -0.0956155 ,\n",
       "         0.01651742,  0.07102602],\n",
       "       [-0.63813522,  0.58901546, -0.77321871, ...,  0.11527004,\n",
       "         0.19589269,  0.20339311]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff1a3c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7570255705450468\n"
     ]
    }
   ],
   "source": [
    "# Erster Test NB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "y = df.label\n",
    "clf = GaussianNB()\n",
    "\n",
    "clf.fit(X_w2v,y)\n",
    "\n",
    "ypred_tfidf = clf.predict(X_w2v)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y, ypred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68f8c6",
   "metadata": {},
   "source": [
    "#### Ergebnis Word2Vec in X_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e499c6b",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5a7c642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3789064, 4126720)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import FastText \n",
    "\n",
    "ft = FastText(data, window = 4)\n",
    "ft.train(data, total_examples=len(df['tweet_cleaned']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5da8cc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tuesdaymotivation', 0.9570346474647522),\n",
       " ('mondaymotivation', 0.9523440599441528),\n",
       " ('motivational', 0.9495548009872437),\n",
       " ('inspiration', 0.8649970889091492),\n",
       " ('affirmation', 0.8257686495780945),\n",
       " ('transformation', 0.8194717764854431),\n",
       " ('meditation', 0.8178461790084839),\n",
       " ('confirmation', 0.7999058961868286),\n",
       " ('innovation', 0.7997733950614929),\n",
       " ('position', 0.7960103154182434)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.wv.most_similar(positive=\"motivation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d3ed0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yr', 0.3835121691226959),\n",
       " ('v', 0.3463435471057892),\n",
       " ('hi', 0.34137162566185),\n",
       " ('gator', 0.3400598168373108),\n",
       " ('sit', 0.32863670587539673),\n",
       " ('cox', 0.3283883035182953),\n",
       " ('xbox', 0.3270142078399658),\n",
       " ('die', 0.32616955041885376),\n",
       " ('snatch', 0.3259742558002472),\n",
       " ('bro', 0.32236719131469727)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.wv.most_similar(negative=\"motivation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "192d9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100\n",
    "\n",
    "def ft_vector(tokenized_tweet, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokenized_tweet:\n",
    "        try:\n",
    "            vec += ft.wv[word].reshape((1, size))\n",
    "            count += 1\n",
    "        except KeyError: \n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "913fe904",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft = np.zeros((len(data),size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efc330ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    X_ft[i,:] = ft_vector(data.iloc[i],size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36d19786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76547772, -0.21383475, -0.40605466, ..., -0.95944892,\n",
       "        -0.78261749,  0.35719954],\n",
       "       [-0.5774988 , -0.04261827, -0.24013078, ..., -0.04634736,\n",
       "         0.61478294,  0.17527176],\n",
       "       [ 0.81800085,  0.57611634,  0.08267095, ..., -1.20095155,\n",
       "        -0.38159819, -0.06786457],\n",
       "       ...,\n",
       "       [-0.16428977, -0.29616243,  0.1838846 , ..., -0.87193196,\n",
       "         0.509852  , -0.64583018],\n",
       "       [-0.16578822, -0.05996855, -0.39674566, ...,  0.05825424,\n",
       "         0.52146263,  0.31040981],\n",
       "       [-0.51245216,  0.18201927, -0.49733237, ..., -0.3617763 ,\n",
       "        -0.34870465, -0.19760916]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65a1b58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7548555101450324\n"
     ]
    }
   ],
   "source": [
    "# Erster Test NB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "y = df.label\n",
    "clf = GaussianNB()\n",
    "\n",
    "clf.fit(X_ft,y)\n",
    "\n",
    "ypred_tfidf = clf.predict(X_ft)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y, ypred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a05e85",
   "metadata": {},
   "source": [
    "#### Ergebnis FastText in X_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea838a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9158e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../sven/vf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd8e3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_base = df.tweet_cleaned\n",
    "y_base =df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c693ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "729faafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doku: https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X_base, y_base, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d692f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    \n",
    "    X_base = df.tweet_cleaned\n",
    "    y_base =df.label\n",
    "    X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X_base, y_base, test_size=0.3, random_state=42)\n",
    "    \n",
    "    return X_train_base, X_test_base, y_train_base, y_test_base\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e192649e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_base_e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19300\\2409905133.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_base_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_base_e' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_base_e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c96bed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19354,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb8651cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8295,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91e68a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8295,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_base.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f45c9",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c6968f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7e1b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_base)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f3b5ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19354, 6293)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c66c7f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8295, 6293)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94082c1b",
   "metadata": {},
   "source": [
    "### bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5848f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48bca1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = bow_vectorizer.fit_transform(X_train_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ec678b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bow = bow_vectorizer.fit_transform(X_test_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e52f3ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19354, 6293)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9aac72b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8295, 5672)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "295197cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.split()>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "16adf57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_bow():\n",
    "    df = pd.read_csv('../../../data/twitter_hate-speech/train_cleaned.csv', index_col=0)\n",
    "    df = df[df['tweet_cleaned'].notna()]\n",
    "    \n",
    "    X_base = df.tweet_cleaned\n",
    "    y_base =df.label\n",
    "    \n",
    "    X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X_base, y_base, test_size=0.3, random_state=42)\n",
    "    \n",
    "    bow_vectorizer = CountVectorizer()\n",
    "    \n",
    "    X_train_bow, = bow_vectorizer.fit_transform(X_train_base)\n",
    "    X_test_bow = bow_vectorizer.fit_transform(X_test_base)\n",
    "    \n",
    "    return X_train_bow, X_test_bow, y_train_base, y_test_base\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "73cade5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tfidf():\n",
    "    df = pd.read_csv('../../../data/twitter_hate-speech/train_cleaned.csv', index_col=0)\n",
    "    df = df[df['tweet_cleaned'].notna()]\n",
    "    \n",
    "    X_base = df.tweet_cleaned\n",
    "    y_base =df.label\n",
    "    \n",
    "    X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X_base, y_base, test_size=0.3, random_state=42)\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_base)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test_base)\n",
    "    \n",
    "    return X_train_tfidf, X_test_tfidf, y_train_base, y_test_base\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98940363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_w2v():\n",
    "    df = pd.read_csv('../../../data/twitter_hate-speech/train_cleaned.csv', index_col=0)\n",
    "    df = df[df['tweet_cleaned'].notna()]\n",
    "\n",
    "    X_base = df.tweet_cleaned\n",
    "    y_base = df.label\n",
    "\n",
    "    X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X_base, y_base, test_size=0.3,\n",
    "                                                                            random_state=42)\n",
    "\n",
    "    X_train_base_tokenized = X_train_base.map(word_tokenize)\n",
    "    X_test_base_tokenized = X_test_base.map(word_tokenize)\n",
    "\n",
    "    w2v = Word2Vec(X_train_base, window=4, min_count=1, sg=0)\n",
    "    w2v.train(X_train_base_tokenized, total_examples=len(X_train_base), epochs=20)\n",
    "\n",
    "    def w2v_vector(tokenized_tweet, size):\n",
    "        vec = np.zeros(size).reshape((1, size))\n",
    "        count = 0\n",
    "        for word in tokenized_tweet:\n",
    "            try:\n",
    "                vec += w2v.wv[word].reshape((1, size))\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "\n",
    "                continue\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "\n",
    "    size = 100\n",
    "    X_train_w2v = np.zeros((len(X_train_base_tokenized), size))\n",
    "    for i in range(len(X_train_base_tokenized)):\n",
    "        X_train_w2v[i, :] = w2v_vector(X_train_base_tokenized.iloc[i], size)\n",
    "\n",
    "    X_test_w2v = np.zeros((len(X_test_base_tokenized), size))\n",
    "    for i in range(len(X_test_base_tokenized)):\n",
    "        X_test_w2v[i, :] = w2v_vector(X_test_base_tokenized.iloc[i], size)\n",
    "\n",
    "    return X_train_w2v, X_test_w2v, y_train_base, y_test_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_ft():\n",
    "    def ft_vector(tokenized_tweet, size):\n",
    "        vec = np.zeros(size).reshape((1, size))\n",
    "        count = 0\n",
    "        for word in tokenized_tweet:\n",
    "            try:\n",
    "                vec += ft.wv[word].reshape((1, size))\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "\n",
    "                continue\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "\n",
    "    df = pd.read_csv('../../../data/twitter_hate-speech/train_cleaned.csv', index_col=0)\n",
    "    df = df[df['tweet_cleaned'].notna()]\n",
    "\n",
    "    X_base = df.tweet_cleaned\n",
    "    y_base = df.label\n",
    "\n",
    "    X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X_base, y_base, test_size=0.3,\n",
    "                                                                            random_state=42)\n",
    "\n",
    "    X_train_base_tokenized = X_train_base.map(word_tokenize)\n",
    "    X_test_base_tokenized = X_test_base.map(word_tokenize)\n",
    "\n",
    "    ft = FastText(X_train_base, window=4)\n",
    "    ft.train(X_train_base_tokenized, total_examples=len(X_train_base), epochs=20)\n",
    "\n",
    "    size = 100\n",
    "    X_train_ft = np.zeros((len(X_train_base_tokenized), size))\n",
    "    for i in range(len(X_train_base_tokenized)):\n",
    "        X_train_ft[i, :] = ft_vector(X_train_base_tokenized.iloc[i], size)\n",
    "\n",
    "    X_test_ft = np.zeros((len(X_test_base_tokenized), size))\n",
    "    for i in range(len(X_test_base_tokenized)):\n",
    "        X_test_ft[i, :] = ft_vector(X_test_base_tokenized.iloc[i], size)\n",
    "\n",
    "    return X_train_ft, X_test_ft, y_train_base, y_test_base\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
