{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Preprocessing** \n","   - 1 Lower casing\n","   - 2 Punctuation removal\n","   - 3 Stopwords removal\n","   - 4 Frequent words removal\n","   - 5 Rare words removal\n","   - 6 Spelling correction\n","   - 7 Tokenization\n","   - 8 Stemming\n","   - 9 Lemmatization\n"]},{"cell_type":"markdown","metadata":{},"source":["### **Load necessary libraries**"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{},"source":["### **Read dataset**"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["train = pd.read_csv(\"..\\\\..\\\\..\\\\data\\\\twitter_hate-speech\\\\train.csv\")\n","test = pd.read_csv(\"..\\\\..\\\\..\\\\data\\\\twitter_hate-speech\\\\test.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["### **Preview dataset**"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  label                                              tweet\n","0   1      0   @user when a father is dysfunctional and is s...\n","1   2      0  @user @user thanks for #lyft credit i can't us...\n","2   3      0                                bihday your majesty\n","3   4      0  #model   i love u take with u all the time in ...\n","4   5      0             factsguide: society now    #motivation"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["train.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>31963</td>\n","      <td>#studiolife #aislife #requires #passion #dedic...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>31964</td>\n","      <td>@user #white #supremacists want everyone to s...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>31965</td>\n","      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>31966</td>\n","      <td>is the hp and the cursed child book up for res...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>31967</td>\n","      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id                                              tweet\n","0  31963  #studiolife #aislife #requires #passion #dedic...\n","1  31964   @user #white #supremacists want everyone to s...\n","2  31965  safe ways to heal your #acne!!    #altwaystohe...\n","3  31966  is the hp and the cursed child book up for res...\n","4  31967    3rd #bihday to my amazing, hilarious #nephew..."]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["test.head()"]},{"cell_type":"markdown","metadata":{},"source":["## **1. Lower Casing**\n","\n","\n","- transform our tweets into lower case. \n","- This avoids having multiple copies of the same words. "]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["def lower_case(df):\n","    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n","    print(df['tweet'].head())"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    @user when a father is dysfunctional and is so...\n","1    @user @user thanks for #lyft credit i can't us...\n","2                                  bihday your majesty\n","3    #model i love u take with u all the time in ur...\n","4                  factsguide: society now #motivation\n","Name: tweet, dtype: object\n"]}],"source":["lower_case(train)"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    #studiolife #aislife #requires #passion #dedic...\n","1    @user #white #supremacists want everyone to se...\n","2    safe ways to heal your #acne!! #altwaystoheal ...\n","3    is the hp and the cursed child book up for res...\n","4    3rd #bihday to my amazing, hilarious #nephew e...\n","Name: tweet, dtype: object\n"]}],"source":["lower_case(test)"]},{"cell_type":"markdown","metadata":{"trusted":true},"source":["## **2. Punctuation Removal**\n","\n","\n","- The next step is to remove punctuation as it doesn’t add any extra information while treating text data. \n","\n","- Therefore removing all instances of it will help us reduce the size of the training data."]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["def punctuation_removal(df):\n","    df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n","    print(df['tweet'].head())"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    @user when a father is dysfunctional and is so...\n","1    @user @user thanks for #lyft credit i can't us...\n","2                                  bihday your majesty\n","3    #model i love u take with u all the time in ur...\n","4                  factsguide: society now #motivation\n","Name: tweet, dtype: object\n"]}],"source":["punctuation_removal(train)"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    #studiolife #aislife #requires #passion #dedic...\n","1    @user #white #supremacists want everyone to se...\n","2    safe ways to heal your #acne!! #altwaystoheal ...\n","3    is the hp and the cursed child book up for res...\n","4    3rd #bihday to my amazing, hilarious #nephew e...\n","Name: tweet, dtype: object\n"]}],"source":["punctuation_removal(test)"]},{"cell_type":"markdown","metadata":{},"source":["## **3 Stop Words Removal**\n","\n","\n","\n","-  stop words (or commonly occurring words) should be removed from the text data. \n","- For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries."]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["from nltk.corpus import stopwords\n","stop = stopwords.words('english')"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["def stop_words_removal(df):\n","    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n","    print(df['tweet'].head())"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    @user father dysfunctional selfish drags kids ...\n","1    @user @user thanks #lyft credit can't use caus...\n","2                                       bihday majesty\n","3    #model love u take u time urð±!!! ððð...\n","4                      factsguide: society #motivation\n","Name: tweet, dtype: object\n"]}],"source":["stop_words_removal(train)"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    #studiolife #aislife #requires #passion #dedic...\n","1    @user #white #supremacists want everyone see n...\n","2    safe ways heal #acne!! #altwaystoheal #healthy...\n","3    hp cursed child book reservations already? yes...\n","4    3rd #bihday amazing, hilarious #nephew eli ahm...\n","Name: tweet, dtype: object\n"]}],"source":["stop_words_removal(test)"]},{"cell_type":"markdown","metadata":{"trusted":true},"source":["## **4. Frequent Words Removal**\n","\n","- We can also remove commonly occurring words from our text data.\n","\n","- First, let’s check the 10 most frequently occurring words in our text data then take call to remove or retain.\n"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["@user    17291\n","&amp;     1574\n","day       1454\n","#love     1449\n","happy     1328\n","-         1244\n","u         1116\n","love      1112\n","i'm        992\n","like       920\n","Name: count, dtype: int64"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[:10]\n","freq"]},{"cell_type":"markdown","metadata":{"trusted":true},"source":["Now, we will remove these words as their presence will not of any use in classification of our text data."]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[],"source":["freq = list(freq.index)"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[],"source":["def frequent_words_removal(df):    \n","    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n","    print(df['tweet'].head())"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    father dysfunctional selfish drags kids dysfun...\n","1    thanks #lyft credit can't use cause offer whee...\n","2                                       bihday majesty\n","3    #model take time urð±!!! ðððð ð...\n","4                      factsguide: society #motivation\n","Name: tweet, dtype: object\n"]}],"source":["frequent_words_removal(train)"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    #studiolife #aislife #requires #passion #dedic...\n","1    #white #supremacists want everyone see new â...\n","2    safe ways heal #acne!! #altwaystoheal #healthy...\n","3    hp cursed child book reservations already? yes...\n","4    3rd #bihday amazing, hilarious #nephew eli ahm...\n","Name: tweet, dtype: object\n"]}],"source":["frequent_words_removal(test)"]},{"cell_type":"markdown","metadata":{"trusted":true},"source":["## **5. Rare Words Removal**\n","\n","- Now, we will remove rarely occurring words from the text. \n","- Because they’re so rare, the association between them and other words is dominated by noise. \n","- We can replace rare words with a more general form and then this will have higher counts."]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["ukip        1\n","europ...    1\n","them,we     1\n","joke...     1\n","kylo,       1\n","prick       1\n","berry       1\n","ciroc       1\n","cents       1\n","chisolm.    1\n","Name: count, dtype: int64"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:]\n","freq"]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[],"source":["freq = list(freq.index)"]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[],"source":["def rare_words_removal(df):\n","    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n","    print(df['tweet'].head())"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    father dysfunctional selfish drags kids dysfun...\n","1    thanks #lyft credit can't use cause offer whee...\n","2                                       bihday majesty\n","3    #model take time urð±!!! ðððð ð...\n","4                      factsguide: society #motivation\n","Name: tweet, dtype: object\n"]}],"source":["rare_words_removal(train)"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    #studiolife #aislife #requires #passion #dedic...\n","1    #white #supremacists want everyone see new â...\n","2    safe ways heal #acne!! #altwaystoheal #healthy...\n","3    hp cursed child book reservations already? yes...\n","4    3rd #bihday amazing, hilarious #nephew eli ahm...\n","Name: tweet, dtype: object\n"]}],"source":["rare_words_removal(test)"]},{"cell_type":"markdown","metadata":{},"source":["- All these pre-processing steps are essential and help us in reducing our vocabulary clutter so that the features produced in the end are more effective."]},{"cell_type":"markdown","metadata":{},"source":["## **6 Spelling Correction** \n","\n","- Now tweets can be filled with plethora of spelling mistakes. Our task is to rectify these spelling mistakes.\n","\n","- In that context, spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words. For example, “Analytics” and “analytcs” will be treated as different words even if they are used in the same sense.\n","\n","- To accomplish the above task, we will use the textblob library as follows-\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: textblob in c:\\users\\inteli\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.18.0.post0)\n","Requirement already satisfied: nltk>=3.8 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from textblob) (3.9.1)\n","Requirement already satisfied: click in c:\\users\\inteli\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n","Requirement already satisfied: joblib in c:\\users\\inteli\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk>=3.8->textblob) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk>=3.8->textblob) (2024.9.11)\n","Requirement already satisfied: tqdm in c:\\users\\inteli\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk>=3.8->textblob) (4.66.5)\n","Requirement already satisfied: colorama in c:\\users\\inteli\\appdata\\roaming\\python\\python38\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 24.0 -> 24.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["!pip install textblob"]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[],"source":["from textblob import TextBlob"]},{"cell_type":"code","execution_count":32,"metadata":{"trusted":true},"outputs":[],"source":["def spell_correction(df):\n","    return df['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))"]},{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0    father dysfunctional selfish drags kiss dysfun...\n","1    thanks #left credit can't use cause offer whee...\n","2                                       midday majesty\n","3    #model take time or±!!! ðððð ð¦...\n","4                      factsguide: society #motivation\n","Name: tweet, dtype: object"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["spell_correction(train)"]},{"cell_type":"code","execution_count":34,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0    #studiolife #dislike #requires #passion #educa...\n","1    #white #supremacists want everyone see new â...\n","2    safe ways heal #acne!! #altwaystoheal #healthy...\n","3    he cursed child book reservations already? yes...\n","4    rd #midday amazing, hilarious #nephew epi thei...\n","Name: tweet, dtype: object"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["spell_correction(test)"]},{"cell_type":"markdown","metadata":{},"source":["## **7. Stemming**\n","\n","- [Stemming](https://www.geeksforgeeks.org/introduction-to-stemming/) refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. \n","\n","- So, stemming takes a word and refers it back to its base or root form. **Stems**, **Stemming**, **Stemmed** and **Stemtization** are all based on the single word **stem**.\n","\n","- For this purpose, we will use *PorterStemmer* from the NLTK library."]},{"cell_type":"code","execution_count":73,"metadata":{"trusted":true},"outputs":[],"source":["from nltk.stem import PorterStemmer\n","st = PorterStemmer()"]},{"cell_type":"code","execution_count":74,"metadata":{"trusted":true},"outputs":[],"source":["def stemming(df):\n","    return df['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"]},{"cell_type":"code","execution_count":75,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0    father dysfunct selfish drag kid dysfunction. ...\n","1    thank #lyft credit can't use caus offer wheelc...\n","2                                       bihday majesti\n","3    #model take time urð±!!! ðððð ð...\n","4                           factsguide: societi #motiv\n","Name: tweet, dtype: object"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["stemming(train)"]},{"cell_type":"code","execution_count":76,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0    #studiolif #aislif #requir #passion #dedic #wi...\n","1    #white #supremacist want everyon see new â #...\n","2    safe way heal #acne!! #altwaystoh #healthi #he...\n","3    hp curs child book reserv already? yes, where?...\n","4    3rd #bihday amazing, hilari #nephew eli ahmir!...\n","Name: tweet, dtype: object"]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["stemming(test)"]},{"cell_type":"markdown","metadata":{},"source":["- We can see that *dysfunctional* has been transformed into *dysfunct*, among other changes."]},{"cell_type":"markdown","metadata":{},"source":["## **8. Lemmatization**\n","\n","- [Lemmatization](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/) is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n","\n","- Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. \n","\n","- Lemmatization makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, lemmatization is prefered over stemming."]},{"cell_type":"code","execution_count":78,"metadata":{"trusted":true},"outputs":[],"source":["from textblob import Word"]},{"cell_type":"code","execution_count":79,"metadata":{"trusted":true},"outputs":[],"source":["def lemmatization(df):\n","    df['tweet'] = df['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n","    print(df['tweet'].head())"]},{"cell_type":"code","execution_count":80,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    father dysfunctional selfish drag kid dysfunct...\n","1    thanks #lyft credit can't use cause offer whee...\n","2                                       bihday majesty\n","3    #model take time urð±!!! ðððð ð...\n","4                      factsguide: society #motivation\n","Name: tweet, dtype: object\n"]}],"source":["lemmatization(train)"]},{"cell_type":"code","execution_count":81,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    #studiolife #aislife #requires #passion #dedic...\n","1    #white #supremacists want everyone see new â...\n","2    safe way heal #acne!! #altwaystoheal #healthy ...\n","3    hp cursed child book reservation already? yes,...\n","4    3rd #bihday amazing, hilarious #nephew eli ahm...\n","Name: tweet, dtype: object\n"]}],"source":["lemmatization(test)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
