{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Imports & Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:35.801749800Z",
     "start_time": "2024-11-03T09:22:09.201596100Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install emoji\n",
    "!pip install pyspellchecker\n",
    "!pip install textblob\n",
    "!pip install ftfy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "src_path = os.path.abspath(os.path.join(current_dir, '../../../1. Explorative Analysis & Preprocessing/nasiba/'))\n",
    "os.chdir(src_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:40.171972400Z",
     "start_time": "2024-11-03T09:22:35.805747400Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import Counter\n",
    "from spellchecker import SpellChecker\n",
    "import os\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from ftfy import fix_encoding\n",
    "import spacy\n",
    "\n",
    "\n",
    "from src.preprocessing.cleaning.clean_data_generic_functions import clean_misspelled_words, correct_misspelled_words_in_sentence,create_word_counter,expand_slang,expand_shortcuts,extract_emojis, to_lowercase, to_lowercase_if_string, handle_hashtags, handle_userhandles, lemmatize, remove_word_from_column\n",
    "from src.preprocessing.cleaning.clean_data_generic_functions import remove_special_characters, remove_digits, remove_duplicates, remove_emoji_in_sentence, remove_emojis, remove_freqwords,remove_hashtag_sign_from_tweet,remove_least_frequent_words, remove_most_frequent_words, remove_punctuation, remove_stop_words, remove_url_from_tweet, replace_emoji_in_sentence, replace_emojis\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:40.443971100Z",
     "start_time": "2024-11-03T09:22:40.171972400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nasiba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nasiba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nasiba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Nasiba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Nasiba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Einladen der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:40.554972200Z",
     "start_time": "2024-11-03T09:22:40.443971100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "source_filepath_name = os.path.abspath(os.path.join(src_path, 'data/train.csv'))\n",
    "df_origin = pd.read_csv(source_filepath_name, encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:40.748969900Z",
     "start_time": "2024-11-03T09:22:40.569976Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_filepath_name_test = os.path.abspath(os.path.join(src_path, 'data/test.csv'))\n",
    "df_origin_test = pd.read_csv(source_filepath_name_test, encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenvorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:45.895945100Z",
     "start_time": "2024-11-03T09:22:45.806315500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_clean_base = df_origin.copy()\n",
    "df_clean_base_test = df_origin_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1. Datenbereinigung allgemein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Duplikatenentfernung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:45.905996100Z",
     "start_time": "2024-11-03T09:22:45.840985900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_base.drop_duplicates(inplace=True)\n",
    "df_clean_base_test.drop_duplicates(inplace=True)\n",
    "df_clean_base.duplicated().sum()\n",
    "df_clean_base_test.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:48.371485800Z",
     "start_time": "2024-11-03T09:22:45.905996100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_clean_base[\"tweet\"] = df_clean_base['tweet'].apply(fix_encoding)\n",
    "df_clean_base_test[\"tweet\"] = df_clean_base_test['tweet'].apply(fix_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Datenbereinigung speziell für Textverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reihenfolge Datenbereinigung\n",
    "\n",
    "Sinnvolle Reihenfolge für die Ausführung der einzelnen Bereinigungsschritte\n",
    "1. Groß-/Kleinschreibung normalisieren\n",
    "2. Abkürzungen/Slang auflösen\n",
    "3. Falsch geschriebene Wörter korrigieren\n",
    "4. Negationen auflösen (falls möglich)\n",
    "5. Umgang mit User-Handles\n",
    "6. Umgang mit Hashtags (in Text belassen UND in gesonderte Spalte extrahieren)\n",
    "7. Emojis konvertieren (in Text konvertieren UND in gesonderte Spalte extrahieren)\n",
    "8. Unnötige Zeichen entfernen\n",
    "    a. Links\n",
    "    b. html tags\n",
    "    d. Punktuation\n",
    "    e. Sonderzeichen\n",
    "    f. Zeilenumbrüche\n",
    "    g. Zahlen\n",
    "    h. das Wort amp\n",
    "9. Stemming / Lemmatization\n",
    "10. Stopwords entfernen\n",
    "11. Most frequent words entfernen\n",
    "12. Rare words entfernen\n",
    "13. Nochmal Duplikate prüfen & entfernen\n",
    "\n",
    "Lemmatization: https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:48.436489200Z",
     "start_time": "2024-11-03T09:22:48.396454600Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_dataframe(base_df):\n",
    "    df_cleaned = base_df.copy()\n",
    "    df_cleaned['tweet_cleaned'] = df_cleaned['tweet']\n",
    "\n",
    "    print(\"Start Cleaning\")\n",
    "    print(\"Cleaning Step 1/18: to_lowercase\")\n",
    "    df_cleaned = to_lowercase(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 2/18: expand_shortcuts\")\n",
    "    df_cleaned = expand_shortcuts(df_cleaned,'tweet_cleaned')\n",
    "    # print(\"Cleaning Step 3/18: remove_negations - SKIP\")\n",
    "    # # df_cleaned = remove_negations(df_cleaned)\n",
    "    print(\"Cleaning Step 4/18: handle_userhandles\")\n",
    "    df_cleaned = handle_userhandles(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 5/18: handle_hashtags\")\n",
    "    df_cleaned = handle_hashtags(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 6/18: extract_emojis\")\n",
    "    df_cleaned = extract_emojis(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 7/18: replace_emojis\")\n",
    "    df_cleaned = replace_emojis(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 8/18: remove_emojis - SKIP\")\n",
    "    # df_cleaned = remove_emojis(df_cleaned)\n",
    "    print(\"Cleaning Step 9/18: remove_url_from_tweet\")\n",
    "    df_cleaned = remove_url_from_tweet(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 10/18: remove_punctuation\")\n",
    "    df_cleaned = remove_punctuation(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 11/18: remove_special_characters\")\n",
    "    df_cleaned = remove_special_characters(df_cleaned, 'tweet_cleaned')\n",
    "    print(\"Cleaning Step 12/18: remove_digis\")\n",
    "    df_cleaned = remove_digits(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 13/18: remove_word_from_column: amp\")\n",
    "    df_cleaned = remove_word_from_column(df=df_cleaned, column_name=\"tweet_cleaned\",word=\"amp\")\n",
    "    print(\"Cleaning Step 14/18: lemmatize\")\n",
    "    df_cleaned = lemmatize(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 15/18: remove_stop_words\")\n",
    "    df_cleaned = remove_stop_words(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 16/18: remove_most_frequent_words\")\n",
    "    df_cleaned = remove_most_frequent_words(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 17/18: remove_least_frequent_words\")\n",
    "    df_cleaned = remove_least_frequent_words(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 18/18: remove_duplicates\")\n",
    "    df_cleaned = remove_duplicates(df_cleaned,'tweet_cleaned')\n",
    "    print(\"All Cleaning done\")\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Datenbereinigung Train Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cleaning\n",
      "Cleaning Step 1/18: to_lowercase\n",
      "Cleaning Step 2/18: expand_shortcuts\n",
      "Cleaning Step 4/18: handle_userhandles\n",
      "Cleaning Step 5/18: handle_hashtags\n",
      "Cleaning Step 6/18: extract_emojis\n",
      "Cleaning Step 7/18: replace_emojis\n",
      "Cleaning Step 8/18: remove_emojis - SKIP\n",
      "Cleaning Step 9/18: remove_url_from_tweet\n",
      "Cleaning Step 10/18: remove_punctuation\n",
      "Cleaning Step 11/18: remove_special_characters\n",
      "Cleaning Step 12/18: remove_digis\n",
      "Cleaning Step 13/18: remove_word_from_column: amp\n",
      "Cleaning Step 14/18: lemmatize\n",
      "Cleaning Step 15/18: remove_stop_words\n",
      "Cleaning Step 16/18: remove_most_frequent_words\n",
      "Cleaning Step 17/18: remove_least_frequent_words\n",
      "Cleaning Step 18/18: remove_duplicates\n",
      "All Cleaning done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_cleaned = clean_dataframe(df_clean_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Datenbereinigung Test Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cleaning\n",
      "Cleaning Step 1/18: to_lowercase\n",
      "Cleaning Step 2/18: expand_shortcuts\n",
      "Cleaning Step 4/18: handle_userhandles\n",
      "Cleaning Step 5/18: handle_hashtags\n",
      "Cleaning Step 6/18: extract_emojis\n",
      "Cleaning Step 7/18: replace_emojis\n",
      "Cleaning Step 8/18: remove_emojis - SKIP\n",
      "Cleaning Step 9/18: remove_url_from_tweet\n",
      "Cleaning Step 10/18: remove_punctuation\n",
      "Cleaning Step 11/18: remove_special_characters\n",
      "Cleaning Step 12/18: remove_digis\n",
      "Cleaning Step 13/18: remove_word_from_column: amp\n",
      "Cleaning Step 14/18: lemmatize\n",
      "Cleaning Step 15/18: remove_stop_words\n",
      "Cleaning Step 16/18: remove_most_frequent_words\n",
      "Cleaning Step 17/18: remove_least_frequent_words\n",
      "Cleaning Step 18/18: remove_duplicates\n",
      "All Cleaning done\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_test = clean_dataframe(df_clean_base_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:28:14.696353600Z",
     "start_time": "2024-11-03T09:28:14.366002500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "filepath_name = os.path.abspath(os.path.join(src_path, 'data/train_cleaned.csv'))\n",
    "df_cleaned.to_csv(filepath_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:56:11.242992900Z",
     "start_time": "2024-11-03T11:56:11.095828700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath_name = os.path.abspath(os.path.join(src_path, 'data/test_cleaned.csv'))\n",
    "df_cleaned_test.to_csv(filepath_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
