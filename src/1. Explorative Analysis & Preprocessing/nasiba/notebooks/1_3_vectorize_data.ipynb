{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def clear_python_cache(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for dir_name in dirs:\n",
    "            if dir_name == \"__pycache__\":\n",
    "                cache_dir = os.path.join(root, dir_name)\n",
    "                shutil.rmtree(cache_dir)\n",
    "                print(f\"GelÃ¶scht: {cache_dir}\")\n",
    "\n",
    "#  Wurzelverzeichnis:\n",
    "project_dir = os.getcwd()  # Aktuelles Verzeichnis\n",
    "clear_python_cache(project_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Nasiba\\\\Documents\\\\Repos_Clones\\\\WebScience24\\\\src\\\\1. Explorative Analysis & Preprocessing\\\\nasiba'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "src_path = os.path.abspath(os.path.join(current_dir, '../../../1. Explorative Analysis & Preprocessing/nasiba/'))\n",
    "os.chdir(src_path) \n",
    "src_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Nasiba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import Counter\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from src.preprocessing.cleaning.clean_data_generic_functions import remove_na_from_column\n",
    "from src.preprocessing.vectorizing.vector_functions import vectorize_bag_of_words, vectorize_tfidf, vectorize_word2vec,  vectorize_fasttext, compare_vectorization_methods\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_name = os.path.abspath(os.path.join(src_path, 'data/train_cleaned.csv'))\n",
    "df_cleaned = pd.read_csv(filepath_name, encoding='utf-8')\n",
    "df_cleaned = remove_na_from_column(df=df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aamp' 'aap' 'ab' ... 'zootopia' 'zoro' 'zzz']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X_train_bow, X_test_bow, y_train, y_test, vectorizer  = vectorize_bag_of_words(df=df_cleaned, text_column=\"tweet_cleaned\",\n",
    " \n",
    "                                                                                           label_column=\"label\")\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten (TF-IDF-Matrix):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Feature-Namen:\n",
      "['aamp' 'aap' 'ab' ... 'zootopia' 'zoro' 'zzz']\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf, X_test_tfidf, y_train, y_test, tfidf_vectorizer = vectorize_tfidf(df=df_cleaned, text_column='tweet_cleaned', \n",
    "                                                                                 label_column=\"label\")\n",
    "\n",
    "print(\"Trainingsdaten (TF-IDF-Matrix):\")\n",
    "print(X_train_tfidf.toarray())\n",
    "print(\"Feature-Namen:\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Word Embeddings Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nasiba\\Documents\\Repos_Clones\\WebScience24\\src\\1. Explorative Analysis & Preprocessing\\nasiba\\src\\preprocessing\\vectorizing\\vector_functions.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[text_column] = df[text_column].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erste Trainingsvektoren:\n",
      "[[ 8.86157334e-03  2.93546597e-01  2.85350674e-02 ... -1.24635624e-01\n",
      "   1.62490445e-01 -3.61440731e-02]\n",
      " [-1.63146095e-03  1.87353872e-01  2.77274716e-02 ... -1.08744206e-01\n",
      "   1.29523960e-01 -1.90963989e-02]\n",
      " [ 1.73104461e-04  2.51385506e-01  8.00646017e-02 ... -1.70219562e-01\n",
      "   1.58880664e-01 -4.80226235e-02]\n",
      " [ 4.75607536e-03  2.18909404e-01  2.92647363e-02 ... -1.12264831e-01\n",
      "   1.30614392e-01 -2.67828682e-02]\n",
      " [-3.23979477e-02  2.21074512e-01  6.07593423e-02 ... -2.51384515e-01\n",
      "   1.29672541e-01 -3.35288490e-02]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_w2v, X_test_w2v, y_train, y_test, w2v_model = vectorize_word2vec(df=df_cleaned, text_column =\"tweet_cleaned\", label_column=\"label\")\n",
    "\n",
    "print(\"Erste Trainingsvektoren:\")\n",
    "print(X_train_w2v[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erste FastText-Vektoren (Trainingsdaten):\n",
      "[[-0.29386375 -0.18855044 -0.28949956  0.1476144  -0.01624956  0.13286856\n",
      "  -0.2226717   0.42894903  0.20227926 -0.34489603  0.14063459 -0.17879955\n",
      "  -0.02656796  0.35571882  0.14739158 -0.18100273 -0.38855459 -0.38690186\n",
      "   0.03949845 -0.1420191  -0.38428877  0.12217931 -0.43021867 -0.27014789\n",
      "  -0.1585444  -0.19736542 -0.1047727  -0.11022075  0.39326515  0.0440884\n",
      "   0.00890011 -0.11697213  0.26496852  0.09481627 -0.24331411  0.0848361\n",
      "  -0.30035191  0.16245278 -0.38106031  0.10987324  0.03810994 -0.11336832\n",
      "  -0.36291646 -0.35525489 -0.30596789 -0.63242    -0.13390998 -0.35441031\n",
      "  -0.12001625  0.02514511  0.18910217  0.00634514  0.21914639 -0.05554438\n",
      "  -0.13937775 -0.30562437 -0.07341961 -0.06837617 -0.10356644 -0.14365796\n",
      "   0.35412748 -0.25115391 -0.28434361  0.4578321   0.03951711  0.10478977\n",
      "  -0.09506598 -0.16083588 -0.14083032  0.36286939  0.15732638  0.3946838\n",
      "  -0.32672708 -0.34217629  0.59179797 -0.22814074  0.11298252 -0.28843242\n",
      "  -0.32987007  0.52645633  0.0501385   0.03011017 -0.16420562 -0.01371829\n",
      "  -0.17916377 -0.10769561 -0.01814499 -0.0876746  -0.14521292  0.15977002\n",
      "  -0.45867692 -0.06157656 -0.01529224  0.05548793 -0.36813689  0.21500242\n",
      "   0.05364267 -0.34932151  0.08393984  0.47701884]\n",
      " [-0.06559157  0.09021058 -0.12046528  0.11623377  0.23109994  0.18426524\n",
      "  -0.02677765  0.28227061  0.54220219 -0.38705356  0.02206335  0.1041011\n",
      "  -0.08640491  0.48932449  0.16009805 -0.12804702 -0.219634   -0.38999196\n",
      "  -0.14537058 -0.26628014 -0.21321763  0.45812123 -0.20700245 -0.29758027\n",
      "  -0.27630261  0.02980136 -0.20673033  0.07404204  0.40286972 -0.08083069\n",
      "  -0.15796475  0.03352352  0.30347147 -0.16988805 -0.28196473  0.17944631\n",
      "   0.06926451  0.1022398  -0.03582516 -0.03565286  0.27344941 -0.10532788\n",
      "  -0.15148835 -0.28552662 -0.32343747 -0.30627772  0.02694427 -0.16596892\n",
      "  -0.53382053  0.28965395  0.17715106  0.25035533  0.1109958   0.00452135\n",
      "   0.04364937 -0.2443161   0.11629698 -0.10160199 -0.06431268 -0.10787215\n",
      "   0.26526456 -0.21099969 -0.20897489  0.32621462  0.22312398  0.25758193\n",
      "   0.07474155 -0.12256824 -0.37670905  0.30036914 -0.00356908  0.22346781\n",
      "  -0.11833267 -0.26446798  0.2042114   0.0535953   0.26970849 -0.04539838\n",
      "  -0.27119825  0.41994827 -0.05890063 -0.25315498  0.08587257 -0.04327831\n",
      "  -0.21438619 -0.06297731 -0.1350496  -0.04067079 -0.09228058  0.07265919\n",
      "  -0.5490243  -0.2313467  -0.0669836  -0.18066368 -0.50236479  0.42313611\n",
      "   0.17413028 -0.15391451  0.15771826  0.64835555]\n",
      " [-0.15893781 -0.37603559 -0.21830507  0.13569292  0.17842773  0.15570964\n",
      "  -0.0565702   0.48516263 -0.00129184 -0.43277982  0.03094721 -0.28101444\n",
      "  -0.2348281   0.50044056  0.22337846 -0.02371105 -0.48404962 -0.2332783\n",
      "   0.10270611  0.10906721 -0.59548533  0.42605441 -0.29437851  0.22615588\n",
      "  -0.24370245 -0.15404353 -0.43614066  0.0401223   0.60458883  0.00899699\n",
      "  -0.11984616  0.05630926  0.21681339 -0.15853795 -0.1185222   0.00842407\n",
      "  -0.22650605  0.13144153 -0.32049314  0.07069241  0.03633094 -0.10649988\n",
      "  -0.19458259 -0.20488105 -0.2205267  -0.84592658  0.07387862 -0.13883835\n",
      "  -0.25910616  0.12361139  0.2939196   0.03595323  0.16007545  0.03903021\n",
      "   0.00467755 -0.47851387  0.05939282 -0.11062295  0.0131952  -0.1392637\n",
      "   0.24146058 -0.25245814 -0.26881351  0.69710511  0.11731476  0.12861798\n",
      "   0.09735677 -0.12696644 -0.11249095  0.63531289  0.33675384  0.52592983\n",
      "  -0.36269492 -0.04278655  0.48938669 -0.12544683  0.265269   -0.20927527\n",
      "  -0.38227438  0.27645086 -0.31058547  0.07611889  0.06001194 -0.27012207\n",
      "  -0.12459795 -0.34437577  0.0884874  -0.27711291 -0.25765163  0.12345121\n",
      "  -0.48205692 -0.04790838  0.0334243  -0.16049681 -0.49441821  0.38916639\n",
      "   0.27230422 -0.35705362  0.16522663  0.44569281]\n",
      " [-0.27808341 -0.11045658 -0.22700584  0.11732838  0.23535368  0.14787536\n",
      "   0.04663993  0.27739477  0.15947376 -0.38837493  0.18861075 -0.00651145\n",
      "  -0.2365954   0.54106998  0.17072871 -0.05809873 -0.34853585 -0.27079314\n",
      "  -0.11566063 -0.06519661 -0.36048973  0.42159407 -0.26402285 -0.02652281\n",
      "  -0.25833616  0.07157224 -0.32046159 -0.00687539  0.50867608 -0.00168648\n",
      "  -0.20235657 -0.01389586  0.27863011  0.10191163 -0.28422483  0.05492687\n",
      "   0.03332813  0.14659111 -0.18094448  0.0224004   0.10953044 -0.1644039\n",
      "  -0.03341338 -0.15919389 -0.26216594 -0.59388692  0.05567417 -0.0973042\n",
      "  -0.32338783  0.21824089  0.16461339  0.04138364  0.28748243  0.04081346\n",
      "  -0.10063504 -0.27393559 -0.11487484 -0.295229   -0.11388032 -0.03941577\n",
      "   0.29534887 -0.26273688 -0.20973871  0.46703888  0.2139582   0.03754467\n",
      "   0.13326522 -0.19933578 -0.19989614  0.47973061  0.19104154  0.2922905\n",
      "  -0.20885662 -0.16637233  0.37051055  0.01676136  0.3008175  -0.06574893\n",
      "  -0.3751721   0.31466761 -0.2021708  -0.01368608 -0.0513921  -0.13584589\n",
      "  -0.2190647  -0.10370262  0.09375908 -0.13373468 -0.19346247  0.1767109\n",
      "  -0.50058543 -0.14360887 -0.0327917  -0.16794448 -0.50100846  0.33892485\n",
      "   0.14742487 -0.29574394  0.10271309  0.59260913]\n",
      " [-0.09677536  0.02637046 -0.11811858  0.09315918  0.37902776  0.34269046\n",
      "   0.15583896  0.0531266  -0.12301917 -0.38727489  0.1017174  -0.13445755\n",
      "  -0.15683679  0.8056824   0.14488234 -0.24366622 -0.3257399  -0.1584896\n",
      "  -0.03176296  0.08620455 -0.62797513  0.22683577 -0.32814379 -0.17059879\n",
      "  -0.10977007  0.06993726 -0.55449498 -0.06303393  0.76951797  0.0868251\n",
      "  -0.36044312 -0.30924236  0.25165773 -0.05249524 -0.24405329  0.19727001\n",
      "  -0.18533653  0.25613183 -0.40656176  0.13216769  0.10991129 -0.22093142\n",
      "  -0.02881837 -0.3906519  -0.37133974 -0.33385873 -0.16009515  0.05378607\n",
      "  -0.38450839  0.46362689  0.04668202  0.13398796  0.38526449 -0.00619775\n",
      "  -0.09871263 -0.54878414 -0.09506184 -0.30350608 -0.10208871 -0.01620049\n",
      "   0.26225521 -0.25017576 -0.20697371  0.77684075  0.3841463   0.10845687\n",
      "   0.15249742 -0.18487348 -0.01342191  0.48260149  0.08526195  0.36414601\n",
      "  -0.20149434 -0.15693565  0.77612514 -0.06470947  0.15205261  0.03686112\n",
      "  -0.14620694  0.39057263 -0.21385411  0.20742881 -0.05325615  0.11508053\n",
      "  -0.04848114  0.15723263 -0.1240438  -0.13765135 -0.08013657  0.00675453\n",
      "  -0.53855246 -0.209652   -0.27263559 -0.35605556 -0.43992394  0.2201841\n",
      "  -0.11740288 -0.4532016   0.00630504  0.71996354]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_ft, X_test_ft, y_train, y_test, ft_model = vectorize_fasttext(\n",
    "    df=df_cleaned,\n",
    "    text_column=\"tweet_cleaned\",\n",
    "    label_column=\"label\",\n",
    "    vector_size=100,\n",
    "    window=3,\n",
    "    min_count=1\n",
    ")\n",
    "\n",
    "print(\"Erste FastText-Vektoren (Trainingsdaten):\")\n",
    "print(X_train_ft[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleiche 4 Methoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vektorisierung: Bag of Words\n",
      "Vektorisierung: TF-IDF\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_vectorization_methods\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_cleaned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtweet_cleaned\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Diagramm erstellen\u001b[39;00m\n\u001b[0;32m      4\u001b[0m results\u001b[38;5;241m.\u001b[39mplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m\"\u001b[39m], kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m\"\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Nasiba\\Documents\\Repos_Clones\\WebScience24\\src\\1. Explorative Analysis & Preprocessing\\nasiba\\src\\preprocessing\\vectorizing\\vector_functions.py:224\u001b[0m, in \u001b[0;36mcompare_vectorization_methods\u001b[1;34m(df, text_column, label_column)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# 2. TF-IDF\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVektorisierung: TF-IDF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 224\u001b[0m X_train_tfidf, X_test_tfidf, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mvectorize_tfidf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train_tfidf, y_train)\n\u001b[0;32m    226\u001b[0m y_pred_tfidf \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test_tfidf)\n",
      "File \u001b[1;32mc:\\Users\\Nasiba\\Documents\\Repos_Clones\\WebScience24\\src\\1. Explorative Analysis & Preprocessing\\nasiba\\src\\preprocessing\\vectorizing\\vector_functions.py:78\u001b[0m, in \u001b[0;36mvectorize_tfidf\u001b[1;34m(df, text_column, label_column, test_size, random_state)\u001b[0m\n\u001b[0;32m     72\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m     75\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> 78\u001b[0m X_train_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m X_test_tfidf \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train_tfidf, X_test_tfidf, y_train, y_test, tfidf_vectorizer\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[1;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:103\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:236\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    233\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "\n",
    "results = compare_vectorization_methods(df=df_cleaned, text_column=\"tweet_cleaned\", label_column=\"label\")\n",
    "\n",
    "\n",
    "results.plot(x=\"Method\", y=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"], kind=\"bar\", figsize=(10, 6))\n",
    "plt.title(\"Vergleich der Vektorisierungsmethoden\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Vektorisierungsmethode\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
