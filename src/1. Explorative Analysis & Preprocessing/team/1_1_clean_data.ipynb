{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Das Verzeichnis festlegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achtung, der globale Pfad wurde auf C:\\Users\\Nasiba\\Documents\\1 Master Data Science\\Projektpraktikum\\WebScience24 gesetzt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Zum übergeordneten Ordner 'src' wechseln\n",
    "current_path = Path().resolve()\n",
    "src_path = current_path.parents[2]  # Zwei Stufen nach oben\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "print(f\"Achtung, der globale Pfad wurde auf {src_path} gesetzt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Imports & Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:35.801749800Z",
     "start_time": "2024-11-03T09:22:09.201596100Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install emoji\n",
    "!pip install pyspellchecker\n",
    "!pip install textblob\n",
    "!pip install ftfy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install ekphrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:40.171972400Z",
     "start_time": "2024-11-03T09:22:35.805747400Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nasiba\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n",
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import Counter\n",
    "from spellchecker import SpellChecker\n",
    "import os\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from ftfy import fix_encoding\n",
    "import spacy\n",
    "\n",
    "\n",
    "from src.functions.clean_data_generic_functions import clean_misspelled_words, correct_misspelled_words_in_sentence,create_word_counter,expand_slang,expand_shortcuts,extract_emojis, to_lowercase, to_lowercase_if_string, handle_hashtags, handle_userhandles, lemmatize, remove_word_from_column, remove_na_from_column, replace_text_smileys, segment_tweets\n",
    "from src.functions.clean_data_generic_functions import remove_special_characters, remove_digits, remove_duplicates, remove_emoji_in_sentence, remove_emojis, remove_freqwords,remove_hashtag_sign_from_tweet,remove_least_frequent_words, remove_most_frequent_words, remove_punctuation, remove_stop_words, remove_url_from_tweet, replace_emoji_in_sentence, replace_emojis\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:40.443971100Z",
     "start_time": "2024-11-03T09:22:40.171972400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Nasiba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nasiba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nasiba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Nasiba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Nasiba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Beladung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:40.554972200Z",
     "start_time": "2024-11-03T09:22:40.443971100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "source_filepath_name = os.path.abspath(os.path.join(src_path, 'data/twitter_hate-speech/train.csv'))\n",
    "\n",
    "df_origin = pd.read_csv(source_filepath_name, encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:40.748969900Z",
     "start_time": "2024-11-03T09:22:40.569976Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_filepath_name_test = os.path.abspath(os.path.join(src_path, 'data/twitter_hate-speech/test.csv'))\n",
    "df_origin_test = pd.read_csv(source_filepath_name_test, encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenvorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:45.895945100Z",
     "start_time": "2024-11-03T09:22:45.806315500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_clean_base = df_origin.copy()\n",
    "df_clean_base_test = df_origin_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1. Datenbereinigung allgemein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Duplikatenentfernung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:45.905996100Z",
     "start_time": "2024-11-03T09:22:45.840985900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_base.drop_duplicates(inplace=True)\n",
    "\n",
    "df_clean_base.duplicated().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:48.371485800Z",
     "start_time": "2024-11-03T09:22:45.905996100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_clean_base[\"tweet\"] = df_clean_base['tweet'].apply(fix_encoding)\n",
    "df_clean_base_test[\"tweet\"] = df_clean_base_test['tweet'].apply(fix_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Datenbereinigung speziell für Textverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reihenfolge Datenbereinigung\n",
    "\n",
    "Sinnvolle Reihenfolge für die Ausführung der einzelnen Bereinigungsschritte\n",
    "1. Groß-/Kleinschreibung normalisieren\n",
    "3. Die Textemojis auflösen und in emojis umwandeln\n",
    "4. Abkürzungen/Slang auflösen\n",
    "\n",
    "7. Umgang mit User-Handles\n",
    "8. Umgang mit Hashtags (in Text belassen UND in gesonderte Spalte extrahieren)\n",
    "9. Die zusammengeschriebene Wörter trennen\n",
    "9. Emojis konvertieren (in Text konvertieren UND in gesonderte Spalte extrahieren)\n",
    "10. Unnötige Zeichen entfernen\n",
    "    a. Links\n",
    "    b. html tags\n",
    "    d. Punktuation\n",
    "    e. Sonderzeichen\n",
    "    f. Zeilenumbrüche\n",
    "    g. Zahlen\n",
    "    h. das Wort amp\n",
    "11. Stemming / Lemmatization\n",
    "12. Stopwords entfernen\n",
    "13. Nochmal Duplikate prüfen & entfernen & leere Zeilen entfernen\n",
    "\n",
    "Lemmatization: https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:22:48.436489200Z",
     "start_time": "2024-11-03T09:22:48.396454600Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_dataframe(base_df):\n",
    "    df_cleaned = base_df.copy()\n",
    "    df_cleaned['tweet_cleaned'] = df_cleaned['tweet']\n",
    "\n",
    "    print(\"Start Cleaning\")\n",
    "   \n",
    "\n",
    "    df_cleaned = to_lowercase(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 2/19: expand_shortcuts\")\n",
    "    df_cleaned = replace_text_smileys(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 2/19: replace_text_smileys\")\n",
    "    df_cleaned = expand_shortcuts(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 4/19: handle_userhandles\")\n",
    "    df_cleaned = handle_userhandles(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 5/19: handle_hashtags\")\n",
    "    df_cleaned = handle_hashtags(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 1/19: segment_tweets\")\n",
    "   \n",
    "    df_cleaned = segment_tweets(df_cleaned,'tweet_cleaned')\n",
    "   \n",
    "    print(\"Cleaning Step 6/19: extract_emojis\")\n",
    "    df_cleaned = extract_emojis(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 7/19: replace_emojis\")\n",
    "    print(\"Cleaning Step 2/19: replace_text_smileys\")\n",
    "    df_cleaned = replace_emojis(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 9/19: remove_url_from_tweet\")\n",
    "    df_cleaned = remove_url_from_tweet(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 10/19: remove_punctuation\")\n",
    "    df_cleaned = remove_punctuation(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 11/19: remove_special_characters\")\n",
    "    df_cleaned = remove_special_characters(df_cleaned, 'tweet_cleaned')\n",
    "    print(\"Cleaning Step 12/19: remove_digis\")\n",
    "    df_cleaned = remove_digits(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 13/19: remove_word_from_column: amp\")\n",
    "    df_cleaned = remove_word_from_column(df=df_cleaned, column_name=\"tweet_cleaned\",word=\"amp\")\n",
    "    print(\"Cleaning Step 14/19: lemmatize\")\n",
    "    df_cleaned = lemmatize(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 15/19: remove_stop_words\")\n",
    "    df_cleaned = remove_stop_words(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 16/19: remove_most_frequent_words\")\n",
    "    df_cleaned = remove_most_frequent_words(df_cleaned,'tweet_cleaned')\n",
    "    # print(\"Cleaning Step 17/19: remove_least_frequent_words\")\n",
    "    # df_cleaned = remove_least_frequent_words(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 18/19: remove_duplicates\")\n",
    "    df_cleaned = remove_duplicates(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 19/19: remove_nans\")\n",
    "    df_cleaned=remove_na_from_column(df=df_cleaned, column_name=\"tweet_cleaned\")\n",
    "    print(\"All Cleaning done\")\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def clean_dataframe_test(base_df):\n",
    "    df_cleaned = base_df.copy()\n",
    "    df_cleaned['tweet_cleaned'] = df_cleaned['tweet']\n",
    "\n",
    "    print(\"Start Cleaning\")\n",
    "   \n",
    "\n",
    "    df_cleaned = to_lowercase(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 2/19: expand_shortcuts\")\n",
    "    df_cleaned = replace_text_smileys(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 2/19: replace_text_smileys\")\n",
    "    df_cleaned = expand_shortcuts(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 4/19: handle_userhandles\")\n",
    "    df_cleaned = handle_userhandles(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 5/19: handle_hashtags\")\n",
    "    df_cleaned = handle_hashtags(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 1/19: segment_tweets\")\n",
    "   \n",
    "    df_cleaned = segment_tweets(df_cleaned,'tweet_cleaned')\n",
    "   \n",
    "    print(\"Cleaning Step 6/19: extract_emojis\")\n",
    "    df_cleaned = extract_emojis(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 7/19: replace_emojis\")\n",
    "    print(\"Cleaning Step 2/19: replace_text_smileys\")\n",
    "    df_cleaned = replace_emojis(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 9/19: remove_url_from_tweet\")\n",
    "    df_cleaned = remove_url_from_tweet(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 10/19: remove_punctuation\")\n",
    "    df_cleaned = remove_punctuation(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 11/19: remove_special_characters\")\n",
    "    df_cleaned = remove_special_characters(df_cleaned, 'tweet_cleaned')\n",
    "    print(\"Cleaning Step 12/19: remove_digis\")\n",
    "    df_cleaned = remove_digits(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 13/19: remove_word_from_column: amp\")\n",
    "    df_cleaned = remove_word_from_column(df=df_cleaned, column_name=\"tweet_cleaned\",word=\"amp\")\n",
    "    print(\"Cleaning Step 14/19: lemmatize\")\n",
    "    df_cleaned = lemmatize(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 15/19: remove_stop_words\")\n",
    "    df_cleaned = remove_stop_words(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 16/19: remove_most_frequent_words\")\n",
    "    df_cleaned = remove_most_frequent_words(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 17/19: remove_least_frequent_words\")\n",
    "    df_cleaned = remove_least_frequent_words(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 18/19: remove_duplicates\")\n",
    "    df_cleaned = remove_duplicates(df_cleaned,'tweet_cleaned')\n",
    "    print(\"Cleaning Step 19/19: remove_nans\")\n",
    "    df_cleaned=remove_na_from_column(df=df_cleaned, column_name=\"tweet_cleaned\")\n",
    "    print(\"All Cleaning done\")\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Datenbereinigung Train Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cleaning\n",
      "Cleaning Step 2/19: expand_shortcuts\n",
      "Cleaning Step 2/19: replace_text_smileys\n",
      "Cleaning Step 4/19: handle_userhandles\n",
      "Cleaning Step 5/19: handle_hashtags\n",
      "Cleaning Step 1/19: segment_tweets\n",
      "Cleaning Step 6/19: extract_emojis\n",
      "Cleaning Step 7/19: replace_emojis\n",
      "Cleaning Step 2/19: replace_text_smileys\n",
      "Cleaning Step 9/19: remove_url_from_tweet\n",
      "Cleaning Step 10/19: remove_punctuation\n",
      "Cleaning Step 11/19: remove_special_characters\n",
      "Cleaning Step 12/19: remove_digis\n",
      "Cleaning Step 13/19: remove_word_from_column: amp\n",
      "Cleaning Step 14/19: lemmatize\n",
      "Cleaning Step 15/19: remove_stop_words\n",
      "Cleaning Step 16/19: remove_most_frequent_words\n",
      "Cleaning Step 18/19: remove_duplicates\n",
      "Cleaning Step 19/19: remove_nans\n",
      "All Cleaning done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_cleaned = clean_dataframe(df_clean_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Datenbereinigung Test Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cleaning\n",
      "Cleaning Step 2/19: expand_shortcuts\n",
      "Cleaning Step 2/19: replace_text_smileys\n",
      "Cleaning Step 4/19: handle_userhandles\n",
      "Cleaning Step 5/19: handle_hashtags\n",
      "Cleaning Step 1/19: segment_tweets\n",
      "Cleaning Step 6/19: extract_emojis\n",
      "Cleaning Step 7/19: replace_emojis\n",
      "Cleaning Step 2/19: replace_text_smileys\n",
      "Cleaning Step 9/19: remove_url_from_tweet\n",
      "Cleaning Step 10/19: remove_punctuation\n",
      "Cleaning Step 11/19: remove_special_characters\n",
      "Cleaning Step 12/19: remove_digis\n",
      "Cleaning Step 13/19: remove_word_from_column: amp\n",
      "Cleaning Step 14/19: lemmatize\n",
      "Cleaning Step 15/19: remove_stop_words\n",
      "Cleaning Step 16/19: remove_most_frequent_words\n",
      "Cleaning Step 18/19: remove_duplicates\n",
      "Cleaning Step 19/19: remove_nans\n",
      "All Cleaning done\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_test = clean_dataframe_test(df_clean_base_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_cleaned</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emojis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14977</th>\n",
       "      <td>0</td>\n",
       "      <td>@user #thanks for #sharing seattle sounders fc,   #sunday :) #sharingiscaring (via ➪  grow followers 🆓)</td>\n",
       "      <td>thank share seattle sounder fc sunday smiley sharing care via rightwards arrow grow follower free button</td>\n",
       "      <td>1</td>\n",
       "      <td>[#thanks, #sharing, #sunday, #sharingiscaring]</td>\n",
       "      <td>rightwards arrow,__FREE_button__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28944</th>\n",
       "      <td>0</td>\n",
       "      <td>@user #thanks for #sharing andrew jones,   #sunday :) #sharingiscaring (via ➪  grow followers 🆓)</td>\n",
       "      <td>thank share andrew jones sunday smiley sharing care via rightwards arrow grow follower free button</td>\n",
       "      <td>1</td>\n",
       "      <td>[#thanks, #sharing, #sunday, #sharingiscaring]</td>\n",
       "      <td>rightwards arrow,__FREE_button__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7187</th>\n",
       "      <td>0</td>\n",
       "      <td>.@user on #periscope: rant!!! orlando, florida 😔😔😔     #hu</td>\n",
       "      <td>periscope rant orlando florida pensive face pensive pensive hu</td>\n",
       "      <td>1</td>\n",
       "      <td>[#periscope, #hu]</td>\n",
       "      <td>__pensive_face__,__pensive_face__,__pensive_face__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24117</th>\n",
       "      <td>0</td>\n",
       "      <td>okay @user i am literally going to scream. you are out of cold brew again!!!! is 6:30am how is this possible! #halp #why   😅☕️</td>\n",
       "      <td>okay literally go scream cold brew possible halp grinning face sweat hot beverage</td>\n",
       "      <td>1</td>\n",
       "      <td>[#halp, #why]</td>\n",
       "      <td>__grinning_face_with_sweat__,__hot_beverage__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23085</th>\n",
       "      <td>0</td>\n",
       "      <td>@user en directo en #periscope: morning at the #gym #workout #aerobics #success #ican #wekhan #love  ❤❤❤❤❤❤❤💪💪💪🐩…</td>\n",
       "      <td>en directo en periscope morning gym work aerobics success khan love red heart red red red red red red flex bicep flex bicep flex bicep poodle …</td>\n",
       "      <td>1</td>\n",
       "      <td>[#periscope, #gym, #workout, #aerobics, #success, #ican, #wekhan, #love]</td>\n",
       "      <td>__red_heart__,__red_heart__,__red_heart__,__red_heart__,__red_heart__,__red_heart__,__red_heart__,__flexed_biceps__,__flexed_biceps__,__flexed_biceps__,__poodle__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29864</th>\n",
       "      <td>0</td>\n",
       "      <td>😌 @user #21hrlater (from that moment of #zenserenity): #theworld, &amp;amp; evrything in't, seems geared 2 make me #anxious  😡 &amp;amp;  😞</td>\n",
       "      <td>relieve face hr later moment zen serenity world ev rything seems gear make anxious enraged disappointed</td>\n",
       "      <td>1</td>\n",
       "      <td>[#21hrlater, #zenserenity, #theworld, #anxious]</td>\n",
       "      <td>__relieved_face__,__enraged_face__,__disappointed_face__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9437</th>\n",
       "      <td>0</td>\n",
       "      <td>@user jts88's phoro: @user 😍 #love #sinkthepink #music #instagay #celebspot</td>\n",
       "      <td>jtss phoro smile face heart - eye love sink pink music insta gay celeb spot</td>\n",
       "      <td>2</td>\n",
       "      <td>[#love, #sinkthepink, #music, #instagay, #celebspot]</td>\n",
       "      <td>__smiling_face_with_heart-eyes__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22134</th>\n",
       "      <td>0</td>\n",
       "      <td>@user mood of the day: peace, love, happiness 💛👍🏻🌺 @user #love #life   #hairstyle #tiger</td>\n",
       "      <td>mood day peace love happiness yellow heart thumb light skin tone hibiscus life hairstyle tiger</td>\n",
       "      <td>2</td>\n",
       "      <td>[#love, #life, #hairstyle, #tiger]</td>\n",
       "      <td>__yellow_heart__,__thumbs_up__,__light_skin_tone__,__hibiscus__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23633</th>\n",
       "      <td>0</td>\n",
       "      <td>@user #thanks for #sharing security tube,   #tuesday :) #sharingiscaring 🔹🆓grow followers ➪  🆓🔹</td>\n",
       "      <td>thank share security tube tuesday smiley sharing care small blue diamond free button grow follower rightward arrow free button small blue diamond</td>\n",
       "      <td>1</td>\n",
       "      <td>[#thanks, #sharing, #tuesday, #sharingiscaring]</td>\n",
       "      <td>__small_blue_diamond__,__FREE_button__,rightwards arrow,__FREE_button__,__small_blue_diamond__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9613</th>\n",
       "      <td>0</td>\n",
       "      <td>.@user no #periscope: #sappyscope let's talk legacy and your value to your family. 😥tears included 😥 …</td>\n",
       "      <td>periscope sappy scope let talk legacy value family sad relieved face tear include sad relieved …</td>\n",
       "      <td>1</td>\n",
       "      <td>[#periscope, #sappyscope]</td>\n",
       "      <td>__sad_but_relieved_face__,__sad_but_relieved_face__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8994</th>\n",
       "      <td>1</td>\n",
       "      <td>excellent! rambling about: the alt right  #tcot #tlot #altright   #blacklivesmatter (@user 👍</td>\n",
       "      <td>excellent ramble alt right tc ot lot alt right black life matter thumb</td>\n",
       "      <td>1</td>\n",
       "      <td>[#tcot, #tlot, #altright, #blacklivesmatter]</td>\n",
       "      <td>__thumbs_up__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24017</th>\n",
       "      <td>0</td>\n",
       "      <td>@user new track as of today :) dedicated to #paulvandyk , so happy he's ok ♥  #dance   #trancefami…</td>\n",
       "      <td>new track today smiley dedicate paul van dyk happy ok heart suit dance trance fami …</td>\n",
       "      <td>1</td>\n",
       "      <td>[#paulvandyk, #dance, #trancefami]</td>\n",
       "      <td>__heart_suit__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>0</td>\n",
       "      <td>🎥 how to: braid sho hair by adding extensions! #anya #howto #diy #salonaks #braids #hair   @user</td>\n",
       "      <td>movie camera braid sho hair add extension anya diy salon aks braid hair</td>\n",
       "      <td>1</td>\n",
       "      <td>[#anya, #howto, #diy, #salonaks, #braids, #hair]</td>\n",
       "      <td>__movie_camera__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>0</td>\n",
       "      <td>it’s #nationalbestfriendsday!👭👫👬 tag your #bff⬇️   📷: @user  #bestfriends #friends #friendship   #bffl</td>\n",
       "      <td>national good friend day woman hold hand woman man hold hand man hold hand tag bff arrow camera friend friend friendship bf fl</td>\n",
       "      <td>1</td>\n",
       "      <td>[#nationalbestfriendsday, #bff, #bestfriends, #friends, #friendship, #bffl]</td>\n",
       "      <td>__women_holding_hands__,__woman_and_man_holding_hands__,__men_holding_hands__,__down_arrow__,__camera__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29116</th>\n",
       "      <td>0</td>\n",
       "      <td>@user goodmorning everyone ☕️😋😘👌🏻.. #feel #free #fly #high   #sad #alone:): #lifeteacheseverything #letthelegsgocrazy ht…</td>\n",
       "      <td>good morning everyone hot beverage face savor food blow kiss ok hand light skin tone feel free fly high sad alone life teach everything let leg go crazy ht …</td>\n",
       "      <td>1</td>\n",
       "      <td>[#feel, #free, #fly, #high, #sad, #alone, #lifeteacheseverything, #letthelegsgocrazy]</td>\n",
       "      <td>__hot_beverage__,__face_savoring_food__,__face_blowing_a_kiss__,__OK_hand__,__light_skin_tone__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13847</th>\n",
       "      <td>1</td>\n",
       "      <td>$2.99! #2016release #ebook #book  the summer that melted everything: a novel by tiffany mcdaniel  via @user</td>\n",
       "      <td>dollar sign release ebook book summer melt everything novel tiffany mcdaniel via</td>\n",
       "      <td>1</td>\n",
       "      <td>[#2016release, #ebook, #book]</td>\n",
       "      <td>dollar sign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5131</th>\n",
       "      <td>0</td>\n",
       "      <td>stay hungry, stay foolish! 👅🎈💎  ph: @user ❤ #crazy   #smile</td>\n",
       "      <td>stay hungry stay foolish tongue balloon gem stone ph red heart crazy smile</td>\n",
       "      <td>1</td>\n",
       "      <td>[#crazy, #smile]</td>\n",
       "      <td>__tongue__,__balloon__,__gem_stone__,__red_heart__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12162</th>\n",
       "      <td>0</td>\n",
       "      <td>royalty 💜💜. makeup: @user aso-oke: @user  beads: @user  #iludioweddings #iludio   #be…</td>\n",
       "      <td>royalty purple heart purple makeup aso - oke bead ilu dio wedding ilu dio …</td>\n",
       "      <td>3</td>\n",
       "      <td>[#iludioweddings, #iludio, #be]</td>\n",
       "      <td>__purple_heart__,__purple_heart__</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  \\\n",
       "id             \n",
       "14977      0   \n",
       "28944      0   \n",
       "7187       0   \n",
       "24117      0   \n",
       "23085      0   \n",
       "29864      0   \n",
       "9437       0   \n",
       "22134      0   \n",
       "23633      0   \n",
       "9613       0   \n",
       "8994       1   \n",
       "24017      0   \n",
       "8377       0   \n",
       "1825       0   \n",
       "29116      0   \n",
       "13847      1   \n",
       "5131       0   \n",
       "12162      0   \n",
       "\n",
       "                                                                                                                                     tweet  \\\n",
       "id                                                                                                                                           \n",
       "14977                              @user #thanks for #sharing seattle sounders fc,   #sunday :) #sharingiscaring (via ➪  grow followers 🆓)   \n",
       "28944                                     @user #thanks for #sharing andrew jones,   #sunday :) #sharingiscaring (via ➪  grow followers 🆓)   \n",
       "7187                                                                           .@user on #periscope: rant!!! orlando, florida 😔😔😔     #hu    \n",
       "24117       okay @user i am literally going to scream. you are out of cold brew again!!!! is 6:30am how is this possible! #halp #why   😅☕️   \n",
       "23085                   @user en directo en #periscope: morning at the #gym #workout #aerobics #success #ican #wekhan #love  ❤❤❤❤❤❤❤💪💪💪🐩…    \n",
       "29864  😌 @user #21hrlater (from that moment of #zenserenity): #theworld, &amp; evrything in't, seems geared 2 make me #anxious  😡 &amp;  😞   \n",
       "9437                                                       @user jts88's phoro: @user 😍 #love #sinkthepink #music #instagay #celebspot       \n",
       "22134                                            @user mood of the day: peace, love, happiness 💛👍🏻🌺 @user #love #life   #hairstyle #tiger    \n",
       "23633                                      @user #thanks for #sharing security tube,   #tuesday :) #sharingiscaring 🔹🆓grow followers ➪  🆓🔹   \n",
       "9613                               .@user no #periscope: #sappyscope let's talk legacy and your value to your family. 😥tears included 😥 …    \n",
       "8994                                          excellent! rambling about: the alt right  #tcot #tlot #altright   #blacklivesmatter (@user 👍   \n",
       "24017                                  @user new track as of today :) dedicated to #paulvandyk , so happy he's ok ♥  #dance   #trancefami…   \n",
       "8377                                     🎥 how to: braid sho hair by adding extensions! #anya #howto #diy #salonaks #braids #hair   @user    \n",
       "1825                               it’s #nationalbestfriendsday!👭👫👬 tag your #bff⬇️   📷: @user  #bestfriends #friends #friendship   #bffl    \n",
       "29116            @user goodmorning everyone ☕️😋😘👌🏻.. #feel #free #fly #high   #sad #alone:): #lifeteacheseverything #letthelegsgocrazy ht…   \n",
       "13847                          $2.99! #2016release #ebook #book  the summer that melted everything: a novel by tiffany mcdaniel  via @user   \n",
       "5131                                                                          stay hungry, stay foolish! 👅🎈💎  ph: @user ❤ #crazy   #smile    \n",
       "12162                                              royalty 💜💜. makeup: @user aso-oke: @user  beads: @user  #iludioweddings #iludio   #be…    \n",
       "\n",
       "                                                                                                                                                       tweet_cleaned  \\\n",
       "id                                                                                                                                                                     \n",
       "14977                                                       thank share seattle sounder fc sunday smiley sharing care via rightwards arrow grow follower free button   \n",
       "28944                                                             thank share andrew jones sunday smiley sharing care via rightwards arrow grow follower free button   \n",
       "7187                                                                                                  periscope rant orlando florida pensive face pensive pensive hu   \n",
       "24117                                                                              okay literally go scream cold brew possible halp grinning face sweat hot beverage   \n",
       "23085                en directo en periscope morning gym work aerobics success khan love red heart red red red red red red flex bicep flex bicep flex bicep poodle …   \n",
       "29864                                                        relieve face hr later moment zen serenity world ev rything seems gear make anxious enraged disappointed   \n",
       "9437                                                                                     jtss phoro smile face heart - eye love sink pink music insta gay celeb spot   \n",
       "22134                                                                 mood day peace love happiness yellow heart thumb light skin tone hibiscus life hairstyle tiger   \n",
       "23633              thank share security tube tuesday smiley sharing care small blue diamond free button grow follower rightward arrow free button small blue diamond   \n",
       "9613                                                                periscope sappy scope let talk legacy value family sad relieved face tear include sad relieved …   \n",
       "8994                                                                                          excellent ramble alt right tc ot lot alt right black life matter thumb   \n",
       "24017                                                                           new track today smiley dedicate paul van dyk happy ok heart suit dance trance fami …   \n",
       "8377                                                                                         movie camera braid sho hair add extension anya diy salon aks braid hair   \n",
       "1825                                  national good friend day woman hold hand woman man hold hand man hold hand tag bff arrow camera friend friend friendship bf fl   \n",
       "29116  good morning everyone hot beverage face savor food blow kiss ok hand light skin tone feel free fly high sad alone life teach everything let leg go crazy ht …   \n",
       "13847                                                                               dollar sign release ebook book summer melt everything novel tiffany mcdaniel via   \n",
       "5131                                                                                      stay hungry stay foolish tongue balloon gem stone ph red heart crazy smile   \n",
       "12162                                                                                    royalty purple heart purple makeup aso - oke bead ilu dio wedding ilu dio …   \n",
       "\n",
       "       user_handle  \\\n",
       "id                   \n",
       "14977            1   \n",
       "28944            1   \n",
       "7187             1   \n",
       "24117            1   \n",
       "23085            1   \n",
       "29864            1   \n",
       "9437             2   \n",
       "22134            2   \n",
       "23633            1   \n",
       "9613             1   \n",
       "8994             1   \n",
       "24017            1   \n",
       "8377             1   \n",
       "1825             1   \n",
       "29116            1   \n",
       "13847            1   \n",
       "5131             1   \n",
       "12162            3   \n",
       "\n",
       "                                                                                    hashtags  \\\n",
       "id                                                                                             \n",
       "14977                                         [#thanks, #sharing, #sunday, #sharingiscaring]   \n",
       "28944                                         [#thanks, #sharing, #sunday, #sharingiscaring]   \n",
       "7187                                                                       [#periscope, #hu]   \n",
       "24117                                                                          [#halp, #why]   \n",
       "23085               [#periscope, #gym, #workout, #aerobics, #success, #ican, #wekhan, #love]   \n",
       "29864                                        [#21hrlater, #zenserenity, #theworld, #anxious]   \n",
       "9437                                    [#love, #sinkthepink, #music, #instagay, #celebspot]   \n",
       "22134                                                     [#love, #life, #hairstyle, #tiger]   \n",
       "23633                                        [#thanks, #sharing, #tuesday, #sharingiscaring]   \n",
       "9613                                                               [#periscope, #sappyscope]   \n",
       "8994                                            [#tcot, #tlot, #altright, #blacklivesmatter]   \n",
       "24017                                                     [#paulvandyk, #dance, #trancefami]   \n",
       "8377                                        [#anya, #howto, #diy, #salonaks, #braids, #hair]   \n",
       "1825             [#nationalbestfriendsday, #bff, #bestfriends, #friends, #friendship, #bffl]   \n",
       "29116  [#feel, #free, #fly, #high, #sad, #alone, #lifeteacheseverything, #letthelegsgocrazy]   \n",
       "13847                                                          [#2016release, #ebook, #book]   \n",
       "5131                                                                        [#crazy, #smile]   \n",
       "12162                                                        [#iludioweddings, #iludio, #be]   \n",
       "\n",
       "                                                                                                                                                                   emojis  \n",
       "id                                                                                                                                                                         \n",
       "14977                                                                                                                                    rightwards arrow,__FREE_button__  \n",
       "28944                                                                                                                                    rightwards arrow,__FREE_button__  \n",
       "7187                                                                                                                   __pensive_face__,__pensive_face__,__pensive_face__  \n",
       "24117                                                                                                                       __grinning_face_with_sweat__,__hot_beverage__  \n",
       "23085  __red_heart__,__red_heart__,__red_heart__,__red_heart__,__red_heart__,__red_heart__,__red_heart__,__flexed_biceps__,__flexed_biceps__,__flexed_biceps__,__poodle__  \n",
       "29864                                                                                                            __relieved_face__,__enraged_face__,__disappointed_face__  \n",
       "9437                                                                                                                                     __smiling_face_with_heart-eyes__  \n",
       "22134                                                                                                     __yellow_heart__,__thumbs_up__,__light_skin_tone__,__hibiscus__  \n",
       "23633                                                                      __small_blue_diamond__,__FREE_button__,rightwards arrow,__FREE_button__,__small_blue_diamond__  \n",
       "9613                                                                                                                  __sad_but_relieved_face__,__sad_but_relieved_face__  \n",
       "8994                                                                                                                                                        __thumbs_up__  \n",
       "24017                                                                                                                                                      __heart_suit__  \n",
       "8377                                                                                                                                                     __movie_camera__  \n",
       "1825                                                              __women_holding_hands__,__woman_and_man_holding_hands__,__men_holding_hands__,__down_arrow__,__camera__  \n",
       "29116                                                                     __hot_beverage__,__face_savoring_food__,__face_blowing_a_kiss__,__OK_hand__,__light_skin_tone__  \n",
       "13847                                                                                                                                                         dollar sign  \n",
       "5131                                                                                                                   __tongue__,__balloon__,__gem_stone__,__red_heart__  \n",
       "12162                                                                                                                                   __purple_heart__,__purple_heart__  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned_check = df_cleaned[df_cleaned['tweet'].str.contains(\":\", regex=False)]\n",
    "\n",
    "df_cleaned_check = df_cleaned_check[df_cleaned_check['user_handle']>= 1]\n",
    "df_cleaned_check = df_cleaned_check[df_cleaned_check['hashtags'].apply(len) > 1]\n",
    "\n",
    "\n",
    "df_cleaned_check = df_cleaned_check[df_cleaned_check['emojis'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "\n",
    "df_cleaned_check.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:28:14.696353600Z",
     "start_time": "2024-11-03T09:28:14.366002500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_cleaned abspeichern mit zusätzlichen Spalten\n",
    "filepath_name = os.path.abspath(os.path.join(src_path, 'data/twitter_hate-speech/train_cleaned.csv'))\n",
    "df_cleaned.to_csv(filepath_name)\n",
    "\n",
    "# train_cleaned abspeichern ohne zusätzliche Spalten\n",
    "df_cleaned_basic = df_cleaned[['label','tweet','tweet_cleaned']]\n",
    "filepath_name = os.path.abspath(os.path.join(src_path, 'data/twitter_hate-speech/train_basic_cleaned.csv'))\n",
    "df_cleaned_basic.to_csv(filepath_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:56:11.242992900Z",
     "start_time": "2024-11-03T11:56:11.095828700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# test_cleaned abspeichern mit zusätzlichen Spalten, ohne Duplikatenentfernung\n",
    "filepath_name_test = os.path.abspath(os.path.join(src_path, 'data/twitter_hate-speech/test_cleaned.csv'))\n",
    "df_cleaned_test.to_csv(filepath_name_test)\n",
    "\n",
    "# test_cleaned abspeichern ohne zusätzliche Spalten, ohne Duplikatenentfernung\n",
    "df_cleaned_test_new =  df_cleaned_test[['tweet','tweet_cleaned']]\n",
    "filepath_name_test = os.path.abspath(os.path.join(src_path, 'data/twitter_hate-speech/test_basic_cleaned.csv'))\n",
    "df_cleaned_test_new.to_csv(filepath_name_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
