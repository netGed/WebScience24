https://www.analyticsvidhya.com/blog/2023/01/ensemble-learning-methods-bagging-boosting-and-stacking/
https://medium.com/sfu-cspmp/surviving-in-a-random-forest-with-imbalanced-datasets-b98b963d52eb
Ausgangsdaten
- Base
    - Orginaldaten ohne umfassende Vorverarbeitung (Duplikate, Encoding)
    - Verhältnis 1:13.7 (27517:2013)
- Clean
    - Orginaldaten umfassend vorverarbeitet
    - Verhältnis 1:14.3 (25839:1811)

--- Daten (Clean/Base) vor Anwendung der Resampling Methods mit Word2Vec vektorisiert ---

Resampling Methods (angewendet auf Clean)
- Oversampling: generiert künstliche Beispieldaten basierend auf der Minderheitenklasse
    - SMOTE (Synthetic Minority Over-sampling Technique)
    - BorderlineSMOTE
    - ADASYN (Adaptive Synthetic Sampling)
- Undersampling: entfernt zufällig Daten aus der Mehrheitsklasse
    - NearMiss: wählt die Daten basierend auf Abstand zwischen Beispieldaten aus (3 Varianten, #3 hier angewendet)
    - Condensed Nearest Neighbor
    - Neighborhood Cleaning


https://www.analyticsvidhya.com/blog/2023/01/ensemble-learning-methods-bagging-boosting-and-stacking/
https://neptune.ai/blog/when-to-choose-catboost-over-xgboost-or-lightgbm
Modelle
- Ensemble Modelle können inhärent Klassenungleichgewichte verarbeiten, da mehrere Modelle trainiert und deren
Ergebnisse kombiniert werden
- Boosting
- Bagging








