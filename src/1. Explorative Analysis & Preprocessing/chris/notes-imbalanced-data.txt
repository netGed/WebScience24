https://www.analyticsvidhya.com/blog/2023/01/ensemble-learning-methods-bagging-boosting-and-stacking/
https://medium.com/sfu-cspmp/surviving-in-a-random-forest-with-imbalanced-datasets-b98b963d52eb
Ausgangsdaten
- Base
    - Orginaldaten ohne umfassende Vorverarbeitung (Duplikate, Encoding)
    - Verhältnis 1:13.7 (27517:2013)
- Clean
    - Orginaldaten umfassend vorverarbeitet
    - Verhältnis 1:14.3 (25839:1811)

--- Daten (Clean/Base) vor Anwendung der Resampling Methods mit Word2Vec vektorisiert ---

Resampling Methods (angewendet auf Clean)
- Oversampling: generiert künstliche Beispieldaten basierend auf der Minderheitenklasse
    - SMOTE (Synthetic Minority Over-sampling Technique)
    - BorderlineSMOTE
    - ADASYN (Adaptive Synthetic Sampling)
- Undersampling: entfernt zufällig Daten aus der Mehrheitsklasse
    - NearMiss: wählt die Daten basierend auf Abstand zwischen Beispieldaten aus (3 Varianten, #3 hier angewendet)
    - Condensed Nearest Neighbor
    - Neighborhood Cleaning

Ergebnisse bei Training von einfachen Modellen
- Ensemble: können inhärent Klassenungleichgewichte verarbeiten, da mehrere Modelle trainiert und deren
Ergebnisse kombiniert werden, Werte durch Over-/Undersampling daher nicht ausreichend aussagekräftig für einen Vergleich und Bewertung der
 zu nutzenden Methode
- SVM
    - ohne balancierte Klassen wird die SVM bei clean/base und bei Undersampling nur auf 0 trainiert
    - mit balancierten Klassen geht die Precison in Richtung der Oversampling-Daten (65-70%)
- Bayes
    - wie SVC ohne balancierte Klasse, Bayes bietet keine Möglichkeit zum Balancing

https://www.analyticsvidhya.com/blog/2023/01/ensemble-learning-methods-bagging-boosting-and-stacking/
https://neptune.ai/blog/when-to-choose-catboost-over-xgboost-or-lightgbm
Modelle
- Boosting
- Bagging









