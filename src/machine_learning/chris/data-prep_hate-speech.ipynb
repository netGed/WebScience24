{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports & Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  avoid characters of text column to be cut off by default Pandas DataFrame limitations\n",
    "pd.set_option('display.max_colwidth', 400)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_raw_val = pd.read_csv(\"C:\\\\Users\\\\admin\\\\PycharmProjects\\\\WebScience24\\\\data\\\\twitter_hate-speech\\\\test.csv\",\n",
    "                     index_col=0)\n",
    "df_raw_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_raw_train = pd.read_csv(\"C:\\\\Users\\\\admin\\\\PycharmProjects\\\\WebScience24\\\\data\\\\twitter_hate-speech\\\\train.csv\",\n",
    "                      index_col=0, encoding=\"utf-8\")\n",
    "df_raw_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_raw_train[df_raw_train.label == 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = df_raw_train.sample(frac = 0.8)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_df = df_raw_val.drop(train_df.index)\n",
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_df.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Duplicate-Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "=> bei der Menge an Daten (ca. 75k) sollten die Duplikate entfernt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Noise // Irrelevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['sentiment'] != 'Irrelevant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Null-Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "=> es fehlen nur Einträge bei Tweet, dabei handelt es sich aber um das entscheidende Feature; ohne Tweet ist der Eintrag wertlos, daher droppen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dropna(axis=\"rows\", inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Inconsistent text & typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Column: Topic')\n",
    "print(df['topic'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "=> keine Typos erkennbar\n",
    "=> Doppelung von CallOfDuty / CallOfDutyBlackopsColdWar -> ggf. zusammenlegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[(df.topic == \"CallOfDuty\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blackops_keywords = \"ops|Ops\"\n",
    "df[(df.topic == \"CallOfDuty\") & (df.tweet.str.contains(blackops_keywords))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[(df.topic == \"CallOfDutyBlackopsColdWar\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cod_keywords = \"verdansk|Verdansk|warzone|Warzone|modern|Modern|warfare|Warfare\"\n",
    "df[(df.topic == \"CallOfDutyBlackopsColdWar\") & (df.tweet.str.contains(cod_keywords))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Column: sentiment')\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "=> keine Typos erkennbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Spalte: Topic\n",
    "print(df['topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Spalte: Tweet\n",
    "# Ansatz: prüfen, ob es Ausreißer bei der Länge und/oder bei der Wortzahl pro Tweets gibt\n",
    "df[\"char_count\"] = df[\"tweet\"].str.len()\n",
    "df[\"word_count\"] = df[\"tweet\"].str.split().str.len()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"char_count\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.histplot(df[\"char_count\"])\n",
    "\n",
    "plt.title('Character Count of all Tweets')\n",
    "\n",
    "plt.xlabel('Character Count')\n",
    "plt.ylabel('Number of Tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.boxplot(df[\"char_count\"])\n",
    "\n",
    "plt.title('Distribution of Number of Characters per Tweet')\n",
    "\n",
    "plt.ylabel('Character Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"word_count\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.histplot(df[\"word_count\"])\n",
    "\n",
    "plt.title('Count of Words of all Tweets')\n",
    "\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Number of Tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=df[\"word_count\"])\n",
    "\n",
    "plt.title('Distribution of Words per Tweet')\n",
    "\n",
    "plt.ylabel('Number of Words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ausreißer auf numerische Art bestimmen: 3 Standardabweichungen vom Mittelwert entfernt = Ausreißer\n",
    "def find_outlier(data):\n",
    "    for num_col in data.select_dtypes([\"number\"]).columns:\n",
    "        sd = data[num_col].std()\n",
    "        mean = data[num_col].mean()\n",
    "        result = [out for out in data[num_col] if (out > mean + 3 * sd) or (out < mean - 3 * sd)]\n",
    "        print()\n",
    "        print(\"Column: \", num_col)\n",
    "        print(\"Mean: \", mean, \"; Std: \", sd)\n",
    "        print(\"Outlier: \", len(result), \"; Values: \", sorted(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_outlier(df[[\"char_count\", \"word_count\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df[\"char_count\"] >= 349]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df[\"word_count\"] >= 63]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "=> Strategie: Einträge droppen, um ML Modelle nicht durch Ausreißer zu beeinflussen (bei DeepLearning-Modellen tendenziell irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[df[\"char_count\"] < 349]\n",
    "df[df[\"char_count\"] >= 349]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[df[\"word_count\"] < 63]\n",
    "df[df[\"word_count\"] >= 63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.drop(['Char_Count', 'Word_Count'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Datensatz nach Bereinigung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.count())\n",
    "print(df_raw.count())\n",
    "print(df.count() - df_raw.count())\n",
    "print(df.count() / df_raw.count() - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Check Val/Test-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_val_raw = pd.read_csv(\"C:\\\\Users\\\\admin\\\\PycharmProjects\\\\WebScience24\\\\data\\\\twitter_hate-speech\\\\twitter_validation.csv\", index_col=0) # Fehlerhaft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "=> Bewertung Validation-Daten: sehr viele Fehler, ohne händisches Säubern ggf. nicht verwendbar, von der Menge her gering (> 2000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = Path('data/twitter_training_cleaned.csv')\n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Korrelationsanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"sentiment_binary\"] = df[\"sentiment\"] == \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(numeric_only=True), annot=True, vmin=-1, vmax=1, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_corr = df.corr(numeric_only=True)[\"sentiment_binary\"]\n",
    "target_corr.abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Korrelationsanalyse feature<->target => keine originären numerischen Werte, bei den erzeugten numerischen Werten keine nennenswerte Korrelation erkennbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Targetanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "color_mapping = {\n",
    "    'Negative': 'red',\n",
    "    'Positive': 'green',\n",
    "    'Neutral': 'orange',\n",
    "    #'Irrelevant': 'grey'\n",
    "}\n",
    "\n",
    "counts = df['sentiment'].value_counts().reset_index()\n",
    "counts.columns = ['sentiment', 'count']\n",
    "\n",
    "counts['color'] = counts['sentiment'].map(color_mapping)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=counts, y='sentiment', x='count', palette=counts['color'])\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(int(p.get_width())), (p.get_width(), p.get_y() + p.get_height() / 2.),\n",
    "                ha='left', va='center', xytext=(5, 0), textcoords='offset points')\n",
    "\n",
    "ax.set_xlim(0, counts['count'].max() * 1.1)\n",
    "\n",
    "ax.set_xlabel('Number of Tweets')\n",
    "ax.set_ylabel('Sentiment')\n",
    "\n",
    "\n",
    "plt.title('Number of Tweets per Sentiment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Featureanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature: Topic\n",
    "sns.histplot(\n",
    "    df[\"topic\"])  # Topics sind (grob) gleichverteilt, ggf. ein Ausreißer falls beide COD-Topics zusammengelegt werden\n",
    "\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.xlabel('Topic')\n",
    "\n",
    "plt.title('Number of Tweets per Topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature: Tweet\n",
    "sns.histplot(df[\"word_count\"])\n",
    "\n",
    "# redudant to one of the charts above (section1) -- is this intended or shall we get rid off one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.histplot(df[\"char_count\"])\n",
    "\n",
    "# redudant to one of the charts above (section1) -- is this intended or shall we get rid off one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Feature x Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Character Count\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 8))\n",
    "\n",
    "# Positive Tweets\n",
    "ax1.hist(df[df['sentiment'] == 'Positive']['char_count'])\n",
    "ax1.set_title('Positive Tweets')\n",
    "ax1.set_xlabel('Character Count')  \n",
    "ax1.set_ylabel('Frequency')          \n",
    "\n",
    "# Negative Tweets\n",
    "ax2.hist(df[df['sentiment'] == 'Negative']['char_count'])\n",
    "ax2.set_title('Negative Tweets')\n",
    "ax2.set_xlabel('Character Count')  \n",
    "ax2.set_ylabel('Frequency')        \n",
    "\n",
    "# Neutral Tweets\n",
    "ax3.hist(df[df['sentiment'] == 'Neutral']['char_count'])\n",
    "ax3.set_title('Neutral Tweets')\n",
    "ax3.set_xlabel('Character Count') \n",
    "ax3.set_ylabel('Frequency')         \n",
    "\n",
    "plt.suptitle('Count of Characters per Sentiment Category', fontsize=16)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Word Count\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 8))\n",
    "\n",
    "# Positive Tweets\n",
    "ax1.hist(df[df['sentiment'] == 'Positive']['word_count'])\n",
    "ax1.set_title('Positive Tweets')\n",
    "ax1.set_xlabel('Word Count')  \n",
    "ax1.set_ylabel('Frequency')    \n",
    "\n",
    "# Negative Tweets\n",
    "ax2.hist(df[df['sentiment'] == 'Negative']['word_count'])\n",
    "ax2.set_title('Negative Tweets')\n",
    "ax2.set_xlabel('Word Count')  \n",
    "ax2.set_ylabel('Frequency')    \n",
    "\n",
    "# Neutral Tweets\n",
    "ax3.hist(df[df['sentiment'] == 'Neutral']['word_count'])\n",
    "ax3.set_title('Neutral Tweets')\n",
    "ax3.set_xlabel('Word Count')  \n",
    "ax3.set_ylabel('Frequency')   \n",
    "\n",
    "plt.suptitle('Count of Words per Sentiment Category', fontsize=16)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing for Naive-Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Text cleaning: emojis, toLower etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    # converting to lowercase, removing URL links, special characters, punctuations...\n",
    "    text = text.lower()  # converting to lowercase\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)  # removing URL links\n",
    "    text = re.sub(r\"\\b\\d+\\b\", \"\", text)  # removing number\n",
    "    text = re.sub('<.*?>+', '', text)  # removing special characters,\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  # punctuations\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('[’“”…]', '', text)\n",
    "\n",
    "    #removing emoji:\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    # removing short form:\n",
    "    text = re.sub(\"isn't\", 'is not', text)\n",
    "    text = re.sub(\"he's\", 'he is', text)\n",
    "    text = re.sub(\"wasn't\", 'was not', text)\n",
    "    text = re.sub(\"there's\", 'there is', text)\n",
    "    text = re.sub(\"couldn't\", 'could not', text)\n",
    "    text = re.sub(\"won't\", 'will not', text)\n",
    "    text = re.sub(\"they're\", 'they are', text)\n",
    "    text = re.sub(\"she's\", 'she is', text)\n",
    "    text = re.sub(\"There's\", 'there is', text)\n",
    "    text = re.sub(\"wouldn't\", 'would not', text)\n",
    "    text = re.sub(\"haven't\", 'have not', text)\n",
    "    text = re.sub(\"That's\", 'That is', text)\n",
    "    text = re.sub(\"you've\", 'you have', text)\n",
    "    text = re.sub(\"He's\", 'He is', text)\n",
    "    text = re.sub(\"what's\", 'what is', text)\n",
    "    text = re.sub(\"weren't\", 'were not', text)\n",
    "    text = re.sub(\"we're\", 'we are', text)\n",
    "    text = re.sub(\"hasn't\", 'has not', text)\n",
    "    text = re.sub(\"you'd\", 'you would', text)\n",
    "    text = re.sub(\"shouldn't\", 'should not', text)\n",
    "    text = re.sub(\"let's\", 'let us', text)\n",
    "    text = re.sub(\"they've\", 'they have', text)\n",
    "    text = re.sub(\"You'll\", 'You will', text)\n",
    "    text = re.sub(\"i'm\", 'i am', text)\n",
    "    text = re.sub(\"we've\", 'we have', text)\n",
    "    text = re.sub(\"it's\", 'it is', text)\n",
    "    text = re.sub(\"don't\", 'do not', text)\n",
    "    text = re.sub(\"that´s\", 'that is', text)\n",
    "    text = re.sub(\"I´m\", 'I am', text)\n",
    "    text = re.sub(\"it’s\", 'it is', text)\n",
    "    text = re.sub(\"she´s\", 'she is', text)\n",
    "    text = re.sub(\"he’s'\", 'he is', text)\n",
    "    text = re.sub('I’m', 'I am', text)\n",
    "    text = re.sub('I’d', 'I did', text)\n",
    "    text = re.sub(\"he’s'\", 'he is', text)\n",
    "    text = re.sub('there’s', 'there is', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = df['tweet'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = pd.DataFrame(dt)\n",
    "dt['sentiment'] = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt['no_sw'] = dt['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "for text in dt[\"no_sw\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "cnt.most_common(20)\n",
    "temp = pd.DataFrame(cnt.most_common(20))\n",
    "temp.columns = ['word', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "most frequent words erstmal nicht entfernen (love, shit etc. vmtl. ausschlaggebend für Klassifikation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "# def remove_freqwords(text):\n",
    "#     return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "# dt[\"no_sw_no_freqwo\"] = dt[\"no_sw\"].apply(lambda text: remove_freqwords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dt[dt.no_sw != dt.no_sw_no_freqwo].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordnet_lem = WordNetLemmatizer()\n",
    "dt['no_sw_lem'] = dt['no_sw'].apply(wordnet_lem.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt[dt.no_sw != dt.no_sw_lem].head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Check again: Duplicates / NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt.drop_duplicates(inplace=True)\n",
    "dt.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = Path('data/twitter_training_cleaned_preprocessed.csv')\n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
