{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39faea16-c802-433e-86e9-06de75f6cca8",
   "metadata": {},
   "source": [
    "# W2V Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d855356-ff45-4fb4-a20f-6e19189be7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5eec24ba-233d-4874-abde-4f8377412541",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vectorizer = joblib.load('joblib_models/vectorizer_w2v.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2124e576-7c9f-47dd-bf5b-37a98cdb9739",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb_w2v = joblib.load('joblib_models/model_nb_w2v.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "778590c7-fa14-4dd9-a63e-60c7d77e3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = Spalte aus pd Dataframe\n",
    "# loaded_vectorizer = joblib w2v vectorizer\n",
    "\n",
    "def vectorize_w2v(tweets, loaded_vectorizer, vector_size=300):\n",
    "    \n",
    "    x_tokenized = tweets.map(word_tokenize)    \n",
    "\n",
    "    def w2v_vector(x_tokenized, vector_size):\n",
    "            vec = np.zeros(vector_size).reshape((1, vector_size))\n",
    "            count = 0\n",
    "            for word in x_tokenized:\n",
    "                try:\n",
    "                    vec += loaded_vectorizer.wv[word].reshape((1, vector_size))\n",
    "                    count += 1\n",
    "                except KeyError:\n",
    "\n",
    "                    continue\n",
    "            if count != 0:\n",
    "                vec /= count\n",
    "            return vec\n",
    "        \n",
    "    tweets_w2v = np.zeros((len(x_tokenized), 300))\n",
    "    for i in range(len(x_tokenized)):\n",
    "        tweets_w2v[i, :] = w2v_vector(x_tokenized.iloc[i], 300)\n",
    "\n",
    "    return tweets_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4be92-5ed5-4733-b846-d7f2e6a66dca",
   "metadata": {},
   "source": [
    "#### Test mit Beispiel Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e21b365d-6a4e-4e9d-95e7-03651aa59ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_hs = ['i hate jews so much i wish the holocaust actually happened']\n",
    "tweet_no_hs = ['today i start breaking in a new pair of docs have mercy on my soles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c44d104-af16-4526-a7f6-3131c5c8fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hs = pd.DataFrame(tweet_hs)\n",
    "df_no_hs = pd.DataFrame(tweet_no_hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de130949-6e19-40a4-a987-e381ac01e4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(clf_nb_w2v.predict(vectorize_w2v(df_hs[0],loaded_vectorizer )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "44f184c7-5681-4aec-9b42-d4d9a4ad09ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(clf_nb_w2v.predict(vectorize_w2v(df_no_hs[0],loaded_vectorizer )))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
