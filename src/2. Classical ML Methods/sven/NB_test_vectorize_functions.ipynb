{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d9ec666-3144-4ba2-976b-c91b5fe258bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # ideal f체r counting features wie bow oder tfidf https://towardsdatascience.com/why-how-to-use-the-naive-bayes-algorithms-in-a-regulated-industry-with-sklearn-python-code-dbd8304ab2cf\n",
    "from sklearn.naive_bayes import GaussianNB # f체r Features in Decimal Form geeignet\n",
    "from sklearn.naive_bayes import ComplementNB # 채hnlich wie Multinomial, soll sich aber besser f체r imbalanced data eignen\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    classification_report,\n",
    ")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e8841-6859-47e5-b924-f5ac52de66f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vectorize_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f636600b-8b3e-4bba-9b02-5f3733f11b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test,y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd93073-baea-4df3-8357-417a034752a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = vectorize_functions.vectorize_bow()\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = vectorize_functions.vectorize_tfidf()\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = vectorize_functions.vectorize_w2v()\n",
    "X_train_ft, X_test_ft, y_train_ft, y_test_ft = vectorize_functions.vectorize_ft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc4290f-c2e3-4566-9e87-48c63d630938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13737, 4925)\n",
      "(5888, 4925)\n",
      "(13737,)\n",
      "(5888,)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)\n",
    "print(y_train_bow.shape)\n",
    "print(y_test_bow.shape)\n",
    "print(type(X_train_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ea87f2d-2887-4f61-991d-28ce07a2dcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19354, 6293)\n",
      "(8295, 6293)\n",
      "(19354,)\n",
      "(8295,)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)\n",
    "print(y_train_tfidf.shape)\n",
    "print(y_test_tfidf.shape)\n",
    "print(type(X_train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0fe48e7-da00-4485-971c-1e20ad49f606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19354, 300)\n",
      "(8295, 300)\n",
      "(19354,)\n",
      "(8295,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_w2v.shape)\n",
    "print(X_test_w2v.shape)\n",
    "print(y_train_w2v.shape)\n",
    "print(y_test_w2v.shape)\n",
    "print(type(X_train_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "509e7a81-1980-433d-9387-d13902b0dd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19354, 300)\n",
      "(8295, 300)\n",
      "(19354,)\n",
      "(8295,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_ft.shape)\n",
    "print(X_test_ft.shape)\n",
    "print(y_train_ft.shape)\n",
    "print(y_test_ft.shape)\n",
    "print(type(X_train_ft))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae4b2b-4f1b-42e0-8c0a-ab3e3761b31e",
   "metadata": {},
   "source": [
    " #### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1b50ddd-4294-4efd-9397-bbb9b9003690",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bow = MultinomialNB()\n",
    "clf_bow.fit(X_train_bow,y_train_bow)\n",
    "y_pred_bow = clf_bow.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3dcd5f4-d238-46d7-b235-da99e629bc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9410488245931284\n",
      "F1 Score: 0.5852417302798982\n",
      "Recall: 0.6377079482439926\n",
      "Precision: 0.5407523510971787\n",
      "      0    1\n",
      "0  7461  293\n",
      "1   196  345\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_bow, y_pred_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bcae57d-daa7-4eeb-a13c-062e381561e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bow_comp = ComplementNB()\n",
    "clf_bow_comp.fit(X_train_bow,y_train_bow)\n",
    "y_pred_bow_comp = clf_bow_comp.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b97228da-97fb-46a9-a7f6-d9d4a19b6e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8546112115732369\n",
      "F1 Score: 0.4300567107750472\n",
      "Recall: 0.8410351201478743\n",
      "Precision: 0.28888888888888886\n",
      "      0     1\n",
      "0  6634  1120\n",
      "1    86   455\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_bow, y_pred_bow_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e253d-b2f6-482f-8d0d-560698142d58",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50ad3277-38d6-4682-a899-5adf81dd1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf = MultinomialNB()\n",
    "clf_tfidf.fit(X_train_tfidf,y_train_tfidf)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52b2017a-c84c-4687-97b8-89fa2f0faa12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9482820976491863\n",
      "F1 Score: 0.3529411764705882\n",
      "Recall: 0.21626617375231053\n",
      "Precision: 0.9590163934426229\n",
      "      0    1\n",
      "0  7749    5\n",
      "1   424  117\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_tfidf, y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a54b057-6066-4583-a2b2-759951bfc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf_comp = ComplementNB()\n",
    "clf_tfidf_comp.fit(X_train_tfidf,y_train_tfidf)\n",
    "y_pred_tfidf_comp = clf_tfidf_comp.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4abb271d-314e-4307-b474-0b4ad5712625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8603978300180832\n",
      "F1 Score: 0.42157842157842157\n",
      "Recall: 0.7800369685767098\n",
      "Precision: 0.2888432580424367\n",
      "      0     1\n",
      "0  6715  1039\n",
      "1   119   422\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_bow, y_pred_tfidf_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c6d01-f0e7-4eb5-a208-04e61b78d9ed",
   "metadata": {},
   "source": [
    "#### W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f3b15b0-4973-4feb-8b60-32dfa1027b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_w2v = GaussianNB()\n",
    "clf_w2v.fit(X_train_w2v,y_train_w2v)\n",
    "y_pred_w2v = clf_w2v.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9762cf1d-8940-49db-a4bb-76b69f4a7831",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m evaluate(y_test_w2v, \u001b[43my_pred_w2v\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred_w2v' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_w2v, y_pred_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c4ad9-cbeb-4f89-9140-3a814e123ba5",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6cad30b8-ccc0-4c65-a055-74c117215c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ft = GaussianNB()\n",
    "clf_ft.fit(X_train_ft,y_train_ft)\n",
    "y_pred_ft = clf_ft.predict(X_test_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "004e4adc-383c-48fd-a2d3-4562f09acd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.747799879445449\n",
      "F1 Score: 0.29704301075268813\n",
      "Recall: 0.8170055452865065\n",
      "Precision: 0.1815195071868583\n",
      "      0     1\n",
      "0  5761  1993\n",
      "1    99   442\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_ft, y_pred_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c90362-84db-41e4-bf56-b44d30106acc",
   "metadata": {},
   "source": [
    "## Tests mit modifizierter Vektorisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6212da-103e-4d25-a5a9-1b61a49675c6",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb4e9d-6f90-4e96-9496-bf16cc86a51d",
   "metadata": {},
   "source": [
    "#### Vektoren summieren anstelle des Mittelwerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "18ca3598-14c6-4668-ac80-4711f27537b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def vectorize_w2v_sum():\n",
    "    df = pd.read_csv('../../../data/twitter_hate-speech/train_cleaned.csv', index_col=0)\n",
    "    df = df[df['tweet_cleaned'].notna()]\n",
    "\n",
    "    X_base = df.tweet_cleaned\n",
    "    y_base = df.label\n",
    "\n",
    "    X_train_base, X_test_base, y_train_base_sum, y_test_base_sum = train_test_split(X_base, y_base, test_size=0.3,\n",
    "                                                                            random_state=42)\n",
    "\n",
    "    X_train_base_tokenized = X_train_base.map(word_tokenize)\n",
    "    X_test_base_tokenized = X_test_base.map(word_tokenize)\n",
    "\n",
    "    w2v = Word2Vec(min_count=1, window=35, vector_size=300, sg=0)\n",
    "    w2v.build_vocab(X_train_base_tokenized)#, progress_per=10000)\n",
    "    w2v.train(X_train_base_tokenized, total_examples=len(X_train_base_tokenized), epochs=30)\n",
    "\n",
    "    def w2v_vector(tokenized_tweet, size):\n",
    "        vec = np.zeros(size).reshape((1, size))\n",
    "        count = 0\n",
    "        for word in tokenized_tweet:\n",
    "            try:\n",
    "                vec += w2v.wv[word].reshape((1, size))\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "\n",
    "                continue\n",
    "        #if count != 0:\n",
    "         #   vec /= count\n",
    "        return vec\n",
    "\n",
    "    size = 300\n",
    "    X_train_w2v_sum = np.zeros((len(X_train_base_tokenized), size))\n",
    "    for i in range(len(X_train_base_tokenized)):\n",
    "        X_train_w2v_sum[i, :] = w2v_vector(X_train_base_tokenized.iloc[i], size)\n",
    "\n",
    "    X_test_w2v_sum = np.zeros((len(X_test_base_tokenized), size))\n",
    "    for i in range(len(X_test_base_tokenized)):\n",
    "        X_test_w2v_sum[i, :] = w2v_vector(X_test_base_tokenized.iloc[i], size)\n",
    "\n",
    "    return X_train_w2v_sum, X_test_w2v_sum, y_train_base_sum, y_test_base_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "da1f4dec-7741-401a-93b2-c42a403d232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v_sum, X_test_w2v_sum, y_train_w2v_sum, y_test_w2v_sum = vectorize_w2v_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ca3c9cd8-6972-4849-9a14-a81fafec645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_w2v_sum = GaussianNB()\n",
    "clf_w2v_sum.fit(X_train_w2v_sum,y_train_w2v_sum)\n",
    "y_pred_w2v_sum = clf_w2v_sum.predict(X_test_w2v_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a1a58bdf-969f-471a-b503-98c3f8c1106c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4412296564195298\n",
      "F1 Score: 0.17658553917214426\n",
      "Recall: 0.9186691312384473\n",
      "Precision: 0.09768081761006289\n",
      "      0     1\n",
      "0  3163  4591\n",
      "1    44   497\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_w2v_sum, y_pred_w2v_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b65e08-4973-4707-b5c7-28440231a0e7",
   "metadata": {},
   "source": [
    "Recall h철her als in Durchschnittsvariante, alle anderen schlechter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ddb30-6a99-49c3-b6df-4c7658bc472b",
   "metadata": {},
   "source": [
    "#### Vorhandenes Twitter W철rterbuch verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2894c-ea2c-4eb8-bfbe-fcaff3236c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove 200: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b80143d-955b-4c12-87f0-3b74b79a087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader as api\n",
    "\n",
    "def vectorize_glv():\n",
    "    df = pd.read_csv('../../../data/twitter_hate-speech/train_cleaned.csv', index_col=0)\n",
    "    df = df[df['tweet_cleaned'].notna()]\n",
    "\n",
    "    X_base = df.tweet_cleaned\n",
    "    y_base = df.label\n",
    "\n",
    "    X_train_base, X_test_base, y_train_base_glv, y_test_base_glv = train_test_split(X_base, y_base, test_size=0.3,\n",
    "                                                                            random_state=42)\n",
    "\n",
    "    X_train_base_tokenized = X_train_base.map(word_tokenize)\n",
    "    X_test_base_tokenized = X_test_base.map(word_tokenize)\n",
    "\n",
    "    w2v = api.load(\"glove-twitter-200\") #Word2Vec(min_count=1, window=35, vector_size=300, sg=0)\n",
    "    #w2v.build_vocab(X_train_base_tokenized)#, progress_per=10000)\n",
    "    #w2v.train(X_train_base_tokenized, total_examples=len(X_train_base_tokenized), epochs=30)\n",
    "\n",
    "    def w2v_vector(tokenized_tweet, size):\n",
    "        vec = np.zeros(size).reshape((1, size))\n",
    "        count = 0\n",
    "        for word in tokenized_tweet:\n",
    "            try:\n",
    "                vec += w2v[word].reshape((1, size))\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "\n",
    "                continue\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "\n",
    "    size = 200\n",
    "    X_train_w2v_glv = np.zeros((len(X_train_base_tokenized), size))\n",
    "    for i in range(len(X_train_base_tokenized)):\n",
    "        X_train_w2v_glv[i, :] = w2v_vector(X_train_base_tokenized.iloc[i], size)\n",
    "\n",
    "    X_test_w2v_glv = np.zeros((len(X_test_base_tokenized), size))\n",
    "    for i in range(len(X_test_base_tokenized)):\n",
    "        X_test_w2v_glv[i, :] = w2v_vector(X_test_base_tokenized.iloc[i], size)\n",
    "\n",
    "    return X_train_w2v_glv, X_test_w2v_glv, y_train_base_glv, y_test_base_glv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c438b-1342-498e-9eb0-b580e62ede54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glv, X_test_glv, y_train_glv, y_test_glv = vectorize_glv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed1b006-db02-4fcd-a8a5-62443fd71729",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_glv = GaussianNB()\n",
    "clf_glv.fit(X_train_glv,y_train_glv)\n",
    "y_pred_glv = clf_glv.predict(X_test_glv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "744fef52-0fc3-4bd7-bd5b-c0ae6d95aaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8738111413043478\n",
      "F1 Score: 0.4117181314330958\n",
      "Recall: 0.6951871657754011\n",
      "Precision: 0.2924634420697413\n",
      "      0    1\n",
      "0  4885  629\n",
      "1   114  260\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_glv, y_pred_glv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114cc80b-aa79-4969-98b7-e8ac448663fa",
   "metadata": {},
   "source": [
    "Schlechterer Wert f체r Recall, alle anderen besser als oben in W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ea177-cb23-4daa-9b78-5cca7cc14e9e",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7eaee6db-f93c-41ed-8dd7-2c21e4b4d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_ft_sum():\n",
    "    df = pd.read_csv('../../../data/twitter_hate-speech/train_cleaned.csv', index_col=0)\n",
    "    df = df[df['tweet_cleaned'].notna()]\n",
    "\n",
    "    X_base = df.tweet_cleaned\n",
    "    y_base = df.label\n",
    "\n",
    "    X_train_base, X_test_base, y_train_base_ft_sum, y_test_base_ft_sum = train_test_split(X_base, y_base, test_size=0.3,\n",
    "                                                                            random_state=42)\n",
    "\n",
    "    X_train_base_tokenized = X_train_base.map(word_tokenize)\n",
    "    X_test_base_tokenized = X_test_base.map(word_tokenize)\n",
    "\n",
    "    ft = FastText(window=35, min_count=1, vector_size=300)\n",
    "    ft.build_vocab(corpus_iterable=X_train_base_tokenized)\n",
    "    ft.train(corpus_iterable=X_train_base_tokenized, total_examples=len(X_train_base_tokenized), epochs=30)\n",
    "\n",
    "    def ft_vector(tokenized_tweet, size):\n",
    "        vec = np.zeros(size).reshape((1, size))\n",
    "        count = 0\n",
    "        for word in tokenized_tweet:\n",
    "            try:\n",
    "                vec += ft.wv[word].reshape((1, size))\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "\n",
    "                continue\n",
    "        #if count != 0:\n",
    "        #    vec /= count\n",
    "        return vec\n",
    "\n",
    "    size = 300\n",
    "    X_train_ft_sum = np.zeros((len(X_train_base_tokenized), size))\n",
    "    for i in range(len(X_train_base_tokenized)):\n",
    "        X_train_ft_sum[i, :] = ft_vector(X_train_base_tokenized.iloc[i], size)\n",
    "\n",
    "    X_test_ft_sum = np.zeros((len(X_test_base_tokenized), size))\n",
    "    for i in range(len(X_test_base_tokenized)):\n",
    "        X_test_ft_sum[i, :] = ft_vector(X_test_base_tokenized.iloc[i], size)\n",
    "\n",
    "    return X_train_ft_sum, X_test_ft_sum, y_train_base_ft_sum, y_test_base_ft_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7cbeea78-75b4-4fbb-a464-6fbc5a8b7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ft_sum, X_test_ft_sum, y_train_ft_sum, y_test_ft_sum = vectorize_ft_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5c111cb1-44a4-4b56-ba0f-3c6cdb7f4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ft_sum = GaussianNB()\n",
    "clf_ft_sum.fit(X_train_ft_sum,y_train_ft_sum)\n",
    "y_pred_ft_sum = clf_ft_sum.predict(X_test_ft_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0ca3cadd-c728-41bc-b404-9db3ba9f425b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5057263411693791\n",
      "F1 Score: 0.19513152728700434\n",
      "Recall: 0.9186691312384473\n",
      "Precision: 0.10915879639797936\n",
      "      0     1\n",
      "0  3698  4056\n",
      "1    44   497\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_ft_sum, y_pred_ft_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823436ec-b69a-4cae-8acd-090353bdf825",
   "metadata": {},
   "source": [
    "Recall besser, alle anderen schlechter als oben"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce5a26-cdc4-46a9-83d1-bf54813f34e4",
   "metadata": {},
   "source": [
    "## Evaluation neue Vectorize-Funktionen (08.12.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d1b5ff-2a21-4fc2-80d0-0ab9d8cd8671",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../functions/vectorize_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "512a7731-bbeb-4a1a-89f5-eb2b208d84c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_name = (('../../../data/twitter_hate-speech/train_cleaned.csv'))\n",
    "df_cleaned = pd.read_csv(filepath_name, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d73a06de-70e1-4076-907b-66d3205c5922",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = vectorize_tfidf(df=df_cleaned, text_column='tweet_cleaned', \n",
    "                                                                                 label_column=\"label\")\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = vectorize_w2v(df=df_cleaned, text_column =\"tweet_cleaned\", label_column=\"label\")\n",
    "X_train_glv, X_test_glv, y_train_glv, y_test_glv = vectorize_glove(df=df_cleaned, text_column='tweet_cleaned', \n",
    "                                                                                 label_column=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a198a29-26dc-4c6c-8155-b307c8bd7c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13737, 4925)\n",
      "(5888, 4925)\n",
      "(13737,)\n",
      "(5888,)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)\n",
    "print(y_train_tfidf.shape)\n",
    "print(y_test_tfidf.shape)\n",
    "print(type(X_train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d012c6c6-6df3-4d6d-acf6-c1a0456995f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf = MultinomialNB()\n",
    "clf_tfidf.fit(X_train_tfidf,y_train_tfidf)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d37edd1-4623-4c6d-af74-07119a2e79ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9458220108695652\n",
      "F1 Score: 0.27334851936218685\n",
      "Recall: 0.16042780748663102\n",
      "Precision: 0.9230769230769231\n",
      "      0   1\n",
      "0  5509   5\n",
      "1   314  60\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_tfidf, y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "713516c3-e398-4220-829f-c035d2d4ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf_comp = ComplementNB()\n",
    "clf_tfidf_comp.fit(X_train_tfidf,y_train_tfidf)\n",
    "y_pred_tfidf_comp = clf_tfidf_comp.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d04fa54a-f46a-4b92-9bc8-c2a0733b3077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8566576086956522\n",
      "F1 Score: 0.4089635854341736\n",
      "Recall: 0.7807486631016043\n",
      "Precision: 0.27703984819734345\n",
      "      0    1\n",
      "0  4752  762\n",
      "1    82  292\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_tfidf, y_pred_tfidf_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74bd8c26-a5fe-4095-b832-d3264c8f73b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svenw\\miniconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\svenw\\miniconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "C:\\Users\\svenw\\miniconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\svenw\\miniconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha   norm  train_accuracy  train_recall  train_precision  train_f1  \\\n",
      "0    0.00   True        0.487151      1.000000         0.116614  0.208871   \n",
      "1    0.00  False        0.949989      0.979570         0.576947  0.726186   \n",
      "2    0.01   True        0.735459      1.000000         0.203769  0.338551   \n",
      "3    0.01  False        0.936667      0.979570         0.517026  0.676820   \n",
      "4    0.10   True        0.888112      0.979570         0.375051  0.542423   \n",
      "5    0.10  False        0.911116      0.972043         0.430681  0.596897   \n",
      "6    0.25   True        0.931644      0.913978         0.497367  0.644183   \n",
      "7    0.25  False        0.894591      0.949462         0.386602  0.549471   \n",
      "8    0.50   True        0.950571      0.830108         0.597061  0.694557   \n",
      "9    0.50  False        0.883308      0.916129         0.358435  0.515271   \n",
      "10   0.75   True        0.955012      0.759140         0.641818  0.695567   \n",
      "11   0.75  False        0.882725      0.893548         0.354673  0.507791   \n",
      "12   1.00   True        0.956541      0.710753         0.668352  0.688900   \n",
      "13   1.00  False        0.886875      0.869892         0.360839  0.510088   \n",
      "\n",
      "    test_accuracy  test_recall  test_precision   test_f1  \n",
      "0        0.470958     0.946524        0.102638  0.185195  \n",
      "1        0.916950     0.491979        0.380952  0.429405  \n",
      "2        0.703465     0.927807        0.167957  0.284426  \n",
      "3        0.912024     0.727273        0.395349  0.512241  \n",
      "4        0.855978     0.844920        0.285714  0.427027  \n",
      "5        0.879755     0.820856        0.323840  0.464448  \n",
      "6        0.904212     0.772727        0.376302  0.506130  \n",
      "7        0.856488     0.823529        0.283349  0.421629  \n",
      "8        0.921705     0.660428        0.425129  0.517277  \n",
      "9        0.848845     0.820856        0.271681  0.408245  \n",
      "10       0.926291     0.614973        0.442308  0.514541  \n",
      "11       0.851393     0.807487        0.273303  0.408384  \n",
      "12       0.927989     0.558824        0.446581  0.496437  \n",
      "13       0.856658     0.780749        0.277040  0.408964  \n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "\n",
    "param_grid = {\n",
    "    'alpha' : [0, 0.01, 0.1, 0.25, 0.5, 0.75, 1],\n",
    "    'norm' : [True, False]\n",
    "}\n",
    "\n",
    "cnb = ComplementNB()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=cnb, param_grid=param_grid, scoring='f1', cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "for params in grid_search.cv_results_['params']:\n",
    "    model = ComplementNB(**params)  \n",
    "    model.fit(X_train_tfidf, y_train_tfidf)  \n",
    "\n",
    "    y_train_pred_tfidf = model.predict(X_train_tfidf)\n",
    "\n",
    "    y_test_pred_tfidf = model.predict(X_test_tfidf)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train_tfidf, y_train_pred_tfidf)\n",
    "    train_recall = recall_score(y_train_tfidf, y_train_pred_tfidf)\n",
    "    train_precision = precision_score(y_train_tfidf, y_train_pred_tfidf)\n",
    "    train_f1 = f1_score(y_train_tfidf, y_train_pred_tfidf)\n",
    "\n",
    "    test_accuracy = accuracy_score(y_test_tfidf, y_test_pred_tfidf)\n",
    "    test_recall = recall_score(y_test_tfidf, y_test_pred_tfidf)\n",
    "    test_precision = precision_score(y_test_tfidf, y_test_pred_tfidf)\n",
    "    test_f1 = f1_score(y_test_tfidf, y_test_pred_tfidf)\n",
    "\n",
    "    result_dict = {\n",
    "        'alpha': params['alpha'],\n",
    "        'norm': params['norm'],\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'train_recall': train_recall,\n",
    "        'train_precision': train_precision,\n",
    "        'train_f1': train_f1,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_recall': test_recall,\n",
    "        'test_precision': test_precision,\n",
    "        'test_f1': test_f1\n",
    "    }\n",
    "\n",
    "    results_list.append(result_dict)\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "results_df.to_csv('cnb_grid_tfidf.csv', index=False)\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8f2d7-ef26-43d9-abb0-50f46d27931c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
