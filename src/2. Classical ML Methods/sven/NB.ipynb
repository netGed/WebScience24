{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9ec666-3144-4ba2-976b-c91b5fe258bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # ideal für counting features wie bow oder tfidf https://towardsdatascience.com/why-how-to-use-the-naive-bayes-algorithms-in-a-regulated-industry-with-sklearn-python-code-dbd8304ab2cf\n",
    "from sklearn.naive_bayes import GaussianNB # für Features in Decimal Form geeignet\n",
    "from sklearn.naive_bayes import ComplementNB # ähnlich wie Multinomial, soll sich aber besser für imbalanced data eignen\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    classification_report,\n",
    ")\n",
    "import vectorize_functions\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f636600b-8b3e-4bba-9b02-5f3733f11b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test,y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd93073-baea-4df3-8357-417a034752a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = vectorize_functions.vectorize_bow()\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = vectorize_functions.vectorize_tfidf()\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = vectorize_functions.vectorize_w2v()\n",
    "X_train_ft, X_test_ft, y_train_ft, y_test_ft = vectorize_functions.vectorize_ft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc4290f-c2e3-4566-9e87-48c63d630938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13737, 4925)\n",
      "(5888, 4925)\n",
      "(13737,)\n",
      "(5888,)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)\n",
    "print(y_train_bow.shape)\n",
    "print(y_test_bow.shape)\n",
    "print(type(X_train_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ea87f2d-2887-4f61-991d-28ce07a2dcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19354, 6293)\n",
      "(8295, 6293)\n",
      "(19354,)\n",
      "(8295,)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)\n",
    "print(y_train_tfidf.shape)\n",
    "print(y_test_tfidf.shape)\n",
    "print(type(X_train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0fe48e7-da00-4485-971c-1e20ad49f606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19354, 300)\n",
      "(8295, 300)\n",
      "(19354,)\n",
      "(8295,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_w2v.shape)\n",
    "print(X_test_w2v.shape)\n",
    "print(y_train_w2v.shape)\n",
    "print(y_test_w2v.shape)\n",
    "print(type(X_train_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "509e7a81-1980-433d-9387-d13902b0dd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19354, 300)\n",
      "(8295, 300)\n",
      "(19354,)\n",
      "(8295,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_ft.shape)\n",
    "print(X_test_ft.shape)\n",
    "print(y_train_ft.shape)\n",
    "print(y_test_ft.shape)\n",
    "print(type(X_train_ft))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae4b2b-4f1b-42e0-8c0a-ab3e3761b31e",
   "metadata": {},
   "source": [
    " #### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1b50ddd-4294-4efd-9397-bbb9b9003690",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bow = MultinomialNB()\n",
    "clf_bow.fit(X_train_bow,y_train_bow)\n",
    "y_pred_bow = clf_bow.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3dcd5f4-d238-46d7-b235-da99e629bc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9410488245931284\n",
      "F1 Score: 0.5852417302798982\n",
      "Recall: 0.6377079482439926\n",
      "Precision: 0.5407523510971787\n",
      "      0    1\n",
      "0  7461  293\n",
      "1   196  345\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_bow, y_pred_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bcae57d-daa7-4eeb-a13c-062e381561e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bow_comp = ComplementNB()\n",
    "clf_bow_comp.fit(X_train_bow,y_train_bow)\n",
    "y_pred_bow_comp = clf_bow_comp.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b97228da-97fb-46a9-a7f6-d9d4a19b6e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8546112115732369\n",
      "F1 Score: 0.4300567107750472\n",
      "Recall: 0.8410351201478743\n",
      "Precision: 0.28888888888888886\n",
      "      0     1\n",
      "0  6634  1120\n",
      "1    86   455\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_bow, y_pred_bow_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e253d-b2f6-482f-8d0d-560698142d58",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50ad3277-38d6-4682-a899-5adf81dd1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf = MultinomialNB()\n",
    "clf_tfidf.fit(X_train_tfidf,y_train_tfidf)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52b2017a-c84c-4687-97b8-89fa2f0faa12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9482820976491863\n",
      "F1 Score: 0.3529411764705882\n",
      "Recall: 0.21626617375231053\n",
      "Precision: 0.9590163934426229\n",
      "      0    1\n",
      "0  7749    5\n",
      "1   424  117\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_tfidf, y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a54b057-6066-4583-a2b2-759951bfc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf_comp = ComplementNB()\n",
    "clf_tfidf_comp.fit(X_train_tfidf,y_train_tfidf)\n",
    "y_pred_tfidf_comp = clf_tfidf_comp.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4abb271d-314e-4307-b474-0b4ad5712625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8603978300180832\n",
      "F1 Score: 0.42157842157842157\n",
      "Recall: 0.7800369685767098\n",
      "Precision: 0.2888432580424367\n",
      "      0     1\n",
      "0  6715  1039\n",
      "1   119   422\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_bow, y_pred_tfidf_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c6d01-f0e7-4eb5-a208-04e61b78d9ed",
   "metadata": {},
   "source": [
    "#### W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f3b15b0-4973-4feb-8b60-32dfa1027b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_w2v = GaussianNB()\n",
    "clf_w2v.fit(X_train_w2v,y_train_w2v)\n",
    "y_pred_w2v = clf_w2v.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9762cf1d-8940-49db-a4bb-76b69f4a7831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.708499095840868\n",
      "F1 Score: 0.2716867469879518\n",
      "Recall: 0.833641404805915\n",
      "Precision: 0.16228859301907161\n",
      "      0     1\n",
      "0  5426  2328\n",
      "1    90   451\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_w2v, y_pred_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c4ad9-cbeb-4f89-9140-3a814e123ba5",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6cad30b8-ccc0-4c65-a055-74c117215c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ft = GaussianNB()\n",
    "clf_ft.fit(X_train_ft,y_train_ft)\n",
    "y_pred_ft = clf_ft.predict(X_test_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "004e4adc-383c-48fd-a2d3-4562f09acd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.747799879445449\n",
      "F1 Score: 0.29704301075268813\n",
      "Recall: 0.8170055452865065\n",
      "Precision: 0.1815195071868583\n",
      "      0     1\n",
      "0  5761  1993\n",
      "1    99   442\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_ft, y_pred_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c90362-84db-41e4-bf56-b44d30106acc",
   "metadata": {},
   "source": [
    "## Tests mit modifizierter Vektorisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6212da-103e-4d25-a5a9-1b61a49675c6",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb4e9d-6f90-4e96-9496-bf16cc86a51d",
   "metadata": {},
   "source": [
    "#### Vektoren summieren anstelle des Mittelwerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "18ca3598-14c6-4668-ac80-4711f27537b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def vectorize_w2v_sum():\n",
    "    df = pd.read_csv('../../../data/twitter_hate-speech/train_cleaned.csv', index_col=0)\n",
    "    df = df[df['tweet_cleaned'].notna()]\n",
    "\n",
    "    X_base = df.tweet_cleaned\n",
    "    y_base = df.label\n",
    "\n",
    "    X_train_base, X_test_base, y_train_base_sum, y_test_base_sum = train_test_split(X_base, y_base, test_size=0.3,\n",
    "                                                                            random_state=42)\n",
    "\n",
    "    X_train_base_tokenized = X_train_base.map(word_tokenize)\n",
    "    X_test_base_tokenized = X_test_base.map(word_tokenize)\n",
    "\n",
    "    w2v = Word2Vec(min_count=1, window=35, vector_size=300, sg=0)\n",
    "    w2v.build_vocab(X_train_base_tokenized)#, progress_per=10000)\n",
    "    w2v.train(X_train_base_tokenized, total_examples=len(X_train_base_tokenized), epochs=30)\n",
    "\n",
    "    def w2v_vector(tokenized_tweet, size):\n",
    "        vec = np.zeros(size).reshape((1, size))\n",
    "        count = 0\n",
    "        for word in tokenized_tweet:\n",
    "            try:\n",
    "                vec += w2v.wv[word].reshape((1, size))\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "\n",
    "                continue\n",
    "        #if count != 0:\n",
    "         #   vec /= count\n",
    "        return vec\n",
    "\n",
    "    size = 300\n",
    "    X_train_w2v_sum = np.zeros((len(X_train_base_tokenized), size))\n",
    "    for i in range(len(X_train_base_tokenized)):\n",
    "        X_train_w2v_sum[i, :] = w2v_vector(X_train_base_tokenized.iloc[i], size)\n",
    "\n",
    "    X_test_w2v_sum = np.zeros((len(X_test_base_tokenized), size))\n",
    "    for i in range(len(X_test_base_tokenized)):\n",
    "        X_test_w2v_sum[i, :] = w2v_vector(X_test_base_tokenized.iloc[i], size)\n",
    "\n",
    "    return X_train_w2v_sum, X_test_w2v_sum, y_train_base_sum, y_test_base_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "da1f4dec-7741-401a-93b2-c42a403d232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v_sum, X_test_w2v_sum, y_train_w2v_sum, y_test_w2v_sum = vectorize_w2v_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ca3c9cd8-6972-4849-9a14-a81fafec645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_w2v_sum = GaussianNB()\n",
    "clf_w2v_sum.fit(X_train_w2v_sum,y_train_w2v_sum)\n",
    "y_pred_w2v_sum = clf_w2v_sum.predict(X_test_w2v_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a1a58bdf-969f-471a-b503-98c3f8c1106c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4412296564195298\n",
      "F1 Score: 0.17658553917214426\n",
      "Recall: 0.9186691312384473\n",
      "Precision: 0.09768081761006289\n",
      "      0     1\n",
      "0  3163  4591\n",
      "1    44   497\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_w2v_sum, y_pred_w2v_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b65e08-4973-4707-b5c7-28440231a0e7",
   "metadata": {},
   "source": [
    "Recall höher als in Durchschnittsvariante, alle anderen schlechter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ddb30-6a99-49c3-b6df-4c7658bc472b",
   "metadata": {},
   "source": [
    "#### Vorhandenes Twitter Wörterbuch verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2894c-ea2c-4eb8-bfbe-fcaff3236c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove 200: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b80143d-955b-4c12-87f0-3b74b79a087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader as api\n",
    "\n",
    "def vectorize_glv():\n",
    "    df = pd.read_csv('../../../data/twitter_hate-speech/train_cleaned.csv', index_col=0)\n",
    "    df = df[df['tweet_cleaned'].notna()]\n",
    "\n",
    "    X_base = df.tweet_cleaned\n",
    "    y_base = df.label\n",
    "\n",
    "    X_train_base, X_test_base, y_train_base_glv, y_test_base_glv = train_test_split(X_base, y_base, test_size=0.3,\n",
    "                                                                            random_state=42)\n",
    "\n",
    "    X_train_base_tokenized = X_train_base.map(word_tokenize)\n",
    "    X_test_base_tokenized = X_test_base.map(word_tokenize)\n",
    "\n",
    "    w2v = api.load(\"glove-twitter-200\") #Word2Vec(min_count=1, window=35, vector_size=300, sg=0)\n",
    "    #w2v.build_vocab(X_train_base_tokenized)#, progress_per=10000)\n",
    "    #w2v.train(X_train_base_tokenized, total_examples=len(X_train_base_tokenized), epochs=30)\n",
    "\n",
    "    def w2v_vector(tokenized_tweet, size):\n",
    "        vec = np.zeros(size).reshape((1, size))\n",
    "        count = 0\n",
    "        for word in tokenized_tweet:\n",
    "            try:\n",
    "                vec += w2v[word].reshape((1, size))\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "\n",
    "                continue\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "\n",
    "    size = 200\n",
    "    X_train_w2v_glv = np.zeros((len(X_train_base_tokenized), size))\n",
    "    for i in range(len(X_train_base_tokenized)):\n",
    "        X_train_w2v_glv[i, :] = w2v_vector(X_train_base_tokenized.iloc[i], size)\n",
    "\n",
    "    X_test_w2v_glv = np.zeros((len(X_test_base_tokenized), size))\n",
    "    for i in range(len(X_test_base_tokenized)):\n",
    "        X_test_w2v_glv[i, :] = w2v_vector(X_test_base_tokenized.iloc[i], size)\n",
    "\n",
    "    return X_train_w2v_glv, X_test_w2v_glv, y_train_base_glv, y_test_base_glv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c438b-1342-498e-9eb0-b580e62ede54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glv, X_test_glv, y_train_glv, y_test_glv = vectorize_glv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed1b006-db02-4fcd-a8a5-62443fd71729",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_glv = GaussianNB()\n",
    "clf_glv.fit(X_train_glv,y_train_glv)\n",
    "y_pred_glv = clf_glv.predict(X_test_glv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "744fef52-0fc3-4bd7-bd5b-c0ae6d95aaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8738111413043478\n",
      "F1 Score: 0.4117181314330958\n",
      "Recall: 0.6951871657754011\n",
      "Precision: 0.2924634420697413\n",
      "      0    1\n",
      "0  4885  629\n",
      "1   114  260\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_glv, y_pred_glv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114cc80b-aa79-4969-98b7-e8ac448663fa",
   "metadata": {},
   "source": [
    "Schlechterer Wert für Recall, alle anderen besser als oben in W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ea177-cb23-4daa-9b78-5cca7cc14e9e",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7eaee6db-f93c-41ed-8dd7-2c21e4b4d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_ft_sum():\n",
    "    df = pd.read_csv('../../../data/twitter_hate-speech/train_cleaned.csv', index_col=0)\n",
    "    df = df[df['tweet_cleaned'].notna()]\n",
    "\n",
    "    X_base = df.tweet_cleaned\n",
    "    y_base = df.label\n",
    "\n",
    "    X_train_base, X_test_base, y_train_base_ft_sum, y_test_base_ft_sum = train_test_split(X_base, y_base, test_size=0.3,\n",
    "                                                                            random_state=42)\n",
    "\n",
    "    X_train_base_tokenized = X_train_base.map(word_tokenize)\n",
    "    X_test_base_tokenized = X_test_base.map(word_tokenize)\n",
    "\n",
    "    ft = FastText(window=35, min_count=1, vector_size=300)\n",
    "    ft.build_vocab(corpus_iterable=X_train_base_tokenized)\n",
    "    ft.train(corpus_iterable=X_train_base_tokenized, total_examples=len(X_train_base_tokenized), epochs=30)\n",
    "\n",
    "    def ft_vector(tokenized_tweet, size):\n",
    "        vec = np.zeros(size).reshape((1, size))\n",
    "        count = 0\n",
    "        for word in tokenized_tweet:\n",
    "            try:\n",
    "                vec += ft.wv[word].reshape((1, size))\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "\n",
    "                continue\n",
    "        #if count != 0:\n",
    "        #    vec /= count\n",
    "        return vec\n",
    "\n",
    "    size = 300\n",
    "    X_train_ft_sum = np.zeros((len(X_train_base_tokenized), size))\n",
    "    for i in range(len(X_train_base_tokenized)):\n",
    "        X_train_ft_sum[i, :] = ft_vector(X_train_base_tokenized.iloc[i], size)\n",
    "\n",
    "    X_test_ft_sum = np.zeros((len(X_test_base_tokenized), size))\n",
    "    for i in range(len(X_test_base_tokenized)):\n",
    "        X_test_ft_sum[i, :] = ft_vector(X_test_base_tokenized.iloc[i], size)\n",
    "\n",
    "    return X_train_ft_sum, X_test_ft_sum, y_train_base_ft_sum, y_test_base_ft_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7cbeea78-75b4-4fbb-a464-6fbc5a8b7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ft_sum, X_test_ft_sum, y_train_ft_sum, y_test_ft_sum = vectorize_ft_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5c111cb1-44a4-4b56-ba0f-3c6cdb7f4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ft_sum = GaussianNB()\n",
    "clf_ft_sum.fit(X_train_ft_sum,y_train_ft_sum)\n",
    "y_pred_ft_sum = clf_ft_sum.predict(X_test_ft_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0ca3cadd-c728-41bc-b404-9db3ba9f425b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5057263411693791\n",
      "F1 Score: 0.19513152728700434\n",
      "Recall: 0.9186691312384473\n",
      "Precision: 0.10915879639797936\n",
      "      0     1\n",
      "0  3698  4056\n",
      "1    44   497\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_ft_sum, y_pred_ft_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823436ec-b69a-4cae-8acd-090353bdf825",
   "metadata": {},
   "source": [
    "Recall besser, alle anderen schlechter als oben"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
