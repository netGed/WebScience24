{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import vectorize_functions\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einladen der vektorisierten Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = vectorize_functions.vectorize_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "- Positive: 1272\n",
      "- Negative: 18055\n",
      "- Verh채ltnis: 14.194182389937106\n",
      "Test\n",
      "- Positive: 533\n",
      "- Negative: 7750\n",
      "- Verh채ltnis: 14.540337711069418\n"
     ]
    }
   ],
   "source": [
    "positive = np.count_nonzero(y_train == 1)\n",
    "negative = np.count_nonzero(y_train == 0)\n",
    "print(\"Train\")\n",
    "print(\"- Positive:\", positive)\n",
    "print(\"- Negative:\", negative)\n",
    "print(\"- Verh채ltnis:\", negative / positive)\n",
    "\n",
    "positive = np.count_nonzero(y_test == 1)\n",
    "negative = np.count_nonzero(y_test == 0)\n",
    "print(\"Test\")\n",
    "print(\"- Positive:\", positive)\n",
    "print(\"- Negative:\", negative)\n",
    "print(\"- Verh채ltnis:\", negative / positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (19327, 6319)\n",
      "y_train shape (19327,)\n",
      "X_test shape (8283, 6319)\n",
      "y_test shape (8283,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(\n",
    "    columns=[\"model\", \"variant\", \"train_acc\", \"train_prec\", \"train_rec\", \"train_f1\", \"test_acc\", \"test_prec\",\n",
    "             \"test_rec\", \"test_f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_eval_df(model, model_name, variant, x_data_train, y_data_train, x_data_test, y_data_test):\n",
    "    train_acc = model.score(x_data_train, y_data_train)\n",
    "    train_precision = precision_score(y_data_train, model.predict(x_data_train))\n",
    "    train_recall = recall_score(y_data_train, model.predict(x_data_train))\n",
    "    train_f1 = f1_score(y_data_train, model.predict(x_data_train))\n",
    "\n",
    "    test_acc = model.score(x_data_test, y_data_test)\n",
    "    test_precision = precision_score(y_data_test, model.predict(x_data_test))\n",
    "    test_recall = recall_score(y_data_test, model.predict(x_data_test))\n",
    "    test_f1 = f1_score(y_data_test, model.predict(x_data_test))\n",
    "\n",
    "    evaluation.loc[len(evaluation.index)] = [model_name, variant, train_acc, train_precision, train_recall, train_f1,\n",
    "                                             test_acc, test_precision, test_recall, test_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test, y_test, sampling_method):\n",
    "    pred = model.predict(x_test)\n",
    "    accscore = metrics.accuracy_score(pred, y_test)\n",
    "\n",
    "    print(f'{sampling_method} model accuracy for classification is =', str('{:04.2f}'.format(accscore * 100)) + '%')\n",
    "    print('------------------------------------------------')\n",
    "    print('Confusion Matrix:')\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, pred)))\n",
    "    print('------------------------------------------------')\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, pred))\n",
    "\n",
    "    # probs = model.predict_proba(x_test)\n",
    "    # preds = probs[:, 1]\n",
    "    # fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    # roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # plt.title('Receiver Operating Characteristic')\n",
    "    # plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "    # plt.legend(loc='lower right')\n",
    "    # plt.plot([0, 1], [0, 1], 'r--')\n",
    "    # plt.xlim([0, 1])\n",
    "    # plt.ylim([0, 1])\n",
    "    # plt.ylabel('True Positive Rate')\n",
    "    # plt.xlabel('False Positive Rate')\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "GridSearchCV model accuracy for classification is = 95.55%\n",
      "------------------------------------------------\n",
      "Confusion Matrix:\n",
      "      0    1\n",
      "0  7590  160\n",
      "1   209  324\n",
      "------------------------------------------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      7750\n",
      "           1       0.67      0.61      0.64       533\n",
      "\n",
      "    accuracy                           0.96      8283\n",
      "   macro avg       0.82      0.79      0.81      8283\n",
      "weighted avg       0.95      0.96      0.95      8283\n",
      "\n",
      "Best parameters: {'C': 1, 'class_weight': {0: 1, 1: 3}, 'degree': 3, 'gamma': 0.01, 'kernel': 'linear', 'random_state': 42}\n",
      "  model                                            variant  train_acc  \\\n",
      "0   SVM  best_params={'C': 1, 'class_weight': {0: 1, 1:...   0.981528   \n",
      "\n",
      "   train_prec  train_rec  train_f1  test_acc  test_prec  test_rec   test_f1  \n",
      "0    0.851113   0.871855  0.861359  0.955451   0.669421   0.60788  0.637168  \n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'sigmoid'],\n",
    "    'degree': [3, 4, 5],\n",
    "    'C': [0.01, 0.1, 1],\n",
    "    'gamma': [0.01, 0.1, 1],\n",
    "    'random_state': [42],\n",
    "    'class_weight': [{0: 1, 1: 1}, {0: 1, 1: 3}]\n",
    "}\n",
    "# Initialize SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, scoring='f1', cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Evaluate the best model\n",
    "evaluate_model(best_model, X_test, y_test, \"GridSearchCV\")\n",
    "add_to_eval_df(best_model, \"SVM\", f\"best_params={best_params}\", X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Show the evaluation DataFrame\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the evaluation results as a CSV file\n",
    "evaluation.to_csv(\"evaluation_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
